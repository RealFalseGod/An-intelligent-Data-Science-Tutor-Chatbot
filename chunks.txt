Data Science and Machine Learning
Mathematical and Statistical Methods
Dirk P. Kroese, Zdravko I. Botev, Thomas Taimre, Radislav Vaisman
22nd August 2024
To my wife and daughters: Lesley, Elise, and Jessica
— DPK
To Sarah, Sofia, and my parents
— ZIB
To my grandparents: Arno, Harry, Juta, and Maila
— TT
To Valerie
— RV
CONTENTS
Preface
xiii
Notation
xvii
1
Importing, Summarizing, and Visualizing Data
1
1.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Structuring Features According to Type
. . . . . . . . . . . . . . . . . .
3
1.3
Summary Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.4
Summary Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.5
Visualizing Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.5.1
Plotting Qualitative Variables . . . . . . . . . . . . . . . . . . . .
9
1.5.2
Plotting Quantitative Variables . . . . . . . . . . . . . . . . . . .
9
1.5.3
Data Visualization in a Bivariate Setting . . . . . . . . . . . . . .
12
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
2
Statistical Learning
19
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2.2
Supervised and Unsupervised Learning . . . . . . . . . . . . . . . . . . .
20
2.3
Training and Test Loss
. . . . . . . . . . . . . . . . . . . . . . . . . . .
23
2.4
Tradeoffs in Statistical Learning
. . . . . . . . . . . . . . . . . . . . . .
31
2.5
Estimating Risk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
2.5.1
In-Sample Risk . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
2.5.2
Cross-Validation
. . . . . . . . . . . . . . . . . . . . . . . . . .
38
2.6
Modeling Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
2.7
Multivariate Normal Models
. . . . . . . . . . . . . . . . . . . . . . . .
45
2.8
Normal Linear Models
. . . . . . . . . . . . . . . . . . . . . . . . . . .
47
2

.9
Bayesian Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
3
Monte Carlo Methods
67
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
3.2
Monte Carlo Sampling
. . . . . . . . . . . . . . . . . . . . . . . . . . .
68
3.2.1
Generating Random Numbers
. . . . . . . . . . . . . . . . . . .
68
3.2.2
Simulating Random Variables
. . . . . . . . . . . . . . . . . . .
69
3.2.3
Simulating Random Vectors and Processes . . . . . . . . . . . . .
74
3.2.4
Resampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
3.2.5
Markov Chain Monte Carlo . . . . . . . . . . . . . . . . . . . . .
78
3.3
Monte Carlo Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
vii
viii
Contents
3.3.1
Crude Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . . .
85
3.3.2
Bootstrap Method . . . . . . . . . . . . . . . . . . . . . . . . . .
88
3.3.3
Variance Reduction . . . . . . . . . . . . . . . . . . . . . . . . .
92
3.4
Monte Carlo for Optimization . . . . . . . . . . . . . . . . . . . . . . . .
96
3.4.1
Simulated Annealing . . . . . . . . . . . . . . . . . . . . . . . .
96
3.4.2
Cross-Entropy Method . . . . . . . . . . . . . . . . . . . . . . .
100
3.4.3
Splitting for Optimization . . . . . . . . . . . . . . . . . . . . . .
103
3.4.4
Noisy Optimization . . . . . . . . . . . . . . . . . . . . . . . . .
106
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
114
4
Unsupervised Learning
121
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
4.2
Risk and Loss in Unsupervised Learning . . . . . . . . . . . . . . . . . .
122
4.3
Expectation–Maximization (EM) Algorithm . . . . . . . . . . . . . . . .
128
4.4
Empirical Distribution and Density Estimation . . . . . . . . . . . . . . .
131
4.5
Clustering via Mixture Models . . . . . . . . . . . . . . . . . . . . . 

. .
135
4.5.1
Mixture Models . . . . . . . . . . . . . . . . . . . . . . . . . . .
135
4.5.2
EM Algorithm for Mixture Models . . . . . . . . . . . . . . . . .
137
4.6
Clustering via Vector Quantization . . . . . . . . . . . . . . . . . . . . .
142
4.6.1
K-Means
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
144
4.6.2
Clustering via Continuous Multiextremal Optimization . . . . . .
146
4.7
Hierarchical Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . .
147
4.8
Principal Component Analysis (PCA)
. . . . . . . . . . . . . . . . . . .
153
4.8.1
Motivation: Principal Axes of an Ellipsoid . . . . . . . . . . . . .
154
4.8.2
PCA and Singular Value Decomposition (SVD) . . . . . . . . . .
155
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
160
5
Regression
167
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
167
5.2
Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
169
5.3
Analysis via Linear Models . . . . . . . . . . . . . . . . . . . . . . . . .
171
5.3.1
Parameter Estimation . . . . . . . . . . . . . . . . . . . . . . . .
171
5.3.2
Model Selection and Prediction . . . . . . . . . . . . . . . . . . .
172
5.3.3
Cross-Validation and Predictive Residual Sum of Squares . . . . .
173
5.3.4
In-Sample Risk and Akaike Information Criterion . . . . . . . . .
175
5.3.5
Categorical Features
. . . . . . . . . . . . . . . . . . . . . . . .
177
5.3.6
Nested Models
. . . . . . . . . . . . . . . . . . . . . . . . . . .
180
5.3.7
Coefficient of Determination . . . . . . . . . . . . . . . . . . . .
181
5.4
Inference for Normal Linear Models . . . . . . . . . . . . . . . . . . . .
182
5.4.1
Comparing Two Normal Linear Models . . . . . . . . . . . . . .
183
5.4.2
Confidence and Prediction Intervals
. . . . . . . . . . . . . . . .
186
5.5
Nonlinear Regression Models . . . . . . . . . . . . . . . . . . . . . . . .
188
5.6
Linear Models in Python
. . . . . . . . . . . . .

 . . . . . . . . . . . . .
191
5.6.1
Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
191
5.6.2
Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
193
5.6.3
Analysis of Variance (ANOVA)
. . . . . . . . . . . . . . . . . .
196
Contents
ix
5.6.4
Confidence and Prediction Intervals
. . . . . . . . . . . . . . . .
198
5.6.5
Model Validation . . . . . . . . . . . . . . . . . . . . . . . . . .
199
5.6.6
Variable Selection . . . . . . . . . . . . . . . . . . . . . . . . . .
200
5.7
Generalized Linear Models . . . . . . . . . . . . . . . . . . . . . . . . .
204
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
207
6
Regularization and Kernel Methods
215
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
215
6.2
Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
216
6.3
Reproducing Kernel Hilbert Spaces . . . . . . . . . . . . . . . . . . . . .
222
6.4
Construction of Reproducing Kernels . . . . . . . . . . . . . . . . . . . .
224
6.4.1
Reproducing Kernels via Feature Mapping . . . . . . . . . . . . .
224
6.4.2
Kernels from Characteristic Functions . . . . . . . . . . . . . . .
225
6.4.3
Reproducing Kernels Using Orthonormal Features
. . . . . . . .
227
6.4.4
Kernels from Kernels . . . . . . . . . . . . . . . . . . . . . . . .
229
6.5
Representer Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . .
230
6.6
Smoothing Cubic Splines . . . . . . . . . . . . . . . . . . . . . . . . . .
235
6.7
Gaussian Process Regression . . . . . . . . . . . . . . . . . . . . . . . .
238
6.8
Kernel PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
242
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
245
7
Classification
251
7.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
251
7.2
Classification Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . .
253
7.3


Classification via Bayes’ Rule
. . . . . . . . . . . . . . . . . . . . . . .
257
7.4
Linear and Quadratic Discriminant Analysis . . . . . . . . . . . . . . . .
259
7.5
Logistic Regression and Softmax Classification . . . . . . . . . . . . . .
266
7.6
K-Nearest Neighbors Classification . . . . . . . . . . . . . . . . . . . . .
268
7.7
Support Vector Machine . . . . . . . . . . . . . . . . . . . . . . . . . . .
269
7.8
Classification with Scikit-Learn . . . . . . . . . . . . . . . . . . . . . . .
277
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
279
8
Decision Trees and Ensemble Methods
287
8.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
287
8.2
Top-Down Construction of Decision Trees . . . . . . . . . . . . . . . . .
289
8.2.1
Regional Prediction Functions . . . . . . . . . . . . . . . . . . .
290
8.2.2
Splitting Rules
. . . . . . . . . . . . . . . . . . . . . . . . . . .
291
8.2.3
Termination Criterion . . . . . . . . . . . . . . . . . . . . . . . .
292
8.2.4
Basic Implementation . . . . . . . . . . . . . . . . . . . . . . . .
294
8.3
Additional Considerations . . . . . . . . . . . . . . . . . . . . . . . . . .
298
8.3.1
Binary Versus Non-Binary Trees . . . . . . . . . . . . . . . . . .
298
8.3.2
Data Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . .
298
8.3.3
Alternative Splitting Rules . . . . . . . . . . . . . . . . . . . . .
298
8.3.4
Categorical Variables . . . . . . . . . . . . . . . . . . . . . . . .
299
8.3.5
Missing Values . . . . . . . . . . . . . . . . . . . . . . . . . . .
299
8.4
Controlling the Tree Shape . . . . . . . . . . . . . . . . . . . . . . . . .
300
8.4.1
Cost-Complexity Pruning . . . . . . . . . . . . . . . . . . . . . .
303
x
Contents
8.4.2
Advantages and Limitations of Decision Trees . . . . . . . . . . .
304
8.5
Bootstrap Aggregation
. . . . . . . . . . . . . . . . . . . . . . . . . . .
305
8.6
Random Forests . . . . . . . . . . . . . . . . . . . . . . .

 . . . . . . . .
309
8.7
Boosting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
313
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
321
9
Deep Learning
323
9.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
323
9.2
Feed-Forward Neural Networks . . . . . . . . . . . . . . . . . . . . . . .
326
9.3
Back-Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
331
9.4
Methods for Training . . . . . . . . . . . . . . . . . . . . . . . . . . . .
335
9.4.1
Steepest Descent
. . . . . . . . . . . . . . . . . . . . . . . . . .
335
9.4.2
Levenberg–Marquardt Method . . . . . . . . . . . . . . . . . . .
336
9.4.3
Limited-Memory BFGS Method . . . . . . . . . . . . . . . . . .
337
9.4.4
Adaptive Gradient Methods . . . . . . . . . . . . . . . . . . . . .
339
9.5
Examples in Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
341
9.5.1
Simple Polynomial Regression . . . . . . . . . . . . . . . . . . .
341
9.5.2
Image Classification
. . . . . . . . . . . . . . . . . . . . . . . .
345
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
350
A Linear Algebra and Functional Analysis
355
A.1 Vector Spaces, Bases, and Matrices . . . . . . . . . . . . . . . . . . . . .
355
A.2 Inner Product
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
360
A.3 Complex Vectors and Matrices . . . . . . . . . . . . . . . . . . . . . . .
361
A.4 Orthogonal Projections . . . . . . . . . . . . . . . . . . . . . . . . . . .
362
A.5 Eigenvalues and Eigenvectors . . . . . . . . . . . . . . . . . . . . . . . .
363
A.5.1
Left- and Right-Eigenvectors . . . . . . . . . . . . . . . . . . . .
364
A.6 Matrix Decompositions . . . . . . . . . . . . . . . . . . . . . . . . . . .
368
A.6.1
(P)LU Decomposition
. . . . . . . . . . . . . . . . . . . . . . .
368
A.6.2
Woodbury Identity
. . . . . . . . . . . . . . . . . . . . . . . . .
370
A.6.3
Cholesky Decomposi

tion . . . . . . . . . . . . . . . . . . . . . .
373
A.6.4
QR Decomposition and the Gram–Schmidt Procedure . . . . . . .
375
A.6.5
Singular Value Decomposition . . . . . . . . . . . . . . . . . . .
376
A.6.6
Solving Structured Matrix Equations . . . . . . . . . . . . . . . .
379
A.7 Functional Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
384
A.8 Fourier Transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
390
A.8.1
Discrete Fourier Transform . . . . . . . . . . . . . . . . . . . . .
392
A.8.2
Fast Fourier Transform . . . . . . . . . . . . . . . . . . . . . . .
394
B
Multivariate Differentiation and Optimization
397
B.1
Multivariate Differentiation . . . . . . . . . . . . . . . . . . . . . . . . .
397
B.1.1
Taylor Expansion . . . . . . . . . . . . . . . . . . . . . . . . . .
400
B.1.2
Chain Rule
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
400
B.2
Optimization Theory
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
402
B.2.1
Convexity and Optimization
. . . . . . . . . . . . . . . . . . . .
403
B.2.2
Lagrangian Method . . . . . . . . . . . . . . . . . . . . . . . . .
406
B.2.3
Duality
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
407
Contents
xi
B.3
Numerical Root-Finding and Minimization . . . . . . . . . . . . . . . . .
408
B.3.1
Newton-Like Methods
. . . . . . . . . . . . . . . . . . . . . . .
409
B.3.2
Quasi-Newton Methods . . . . . . . . . . . . . . . . . . . . . . .
411
B.3.3
Normal Approximation Method
. . . . . . . . . . . . . . . . . .
413
B.3.4
Nonlinear Least Squares
. . . . . . . . . . . . . . . . . . . . . .
414
B.4
Constrained Minimization via Penalty Functions . . . . . . . . . . . . . .
415
C Probability and Statistics
421
C.1
Random Experiments and Probability Spaces
. . . . . . . . . . . . . . .
421
C.2
Random Variables and Probability Distributions . . . . . . . . . . . . . .
422
C.3
Expectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
426
C.4
Joint D

istributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
427
C.5
Conditioning and Independence . . . . . . . . . . . . . . . . . . . . . . .
428
C.5.1
Conditional Probability . . . . . . . . . . . . . . . . . . . . . . .
428
C.5.2
Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . .
428
C.5.3
Expectation and Covariance
. . . . . . . . . . . . . . . . . . . .
429
C.5.4
Conditional Density and Conditional Expectation . . . . . . . . .
431
C.6
Functions of Random Variables . . . . . . . . . . . . . . . . . . . . . . .
431
C.7
Multivariate Normal Distribution . . . . . . . . . . . . . . . . . . . . . .
434
C.8
Convergence of Random Variables . . . . . . . . . . . . . . . . . . . . .
439
C.9
Law of Large Numbers and Central Limit Theorem . . . . . . . . . . . .
445
C.10 Markov Chains
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
451
C.11 Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
453
C.12 Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
454
C.12.1 Method of Moments
. . . . . . . . . . . . . . . . . . . . . . . .
455
C.12.2 Maximum Likelihood Method . . . . . . . . . . . . . . . . . . .
456
C.13 Confidence Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
457
C.14 Hypothesis Testing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
458
D Python Primer
463
D.1 Getting Started
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
463
D.2 Python Objects
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
465
D.3 Types and Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
466
D.4 Functions and Methods . . . . . . . . . . . . . . . . . . . . . . . . . . .
468
D.5 Modules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
470
D.6 Flow Control
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
471
D.7 Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 

. . . .
472
D.8 Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
474
D.9 Files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
476
D.10 NumPy
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
478
D.10.1 Creating and Shaping Arrays . . . . . . . . . . . . . . . . . . . .
478
D.10.2 Slicing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
480
D.10.3 Array Operations . . . . . . . . . . . . . . . . . . . . . . . . . .
481
D.10.4 Random Numbers . . . . . . . . . . . . . . . . . . . . . . . . . .
483
D.11 Matplotlib . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
483
D.11.1 Creating a Basic Plot . . . . . . . . . . . . . . . . . . . . . . . .
483
xii
Contents
D.12 Pandas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
486
D.12.1 Series and DataFrame . . . . . . . . . . . . . . . . . . . . . . . .
486
D.12.2 Manipulating Data Frames . . . . . . . . . . . . . . . . . . . . .
487
D.12.3 Extracting Information . . . . . . . . . . . . . . . . . . . . . . .
489
D.12.4 Plotting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
491
D.13 Scikit-learn
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
491
D.13.1 Partitioning the Data
. . . . . . . . . . . . . . . . . . . . . . . .
491
D.13.2 Standardization . . . . . . . . . . . . . . . . . . . . . . . . . . .
492
D.13.3 Fitting and Prediction . . . . . . . . . . . . . . . . . . . . . . . .
493
D.13.4 Testing the Model . . . . . . . . . . . . . . . . . . . . . . . . . .
493
D.14 System Calls, URL Access, and Speed-Up . . . . . . . . . . . . . . . . .
494
Bibliography
496
Index
505
PREFACE
In our present world of automation, cloud computing, algorithms, artificial intelligence,
and big data, few topics are as relevant as data science and machine learning. Their recent
popularity lies not only in their applicability to real-life questions, but also in their natural
blendin

g of many different disciplines, including mathematics, statistics, computer science,
engineering, science, and finance.
To someone starting to learn these topics, the multitude of computational techniques
and mathematical ideas may seem overwhelming. Some may be satisfied with only learn-
ing how to use off-the-shelf recipes to apply to practical situations. But what if the assump-
tions of the black-box recipe are violated? Can we still trust the results? How should the
algorithm be adapted? To be able to truly understand data science and machine learning it
is important to appreciate the underlying mathematics and statistics, as well as the resulting
algorithms.
The purpose of this book is to provide an accessible, yet comprehensive, account of
data science and machine learning. It is intended for anyone interested in gaining a better
understanding of the mathematics and statistics that underpin the rich variety of ideas and
machine learning algorithms in data science. Our viewpoint is that computer languages
come and go, but the underlying key ideas and algorithms will remain forever and will
form the basis for future developments.
Before we turn to a description of the topics in this book, we would like to say a
few words about its philosophy. This book resulted from various courses in data science
and machine learning at the Universities of Queensland and New South Wales, Australia.
When we taught these courses, we noticed that students were eager to learn not only how
to apply algorithms but also to understand how these algorithms actually work. However,
many existing textbooks assumed either too much background knowledge (e.g., measure
theory and functional analysis) or too little (everything is a black box), and the information
overload from often disjointed and contradictory internet sources made it more difficult for
students to gradually build up their knowledge and understanding. We therefore wanted to
write a book about data science and machine learnin

g that can be read as a linear story,
with a substantial “backstory” in the appendices. The main narrative starts very simply and
builds up gradually to quite an advanced level. The backstory contains all the necessary
xiii
xiv
Preface
background, as well as additional information, from linear algebra and functional analysis
(Appendix A), multivariate differentiation and optimization (Appendix B), and probability
and statistics (Appendix C). Moreover, to make the abstract ideas come alive, we believe
it is important that the reader sees actual implementations of the algorithms, directly trans-
lated from the theory. After some deliberation we have chosen Python as our programming
language. It is freely available and has been adopted as the programming language of
choice for many practitioners in data science and machine learning. It has many useful
packages for data manipulation (often ported from R) and has been designed to be easy to
program. A gentle introduction to Python is given in Appendix D.
To keep the book manageable in size we had to be selective in our choice of topics.
Important ideas and connections between various concepts are highlighted via keywords
keywords
and page references (indicated by a ☞) in the margin. Key definitions and theorems are
highlighted in boxes. Whenever feasible we provide proofs of theorems. Finally, we place
great importance on notation. It is often the case that once a consistent and concise system
of notation is in place, seemingly difficult ideas suddenly become obvious. We use differ-
ent fonts to distinguish between different types of objects. Vectors are denoted by letters in
boldface italics, x, X, and matrices by uppercase letters in boldface roman font, A, K. We
also distinguish between random vectors and their values by using upper and lower case
letters, e.g., X (random vector) and x (its value or outcome). Sets are usually denoted by
calligraphic letters G, H. The symbols for probability and expectation are P and E

, respect-
ively. Distributions are indicated by sans serif font, as in Bin and Gamma; exceptions are
the ubiquitous notations N and U for the normal and uniform distributions. A summary of
the most important symbols and abbreviations is given on Pages xvii–xxi.
☞xvii
Data science provides the language and techniques necessary for understanding and
dealing with data. It involves the design, collection, analysis, and interpretation of nu-
merical data, with the aim of extracting patterns and other useful information. Machine
learning, which is closely related to data science, deals with the design of algorithms and
computer resources to learn from data. The organization of the book follows roughly the
typical steps in a data science project: Gathering data to gain information about a research
question; cleaning, summarization, and visualization of the data; modeling and analysis of
the data; translating decisions about the model into decisions and predictions about the re-
search question. As this is a mathematics and statistics oriented book, most emphasis will
be on modeling and analysis.
We start in Chapter 1 with the reading, structuring, summarization, and visualization
of data using the data manipulation package pandas in Python. Although the material
covered in this chapter requires no mathematical knowledge, it forms an obvious starting
point for data science: to better understand the nature of the available data. In Chapter 2, we
introduce the main ingredients of statistical learning. We distinguish between supervised
and unsupervised learning techniques, and discuss how we can assess the predictive per-
formance of (un)supervised learning methods. An important part of statistical learning is
the modeling of data. We introduce various useful models in data science including linear,
multivariate Gaussian, and Bayesian models. Many algorithms in machine learning and
data science make use of Monte Carlo techniques, which is the topic of Chapter 3. Monte
Carlo c

an be used for simulation, estimation, and optimization. Chapter 4 is concerned
with unsupervised learning, where we discuss techniques such as density estimation, clus-
tering, and principal component analysis. We then turn our attention to supervised learning
Preface
xv
in Chapter 5, and explain the ideas behind a broad class of regression models. Therein, we
also describe how Python’s statsmodels package can be used to define and analyze linear
models. Chapter 6 builds upon the previous regression chapter by developing the power-
ful concepts of kernel methods and regularization, which allow the fundamental ideas of
Chapter 5 to be expanded in an elegant way, using the theory of reproducing kernel Hilbert
spaces. In Chapter 7, we proceed with the classification task, which also belongs to the
supervised learning framework, and consider various methods for classification, including
Bayes classification, linear and quadratic discriminant analysis, K-nearest neighbors, and
support vector machines. In Chapter 8 we consider versatile methods for regression and
classification that make use of tree structures. Finally, in Chapter 9, we consider the work-
ings of neural networks and deep learning, and show that these learning algorithms have a
simple mathematical interpretation. An extensive range of exercises is provided at the end
of each chapter.
Python code and data sets for each chapter can be downloaded from the GitHub site:
https://github.com/DSML-book
Acknowledgments
Some of the Python code for Chapters 1 and 5 was adapted from [73]. We thank Benoit
Liquet for making this available, and Lauren Jones for translating the R code into Python.
We thank all who through their comments, feedback, and suggestions have contributed
to this book, including Qibin Duan, Luke Taylor, Rémi Mouzayek, Harry Goodman, Bryce
Stansfield, Ryan Tongs, Dillon Steyl, Bill Rudd, Nan Ye, Christian Hirsch, Chris van der
Heide, Sarat Moka, Aapeli Vuorinen, Joshua Ross, Giang Nguyen, and the a

nonymous
referees. David Grubbs deserves a special accollade for his professionalism and attention
to detail in his role as Editor for this book.
The book was test-run during the 2019 Summer School of the Australian Mathemat-
ical Sciences Institute. More than 80 bright upper-undergraduate (Honours) students used
the book for the course Mathematical Methods for Machine Learning, taught by Zdravko
Botev. We are grateful for the valuable feedback that they provided.
Our special thanks go out to Robert Salomone, Liam Berry, Robin Carrick, and Sam
Daley, who commented in great detail on earlier versions of the entire book and wrote and
improved our Python code. Their enthusiasm, perceptiveness, and kind assistance have
been invaluable.
Of course, none of this work would have been possible without the loving support,
patience, and encouragement from our families, and we thank them with all our hearts.
This book was financially supported by the Australian Research Council Centre of
Excellence for Mathematical & Statistical Frontiers, under grant number CE140100049.
Dirk Kroese, Zdravko Botev,
Thomas Taimre, and Radislav Vaisman
Brisbane and Sydney
xvi
NOTATION
We could, of course, use any notation we want; do not laugh at notations;
invent them, they are powerful. In fact, mathematics is, to a large extent, in-
vention of better notations.
Richard P. Feynman
We have tried to use a notation system that is, in order of importance, simple, descript-
ive, consistent, and compatible with historical choices. Achieving all of these goals all of
the time would be impossible, but we hope that our notation helps to quickly recognize
the type or “flavor” of certain mathematical objects (vectors, matrices, random vectors,
probability measures, etc.) and clarify intricate ideas.
We make use of various typographical aids, and it will be beneficial for the reader to
be aware of some of these.
• Boldface font is used to indicate composite objects, such as column vectors x =
[x1, . . . , 

xn]⊤and matrices X = [xi j]. Note also the difference between the upright bold
font for matrices and the slanted bold font for vectors.
• Random variables are generally specified with upper case roman letters X, Y, Z and their
outcomes with lower case letters x, y, z. Random vectors are thus denoted in upper case
slanted bold font: X = [X1, . . . , Xn]⊤.
• Sets of vectors are generally written in calligraphic font, such as X, but the set of real
numbers uses the common blackboard bold font R. Expectation and probability also use
the latter font.
• Probability distributions use a sans serif font, such as Bin and Gamma. Exceptions to
this rule are the “standard” notations N and U for the normal and uniform distributions.
• We often omit brackets when it is clear what the argument is of a function or operator.
For example, we prefer EX2 to E[X2].
xvii
xviii
Notation
• We employ color to emphasize that certain words refer to a dataset, function, or
package in Python. All code is written in typewriter font. To be compatible with past
notation choices, we introduced a special blue symbol X for the model (design) matrix of
a linear model.
• Important notation such as T , g, g∗is often defined in a mnemonic way, such as T for
“training”, g for “guess”, g∗for the “star” (that is, optimal) guess, and ℓfor “loss”.
• We will occasionally use a Bayesian notation convention in which the same symbol is
used to denote different (conditional) probability densities. In particular, instead of writing
fX(x) and fX | Y(x | y) for the probability density function (pdf) of X and the conditional pdf
of X given Y, we simply write f(x) and f(x | y). This particular style of notation can be of
great descriptive value, despite its apparent ambiguity.
General font/notation rules
x
scalar
x
vector
X
random vector
X
matrix
X
set
bx
estimate or approximation
x∗
optimal
x
average
Common mathematical symbols
∀
for all
∃
there exists
∝
is proportional to
⊥
is perpendicular to
∼
is distributed as
iid∼

, ∼iid
are independent and identically distributed as
approx.
∼
is approximately distributed as
∇f
gradient of f
∇2 f
Hessian of f
f ∈Cp
f has continuous derivatives of order p
≈
is approximately
≃
is asymptotically
≪
is much smaller than
⊕
direct sum
Notation
xix
⊙
elementwise product
∩
intersection
∪
union
:=, =:
is defined as
a.s.
−→
converges almost surely to
d
−→
converges in distribution to
P
−→
converges in probability to
Lp
−→
converges in Lp-norm to
∥· ∥
Euclidean norm
⌈x⌉
smallest integer larger than x
⌊x⌋
largest integer smaller than x
x+
max{x, 0}
Matrix/vector notation
A⊤, x⊤
transpose of matrix A or vector x
A−1
inverse of matrix A
A+
pseudo-inverse of matrix A
A−⊤
inverse of matrix A⊤or transpose of A−1
A ≻0
matrix A is positive definite
A ⪰0
matrix A is positive semidefinite
dim(x)
dimension of vector x
det(A)
determinant of matrix A
|A|
absolute value of the determinant of matrix A
tr(A)
trace of matrix A
Reserved letters and words
C
set of complex numbers
d
differential symbol
E
expectation
e
the number 2.71828 . . .
f
probability density (discrete or continuous)
g
prediction function
1{A} or 1A indicator function of set A
i
the square root of −1
ℓ
risk: expected loss
xx
Notation
Loss
loss function
ln
(natural) logarithm
N
set of natural numbers {0, 1, . . .}
O
big-O order symbol: f(x) = O(g(x)) if | f(x)| ⩽αg(x) for some constant α as
x →a
o
little-o order symbol: f(x) = o(g(x)) if f(x)/g(x) →0 as x →a
P
probability measure
π
the number 3.14159 . . .
R
set of real numbers (one-dimensional Euclidean space)
Rn
n-dimensional Euclidean space
R+
positive real line: [0, ∞)
τ
deterministic training set
T
random training set
X
model (design) matrix
Z
set of integers {. . . , −1, 0, 1, . . .}
Probability distributions
Ber
Bernoulli
Beta
beta
Bin
binomial
Exp
exponential
Geom
geometric
Gamma
gamma
F
Fisher–Snedecor F
N
normal or Gaussian
Pareto
Pareto
Poi
Poisson
t
Student’s t
U
uniform
Abbreviations and acronyms
cdf
cumulative distribution function
CMC
cru

de Monte Carlo
CE
cross-entropy
EM
expectation–maximization
GP
Gaussian process
KDE
Kernel density estimate/estimator
Notation
xxi
KL
Kullback–Leibler
KKT
Karush–Kuhn–Tucker
iid
independent and identically distributed
MAP
maximum a posteriori
MCMC
Markov chain Monte Carlo
MLE
maximum likelihood estimator/estimate
OOB
out-of-bag
PCA
principal component analysis
pdf
probability density function (discrete or continuous)
SVD
singular value decomposition
xxii
CHAPTER1
IMPORTING, SUMMARIZING, AND
VISUALIZING DATA
This chapter describes where to find useful data sets, how to load them into Python,
and how to (re)structure the data. We also discuss various ways in which the data can
be summarized via tables and figures. Which type of plots and numerical summaries
are appropriate depends on the type of the variable(s) in play. Readers unfamiliar with
Python are advised to read Appendix D first.
1.1
Introduction
Data comes in many shapes and forms, but can generally be thought of as being the result
of some random experiment — an experiment whose outcome cannot be determined in
advance, but whose workings are still subject to analysis. Data from a random experiment
are often stored in a table or spreadsheet. A statistical convention is to denote variables —
often called features
features
— as columns and the individual items (or units) as rows. It is useful
to think of three types of columns in such a spreadsheet:
1. The first column is usually an identifier or index column, where each unit/row is
given a unique name or ID.
2. Certain columns (features) can correspond to the design of the experiment, specify-
ing, for example, to which experimental group the unit belongs. Often the entries in
these columns are deterministic; that is, they stay the same if the experiment were to
be repeated.
3. Other columns represent the observed measurements of the experiment. Usually,
these measurements exhibit variability; that is, they would change if the experiment
were to be repeated.
T

here are many data sets available from the Internet and in software packages. A well-
known repository of data sets is the Machine Learning Repository maintained by the Uni-
versity of California at Irvine (UCI), found at https://archive.ics.uci.edu/.
1
2
Introduction
These data sets are typically stored in a CSV (comma separated values) format, which
can be easily read into Python. For example, to access the abalone data set from this web-
site with Python, download the file to your working directory, import the pandas package
via
import pandas as pd
and read in the data as follows:
abalone = pd.read_csv('abalone.data',header = None)
It is important to add header = None, as this lets Python know that the first line of the
CSV does not contain the names of the features, as it assumes so by default. The data set
was originally used to predict the age of abalone from physical measurements, such as
shell weight and diameter.
Another useful repository of over 1000 data sets from various packages in the R pro-
gramming language, collected by Vincent Arel-Bundock, can be found at:
https://vincentarelbundock.github.io/Rdatasets/datasets.html.
For example, to read Fisher’s famous iris data set from R’s datasets package into Py-
thon, type:
urlprefix = 'https://vincentarelbundock.github.io/Rdatasets/csv/'
dataname = 'datasets/iris.csv'
iris = pd.read_csv(urlprefix + dataname)
The iris data set contains four physical measurements (sepal/petal length/width) on
50 specimens (each) of 3 species of iris: setosa, versicolor, and virginica. Note that in this
case the headers are included. The output of read_csv is a DataFrame object, which is
pandas’s implementation of a spreadsheet; see Section D.12.1. The DataFrame method
☞486
head gives the first few rows of the DataFrame, including the feature names. The number
of rows can be passed as an argument and is 5 by default. For the iris DataFrame, we
have:
iris.head()
Unnamed: 0
Sepal.Length
...
Petal.Width
Species
0
1
5.1
...
0.2
se

tosa
1
2
4.9
...
0.2
setosa
2
3
4.7
...
0.2
setosa
3
4
4.6
...
0.2
setosa
4
5
5.0
...
0.2
setosa
[5 rows x 6 columns]
The names of the features can be obtained via the columns attribute of the DataFrame
object, as in iris.columns. Note that the first column is a duplicate index column, whose
name (assigned by pandas) is 'Unnamed: 0'. We can drop this column and reassign the
iris object as follows:
Importing, Summarizing, and Visualizing Data
3
iris = iris.drop('Unnamed: 0',1)
The data for each feature (corresponding to its specific name) can be accessed by using
Python’s slicing notation []. For example, the object iris[’Sepal.Length’] contains
the 150 sepal lengths.
The first three rows of the abalone data set from the UCI repository can be found as
follows:
abalone.head(3)
0
1
2
3
4
5
6
7
8
0
M
0.455
0.365
0.095
0.5140
0.2245
0.1010
0.150
15
1
M
0.350
0.265
0.090
0.2255
0.0995
0.0485
0.070
7
2
F
0.530
0.420
0.135
0.6770
0.2565
0.1415
0.210
9
Here, the missing headers have been assigned according to the order of the natural
numbers. The names should correspond to Sex, Length, Diameter, Height, Whole weight,
Shucked weight, Viscera weight, Shell weight, and Rings, as described in the file with the
name abalone.names on the UCI website. We can manually add the names of the features
to the DataFrame by reassigning the columns attribute, as in:
abalone.columns = ['Sex', 'Length', 'Diameter', 'Height',
'Whole weight','Shucked weight', 'Viscera weight', 'Shell weight',
'Rings']
1.2
Structuring Features According to Type
We can generally classify features as either quantitative or qualitative. Quantitative
Quantitative
features
possess “numerical quantity”, such as height, age, number of births, etc., and can either be
continuous or discrete. Continuous quantitative features take values in a continuous range
of possible values, such as height, voltage, or crop yield; such features capture the idea
that measurements can always be made more precisely. Discrete quantitative 

features have
a countable number of possibilities, such as a count.
In contrast, qualitative
qualitative
features do not have a numerical meaning, but their possible
values can be divided into a fixed number of categories, such as {M,F} for gender or {blue,
black, brown, green} for eye color. For this reason such features are also called categorical
categorical
.
A simple rule of thumb is: if it does not make sense to average the data, it is categorical.
For example, it does not make sense to average eye colors. Of course it is still possible to
represent categorical data with numbers, such as 1 = blue, 2 = black, 3 = brown, but such
numbers carry no quantitative meaning. Categorical features are often called factors
factors
.
When manipulating, summarizing, and displaying data, it is important to correctly spe-
cify the type of the variables (features). We illustrate this using the nutrition_elderly
data set from [73], which contains the results of a study involving nutritional measure-
ments of thirteen features (columns) for 226 elderly individuals (rows). The data set can be
obtained from:
http://www.biostatisticien.eu/springeR/nutrition_elderly.xls.
4
Structuring Features According to Type
Excel files can be read directly into pandas via the read_excel method:
xls = 'http://www.biostatisticien.eu/springeR/nutrition_elderly.xls'
nutri = pd.read_excel(xls)
This creates a DataFrame object nutri. The first three rows are as follows:
pd.set_option('display.max_columns', 8) # to fit display
nutri.head(3)
gender
situation
tea ...
cooked_fruit_veg
chocol
fat
0
2
1
0 ...
4
5
6
1
2
1
1 ...
5
1
4
2
2
1
0 ...
2
5
4
[3 rows x 13 columns]
You can check the type (or structure) of the variables via the info method of nutri.
nutri.info()
<class 'pandas.core.frame.DataFrame '>
RangeIndex: 226 entries , 0 to 225
Data columns (total 13 columns):
gender
226 non-null int64
situation
226 non-null int64
tea
226 non-null int64
coffee
226 non-null int64
height
226 non-null int64
weight


226 non-null int64
age
226 non-null int64
meat
226 non-null int64
fish
226 non-null int64
raw_fruit
226 non-null int64
cooked_fruit_veg
226 non-null int64
chocol
226 non-null int64
fat
226 non-null int64
dtypes: int64(13)
memory usage: 23.0 KB
All 13 features in nutri are (at the moment) interpreted by Python as quantitative
variables, indeed as integers, simply because they have been entered as whole numbers.
The meaning of these numbers becomes clear when we consider the description of the
features, given in Table 1.2. Table 1.1 shows how the variable types should be classified.
Table 1.1: The feature types for the data frame nutri.
Qualitative
gender, situation, fat
meat, fish, raw_fruit, cooked_fruit_veg, chocol
Discrete quantitative
tea, coffee
Continuous quantitative
height, weight, age
Importing, Summarizing, and Visualizing Data
5
Table 1.2: Description of the variables in the nutritional study [73].
Feature
Description
Unit or Coding
gender
Gender
1=Male; 2=Female
situation
Family status
1=Single
2=Living with spouse
3=Living with family
4=Living with someone else
tea
Daily consumption of tea
Number of cups
coffee
Daily consumption of coffee
Number of cups
height
Height
cm
weight
Weight (actually: mass)
kg
age
Age at date of interview
Years
meat
Consumption of meat
0=Never
1=Less than once a week
2=Once a week
3=2–3 times a week
4=4–6 times a week
5=Every day
fish
Consumption of fish
As in meat
raw_fruit
Consumption of raw fruits
As in meat
cooked_fruit_veg
Consumption of cooked
As in meat
fruits and vegetables
chocol
Consumption of chocolate
As in meat
fat
1=Butter
2=Margarine
3=Peanut oil
Type of fat used
4=Sunflower oil
for cooking
5=Olive oil
6=Mix of vegetable oils (e.g., Isio4)
7=Colza oil
8=Duck or goose fat
Note that the categories of the qualitative features in the second row of Table 1.1, meat,
. . . , chocol have a natural order. Such qualitative features are sometimes called ordinal, in
contrast to qualitative features without order, which are c

alled nominal. We will not make
such a distinction in this book.
We can modify the Python value and type for each categorical feature, using the
replace and astype methods. For categorical features, such as gender, we can replace
the value 1 with 'Male' and 2 with 'Female', and change the type to 'category' as
follows.
DICT = {1:'Male', 2:'Female'} # dictionary specifies replacement
nutri['gender'] = nutri['gender'].replace(DICT).astype('category')
The structure of the other categorical-type features can be changed in a similar way.
Continuous features such as height should have type float:
nutri['height'] = nutri['height'].astype(float)
6
Summary Tables
We can repeat this for the other variables (see Exercise 2) and save this modified data
frame as a CSV file, by using the pandas method to_csv.
nutri.to_csv('nutri.csv',index=False)
1.3
Summary Tables
It is often useful to summarize a large spreadsheet of data in a more condensed form. A
table of counts or a table of frequencies makes it easier to gain insight into the underlying
distribution of a variable, especially if the data are qualitative. Such tables can be obtained
with the methods describe and value_counts.
As a first example, we load the nutri DataFrame, which we restructured and saved
(see previous section) as 'nutri.csv', and then construct a summary for the feature
(column) 'fat'.
nutri = pd.read_csv('nutri.csv')
nutri['fat'].describe()
count
226
unique
8
top
sunflower
freq
68
Name: fat, dtype: object
We see that there are 8 different types of fat used and that sunflower has the highest
count, with 68 out of 226 individuals using this type of cooking fat. The method
value_counts gives the counts for the different fat types.
nutri['fat'].value_counts()
sunflower
68
peanut
48
olive
40
margarine
27
Isio4
23
butter
15
duck
4
colza
1
Name: fat, dtype: int64
Column labels are also attributes of a DataFrame, and nutri.fat, for example, is
exactly the same object as nutri['fat'].
Importing, Summarizing, and Vi

sualizing Data
7
It is also possible to use crosstab to cross tabulate between two or more variables,
cross tabulate
giving a contingency table:
pd.crosstab(nutri.gender, nutri.situation)
situation
Couple
Family
Single
gender
Female
56
7
78
Male
63
2
20
We see, for example, that the proportion of single men is substantially smaller than the
proportion of single women in the data set of elderly people. To add row and column totals
to a table, use margins=True.
pd.crosstab(nutri.gender, nutri.situation , margins=True)
situation
Couple
Family
Single
All
gender
Female
56
7
78
141
Male
63
2
20
85
All
119
9
98
226
1.4
Summary Statistics
In the following, x = [x1, . . . , xn]⊤is a column vector of n numbers. For our nutri data,
the vector x could, for example, correspond to the heights of the n = 226 individuals.
The sample mean
sample mean
of x, denoted by x, is simply the average of the data values:
x = 1
n
n
X
i=1
xi.
Using the mean method in Python for the nutri data, we have, for instance:
nutri['height'].mean()
163.96017699115043
The p-sample quantile
sample quantile
(0 < p < 1) of x is a value x such that at least a fraction p of the
data is less than or equal to x and at least a fraction 1−p of the data is greater than or equal
to x. The sample median
sample median
is the sample 0.5-quantile. The p-sample quantile is also called
the 100 × p percentile. The 25, 50, and 75 sample percentiles are called the first, second,
and third quartiles
quartiles
of the data. For the nutri data they are obtained as follows.
nutri['height'].quantile(q=[0.25,0.5,0.75])
0.25
157.0
0.50
163.0
0.75
170.0
8
Visualizing Data
The sample mean and median give information about the location of the data, while the
distance between sample quantiles (say the 0.1 and 0.9 quantiles) gives some indication of
the dispersion (spread) of the data. Other measures for dispersion are the sample range,
sample range
maxixi −minixi, the sample variance
sample variance
s2 =
1
n −1
n
X
i=1
(xi −x )2,
(1.1)


and the sample standard deviation s =
√
s2. For the nutri data, the range (in cm) is:
sample
standard
deviation
☞455
nutri['height'].max() - nutri['height'].min()
48.0
The variance (in cm2) is:
round(nutri['height'].var(), 2)
# round to two decimal places
81.06
And the standard deviation can be found via:
round(nutri['height'].std(), 2)
9.0
We already encountered the describe method in the previous section for summarizing
qualitative features, via the most frequent count and the number of unique elements. When
applied to a quantitative feature, it returns instead the minimum, maximum, mean, and the
three quartiles. For example, the 'height' feature in the nutri data has the following
summary statistics.
nutri['height'].describe()
count
226.000000
mean
163.960177
std
9.003368
min
140.000000
25\%
157.000000
50\%
163.000000
75\%
170.000000
max
188.000000
Name: height, dtype: float64
1.5
Visualizing Data
In this section we describe various methods for visualizing data. The main point we would
like to make is that the way in which variables are visualized should always be adapted to
the variable types; for example, qualitative data should be plotted differently from quantit-
ative data.
Importing, Summarizing, and Visualizing Data
9
For the rest of this section, it is assumed that matplotlib.pyplot, pandas, and
numpy, have been imported in the Python code as follows.
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
1.5.1
Plotting Qualitative Variables
Suppose we wish to display graphically how many elderly people are living by themselves,
as a couple, with family, or other. Recall that the data are given in the situation column
of our nutri data. Assuming that we already restructured the data, as in Section 1.2, we
☞
3
can make a barplot of the number of people in each category via the plt.bar function of
barplot
the standard matplotlib plotting library. The inputs are the x-axis positions, heights, and
widths of each bar respectively.
width = 0.35 

# the width of the bars
x = [0, 0.8, 1.6] # the bar positions on x-axis
situation_counts=nutri['situation'].value_counts()
plt.bar(x, situation_counts , width, edgecolor = 'black')
plt.xticks(x, situation_counts.index)
plt.show()
Couple
Single
Family
0
25
50
75
100
125
Figure 1.1: Barplot for the qualitative variable 'situation'.
1.5.2
Plotting Quantitative Variables
We now present a few useful methods for visualizing quantitative data, again using the
nutri data set. We will first focus on continuous features (e.g., 'age') and then add some
specific graphs related to discrete features (e.g., 'tea'). The aim is to describe the variab-
ility present in a single feature. This typically involves a central tendency, where observa-
tions tend to gather around, with fewer observations further away. The main aspects of the
distribution are the location (or center) of the variability, the spread of the variability (how
far the values extend from the center), and the shape of the variability; e.g., whether or not
values are spread symmetrically on either side of the center.
10
Visualizing Data
1.5.2.1
Boxplot
A boxplot can be viewed as a graphical representation of the five-number summary of
boxplot
the data consisting of the minimum, maximum, and the first, second, and third quartiles.
Figure 1.2 gives a boxplot for the 'age' feature of the nutri data.
plt.boxplot(nutri['age'],widths=width,vert=False)
plt.xlabel('age')
plt.show()
The widths parameter determines the width of the boxplot, which is by default plotted
vertically. Setting vert=False plots the boxplot horizontally, as in Figure 1.2.
65
70
75
80
85
90
age
1
Figure 1.2: Boxplot for 'age'.
The box is drawn from the first quartile (Q1) to the third quartile (Q3). The vertical line
inside the box signifies the location of the median. So-called “whiskers” extend to either
side of the box. The size of the box is called the interquartile range: IQR = Q3 −Q1. The
left whisker extends to the largest of (a) the minimum of t

he data and (b) Q1 −1.5 IQR.
Similarly, the right whisker extends to the smallest of (a) the maximum of the data and
(b) Q3 + 1.5 IQR. Any data point outside the whiskers is indicated by a small hollow dot,
indicating a suspicious or deviant point (outlier). Note that a boxplot may also be used for
discrete quantitative features.
1.5.2.2
Histogram
A histogram is a common graphical representation of the distribution of a quantitative
histogram
feature. We start by breaking the range of the values into a number of bins or classes.
We tally the counts of the values falling in each bin and then make the plot by drawing
rectangles whose bases are the bin intervals and whose heights are the counts. In Python
we can use the function plt.hist. For example, Figure 1.3 shows a histogram of the 226
ages in nutri, constructed via the following Python code.
weights = np.ones_like(nutri.age)/nutri.age.count()
plt.hist(nutri.age,bins=9,weights=weights ,facecolor='cyan',
edgecolor='black', linewidth=1)
plt.xlabel('age')
plt.ylabel('Proportion of Total')
plt.show()
Importing, Summarizing, and Visualizing Data
11
Here 9 bins were used. Rather than using raw counts (the default), the vertical axis
here gives the percentage in each class, defined by count
total . This is achieved by choosing the
“weights” parameter to be equal to the vector with entries 1/266, with length 226. Various
plotting parameters have also been changed.
65
70
75
80
85
90
age
0.00
0.05
0.10
0.15
0.20
Proportion of Total
Figure 1.3: Histogram of 'age'.
Histograms can also be used for discrete features, although it may be necessary to
explicitly specify the bins and placement of the ticks on the axes.
1.5.2.3
Empirical Cumulative Distribution Function
The empirical cumulative distribution function, denoted by Fn, is a step function which
empirical
cumulative
distribution
function
jumps an amount k/n at observation values, where k is the number of tied observations
at that value. For observations x1, . . . , xn, Fn

(x) is the fraction of observations less than or
equal to x, i.e.,
Fn(x) = number of xi ⩽x
n
= 1
n
n
X
i=1
1 {xi ⩽x} ,
(1.2)
where 1 denotes the indicator
indicator
function; that is, 1 {xi ⩽x} is equal to 1 when xi ⩽x and 0
otherwise. To produce a plot of the empirical cumulative distribution function we can use
the plt.step function. The result for the age data is shown in Figure 1.4. The empirical
cumulative distribution function for a discrete quantitative variable is obtained in the same
way.
x = np.sort(nutri.age)
y = np.linspace(0,1,len(nutri.age))
plt.xlabel('age')
plt.ylabel('Fn(x)')
plt.step(x,y)
plt.xlim(x.min(),x.max())
plt.show()
12
Visualizing Data
65
70
75
80
85
90
age
0.0
0.2
0.4
0.6
0.8
1.0
Fn(x)
Figure 1.4: Plot of the empirical distribution function for the continuous quantitative fea-
ture 'age'.
1.5.3
Data Visualization in a Bivariate Setting
In this section, we present a few useful visual aids to explore relationships between two
features. The graphical representation will depend on the type of the two features.
1.5.3.1
Two-way Plots for Two Categorical Variables
Comparing barplots for two categorical variables involves introducing subplots to the fig-
ure. Figure 1.5 visualizes the contingency table of Section 1.3, which cross-tabulates the
family status (situation) with the gender of the elderly people. It simply shows two barplots
next to each other in the same figure.
Couple
Family
Single
0
20
40
60
80
Counts
Male
Female
Figure 1.5: Barplot for two categorical variables.
Importing, Summarizing, and Visualizing Data
13
The figure was made using the seaborn package, which was specifically designed to
simplify statistical visualization tasks.
import seaborn as sns
sns.countplot(x='situation', hue = 'gender', data=nutri,
hue_order = ['Male', 'Female'], palette = ['SkyBlue','Pink'],
saturation = 1, edgecolor='black')
plt.legend(loc='upper center')
plt.xlabel('')
plt.ylabel('Counts')
plt.show()
1.5.3.2
Plots for Two Quantitative Variables
We can 

visualize patterns between two quantitative features using a scatterplot
scatterplot
. This can be
done with plt.scatter. The following code produces a scatterplot of 'weight' against
'height' for the nutri data.
plt.scatter(nutri.height, nutri.weight, s=12, marker='o')
plt.xlabel('height')
plt.ylabel('weight')
plt.show()
140
150
160
170
180
190
height
40
50
60
70
80
90
weight
Figure 1.6: Scatterplot of 'weight' against 'height'.
The next Python code illustrates that it is possible to produce highly sophisticated scat-
ter plots, such as in Figure 1.7. The figure shows the birth weights (mass) of babies whose
mothers smoked (blue triangles) or not (red circles). In addition, straight lines were fitted to
the two groups, suggesting that birth weight decreases with age when the mother smokes,
but increases when the mother does not smoke! The question is whether these trends are
statistically significant or due to chance. We will revisit this data set later on in the book.
☞200
14
Visualizing Data
urlprefix = 'https://vincentarelbundock.github.io/Rdatasets/csv/'
dataname = 'MASS/birthwt.csv'
bwt = pd.read_csv(urlprefix + dataname)
bwt = bwt.drop('Unnamed: 0',1)
#drop unnamed column
styles = {0: ['o','red'], 1: ['^','blue']}
for k in styles:
grp = bwt[bwt.smoke==k]
m,b = np.polyfit(grp.age, grp.bwt, 1) # fit a straight line
plt.scatter(grp.age, grp.bwt, c=styles[k][1], s=15, linewidth=0,
marker = styles[k][0])
plt.plot(grp.age, m*grp.age + b, '-', color=styles[k][1])
plt.xlabel('age')
plt.ylabel('birth weight (g)')
plt.legend(['non-smokers','smokers'],prop={'size':8},
loc=(0.5,0.8))
plt.show()
10
15
20
25
30
35
40
45
50
age
0
1000
2000
3000
4000
5000
6000
birth weight (g)
non-smokers
smokers
Figure 1.7: Birth weight against age for smoking and non-smoking mothers.
1.5.3.3
Plots for One Qualitative and One Quantitative Variable
In this setting, it is interesting to draw boxplots of the quantitative feature for each level
of the categorical feature. Assuming the variables

 are structured correctly, the function
plt.boxplot can be used to produce Figure 1.8, using the following code:
males = nutri[nutri.gender == 'Male']
females = nutri[nutri.gender == 'Female']
plt.boxplot([males.coffee,females.coffee],notch=True,widths
=(0.5,0.5))
plt.xlabel('gender')
plt.ylabel('coffee')
plt.xticks([1,2],['Male','Female'])
plt.show()
Importing, Summarizing, and Visualizing Data
15
Male
Female
gender
0
1
2
3
4
5
coffee
Figure 1.8: Boxplots of a quantitative feature 'coffee' as a function of the levels of a
categorical feature 'gender'. Note that we used a different, “notched”, style boxplot this
time.
Further Reading
The focus in this book is on the mathematical and statistical analysis of data, and for the
rest of the book we assume that the data is available in a suitable form for analysis. How-
ever, a large part of practical data science involves the cleaning of data; that is, putting
it into a form that is amenable to analysis with standard software packages. Standard Py-
thon modules such as numpy and pandas can be used to reformat rows, rename columns,
remove faulty outliers, merge rows, and so on. McKinney, the creator of pandas, gives
many practical case studies in [84]. Effective data visualization techniques are beautifully
illustrated in [65].
Exercises
Before you attempt these exercises, make sure you have up-to-date versions of the relevant
Python packages, specifically matplotlib, pandas, and seaborn. An easy way to ensure
this is to update packages via the Anaconda Navigator, as explained in Appendix D.
1. Visit the UCI Repository https://archive.ics.uci.edu/. Read the description of
the data and download the Mushroom data set agaricus-lepiota.data. Using pandas,
read the data into a DataFrame called mushroom, via read_csv.
(a) How many features are in this data set?
(b) What are the initial names and types of the features?
(c) Rename the first feature (index 0) to 'edibility' and the sixth feature (index 5) to
'odor' [Hint: the colu

mn names in pandas are immutable; so individual columns
cannot be modified directly. However it is possible to assign the entire column names
list via mushroom.columns = newcols. ]
16
Exercises
(d) The 6th column lists the various odors of the mushrooms: encoded as 'a', 'c', ....
Replace these with the names 'almond', 'creosote', etc. (categories correspond-
ing to each letter can be found on the website). Also replace the 'edibility' cat-
egories 'e' and 'p' with 'edible' and 'poisonous'.
(e) Make a contingency table cross-tabulating 'edibility' and 'odor'.
(f) Which mushroom odors should be avoided, when gathering mushrooms for consump-
tion?
(g) What proportion of odorless mushroom samples were safe to eat?
2. Change the type and value of variables in the nutri data set according to Table 1.2 and
save the data as a CSV file. The modified data should have eight categorical features, three
floats, and two integer features.
3. It frequently happens that a table with data needs to be restructured before the data can
be analyzed using standard statistical software. As an example, consider the test scores in
Table 1.3 of 5 students before and after specialized tuition.
Table 1.3: Student scores.
Student
Before
After
1
75
85
2
30
50
3
100
100
4
50
52
5
60
65
This is not in the standard format described in Section 1.1. In particular, the student scores
are divided over two columns, whereas the standard format requires that they are collected
in one column, e.g., labelled 'Score'. Reformat (by hand) the table in standard format,
using three features:
• 'Score', taking continuous values,
• 'Time', taking values 'Before' and 'After',
• 'Student', taking values from 1 to 5.
Useful methods for reshaping tables in pandas are melt, stack, and unstack.
4. Create a similar barplot as in Figure 1.5, but now plot the corresponding proportions of
males and females in each of the three situation categories. That is, the heights of the bars
should sum up to 1 for both barplots with th

e same ’gender’ value. [Hint: seaborn does
not have this functionality built in, instead you need to first create a contingency table and
use matplotlib.pyplot to produce the figure.]
5. The iris data set, mentioned in Section 1.1, contains various features, including
☞2
'Petal.Length' and 'Sepal.Length', of three species of iris: setosa, versicolor, and
virginica.
Importing, Summarizing, and Visualizing Data
17
(a) Load the data set into a pandas DataFrame object.
(b) Using matplotlib.pyplot, produce boxplots of 'Petal.Length' for each the
three species, in one figure.
(c) Make a histogram with 20 bins for 'Petal.Length'.
(d) Produce a similar scatterplot for 'Sepal.Length' against 'Petal.Length' to that
of the left plot in Figure 1.9. Note that the points should be colored according to the
’Species’ feature as per the legend in the right plot of the figure.
(e) Using the kdeplot method of the seaborn package, reproduce the right plot of
Figure 1.9, where kernel density plots for 'Petal.Length' are given.
☞131
1
2
3
4
5
6
7
Petal.Length
5
6
7
8
Sepal.Length
2
4
6
8
Petal.Length
0.0
0.5
1.0
1.5
2.0
2.5
Density
setosa
versicolor
virginica
Figure 1.9: Left: scatterplot of 'Sepal.Length' against 'Petal.Length'. Right: kernel
density estimates of 'Petal.Length' for the three species of iris.
6. Import the data set EuStockMarkets from the same website as the iris data set above.
The data set contains the daily closing prices of four European stock indices during the
1990s, for 260 working days per year.
(a) Create a vector of times (working days) for the stock prices, between 1991.496 and
1998.646 with increments of 1/260.
(b) Reproduce Figure 1.10. [Hint: Use a dictionary to map column names (stock indices)
to colors.]
18
Exercises
1991
1992
1993
1994
1995
1996
1997
1998
1999
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
DAX
SMI
CAC
FTSE
Figure 1.10: Closing stock indices for various European stock markets.
7. Consider the KASANDR data set from the UCI Machine Learning

 Repository, which can
be downloaded from
https://archive.ics.uci.edu/ml/machine-learning-databases/00385/de
.tar.bz2.
This archive file has a size of 900Mb, so it may take a while to download. Uncompressing
the file (e.g., via 7-Zip) yields a directory de containing two large CSV files: test_de.csv
and train_de.csv, with sizes 372Mb and 3Gb, respectively. Such large data files can still
be processed efficiently in pandas, provided there is enough memory. The files contain
records of user information from Kelkoo web logs in Germany as well as meta-data on
users, offers, and merchants. The data sets have 7 attributes and 1919561 and 15844717
rows, respectively. The data sets are anonymized via hex strings.
(a) Load train_de.csv into a pandas DataFrame object de, using
read_csv('train_de.csv',␣delimiter␣=␣'\t').
If not enough memory is available, load test_de.csv instead. Note that entries are
separated here by tabs, not commas. Time how long it takes for the file to load, using
the time package. (It took 38 seconds for train_de.csv to load on one of our
computers.)
(b) How many unique users and merchants are in this data set?
8. Visualizing data involving more than two features requires careful design, which is often
more of an art than a science.
(a) Go to Vincent Arel-Bundocks’s website (URL given in Section 1.1) and read the
Orange data set into a pandas DataFrame object called orange. Remove its first
(unnamed) column.
(b) The data set contains the circumferences of 5 orange trees at various stages in their
development. Find the names of the features.
(c) In Python, import seaborn and visualize the growth curves (circumference against
age) of the trees, using the regplot and FacetGrid methods.
CHAPTER2
STATISTICAL LEARNING
The purpose of this chapter is to introduce the reader to some common concepts
and themes in statistical learning. We discuss the difference between supervised and
unsupervised learning, and how we can assess the predictive performance of super

vised
learning. We also examine the central role that the linear and Gaussian properties play
in the modeling of data. We conclude with a section on Bayesian learning. The required
probability and statistics background is given in Appendix C.
2.1
Introduction
Although structuring and visualizing data are important aspects of data science, the main
challenge lies in the mathematical analysis of the data. When the goal is to interpret the
model and quantify the uncertainty in the data, this analysis is usually referred to as stat-
istical learning. In contrast, when the emphasis is on making predictions using large-scale
statistical
learning
data, then it is common to speak about machine learning or data mining.
machine
learning
data mining
There are two major goals for modeling data: 1) to accurately predict some future
quantity of interest, given some observed data, and 2) to discover unusual or interesting
patterns in the data. To achieve these goals, one must rely on knowledge from three im-
portant pillars of the mathematical sciences.
Function approximation. Building a mathematical model for data usually means under-
standing how one data variable depends on another data variable. The most natural
way to represent the relationship between variables is via a mathematical function or
map. We usually assume that this mathematical function is not completely known,
but can be approximated well given enough computing power and data. Thus, data
scientists have to understand how best to approximate and represent functions using
the least amount of computer processing and memory.
Optimization. Given a class of mathematical models, we wish to find the best possible
model in that class. This requires some kind of efficient search or optimization pro-
cedure. The optimization step can be viewed as a process of fitting or calibrating
a function to observed data. This step usually requires knowledge of optimization
algorithms and efficient computer coding or programming.
19
2

0
Supervised and Unsupervised Learning
Probability and Statistics. In general, the data used to fit the model is viewed as a realiz-
ation of a random process or numerical vector, whose probability law determines the
accuracy with which we can predict future observations. Thus, in order to quantify
the uncertainty inherent in making predictions about the future, and the sources of er-
ror in the model, data scientists need a firm grasp of probability theory and statistical
inference.
2.2
Supervised and Unsupervised Learning
Given an input or feature
feature
vector x, one of the main goals of machine learning is to predict
an output or response
response
variable y. For example, x could be a digitized signature and y a
binary variable that indicates whether the signature is genuine or false. Another example is
where x represents the weight and smoking habits of an expecting mother and y the birth
weight of the baby. The data science attempt at this prediction is encoded in a mathematical
function g, called the prediction function
prediction
function
, which takes as an input x and outputs a guess g(x)
for y (denoted by by, for example). In a sense, g encompasses all the information about the
relationship between the variables x and y, excluding the effects of chance and randomness
in nature.
In regression problems, the response variable y can take any real value. In contrast,
regression
when y can only lie in a finite set, say y ∈{0, . . . , c −1}, then predicting y is conceptually
the same as classifying the input x into one of c categories, and so prediction becomes a
classification
classification
problem.
We can measure the accuracy of a prediction by with respect to a given response y by
using some loss function
loss function
Loss(y,by). In a regression setting the usual choice is the squared-
error loss (y−by)2. In the case of classification, the zero–one (also written 0–1) loss function
Loss(y,by) = 1{y , by} is often used, which incurs a loss of 1 whenever the 

predicted class
by is not equal to the class y. Later on in this book, we will encounter various other useful
loss functions, such as the cross-entropy and hinge loss functions (see, e.g., Chapter 7).
The word error is often used as a measure of distance between a “true” object y and
some approximation by thereof. If y is real-valued, the absolute error |y −by| and the
squared error (y−by)2 are both well-established error concepts, as are the norm ∥y−by∥
and squared norm ∥y−by∥2 for vectors. The squared error (y−by)2 is just one example
of a loss function.
It is unlikely that any mathematical function g will be able to make accurate predictions
for all possible pairs (x, y) one may encounter in Nature. One reason for this is that, even
with the same input x, the output y may be different, depending on chance circumstances
or randomness. For this reason, we adopt a probabilistic approach and assume that each
pair (x, y) is the outcome of a random pair (X, Y) that has some joint probability density
f(x, y). We then assess the predictive performance via the expected loss, usually called the
risk
risk
, for g:
ℓ(g) = E Loss(Y, g(X)).
(2.1)
For example, in the classification case with zero–one loss function the risk is equal to the
Statistical Learning
21
probability of incorrect classification: ℓ(g) = P[Y , g(X)]. In this context, the prediction
function g is called a classifier
classifier
. Given the distribution of (X, Y) and any loss function, we
can in principle find the best possible g∗:= argming E Loss(Y, g(X)) that yields the smallest
risk ℓ∗:= ℓ(g∗). We will see in Chapter 7 that in the classification case with y ∈{0, . . . , c−1}
☞251
and ℓ(g) = P[Y , g(X)], we have
g∗(x) = argmax
y∈{0,...,c−1}
f(y | x),
where f(y | x) = P[Y = y | X = x] is the conditional probability of Y = y given X = x.
As already mentioned, for regression the most widely-used loss function is the squared-
error loss. In this setting, the optimal prediction function g∗is often called the reg

ression
function. The following theorem specifies its exact form.
regression
function
Theorem 2.1: Optimal Prediction Function for Squared-Error Loss
For the squared-error loss Loss(y,by) = (y −by)2, the optimal prediction function g∗is
equal to the conditional expectation of Y given X = x:
g∗(x) = E[Y | X = x].
Proof: Let g∗(x) = E[Y | X = x]. For any function g, the squared-error risk satisfies
E(Y −g(X))2 = E[(Y −g∗(X) + g∗(X) −g(X))2]
= E(Y −g∗(X))2 + 2E[(Y −g∗(X))(g∗(X) −g(X))] + E(g∗(X) −g(X))2
⩾E(Y −g∗(X))2 + 2E[(Y −g∗(X))(g∗(X) −g(X))]
= E(Y −g∗(X))2 + 2E {(g∗(X) −g(X))E[Y −g∗(X) | X]} .
In the last equation we used the tower property. By the definition of the conditional expect-
☞431
ation, we have E[Y −g∗(X) | X] = 0. It follows that E(Y −g(X))2 ⩾E(Y −g∗(X))2, showing
that g∗yields the smallest squared-error risk.
□
One consequence of Theorem 2.1 is that, conditional on X = x, the (random) response
Y can be written as
Y = g∗(x) + ε(x),
(2.2)
where ε(x) can be viewed as the random deviation of the response from its conditional
mean at x. This random deviation satisfies E ε(x) = 0. Further, the conditional variance of
the response Y at x can be written as Var ε(x) = v2(x) for some unknown positive function
v. Note that, in general, the probability distribution of ε(x) is unspecified.
Since, the optimal prediction function g∗depends on the typically unknown joint distri-
bution of (X, Y), it is not available in practice. Instead, all that we have available is a finite
number of (usually) independent realizations from the joint density f(x, y). We denote this
sample by T = {(X1, Y1), . . . , (Xn, Yn)} and call it the training set
training set
(T is a mnemonic for
training) with n examples. It will be important to distinguish between a random training
set T and its (deterministic) outcome {(x1, y1), . . . , (xn, yn)}. We will use the notation τ for
the latter. We will also add the subscript n in τn when we wish to emphasize the size of the
training set.
22
Supe

rvised and Unsupervised Learning
Our goal is thus to “learn” the unknown g∗using the n examples in the training set T .
Let us denote by gT the best (by some criterion) approximation for g∗that we can construct
from T . Note that gT is a random function. A particular outcome is denoted by gτ. It is
often useful to think of a teacher–learner metaphor, whereby the function gT is a learner
learner
who learns the unknown functional relationship g∗: x 7→y from the training data T . We
can imagine a “teacher” who provides n examples of the true relationship between the
output Yi and the input Xi for i = 1, . . . , n, and thus “trains” the learner gT to predict the
output of a new input X, for which the correct output Y is not provided by the teacher (is
unknown).
The above setting is called supervised learning
supervised
learning
, because one tries to learn the functional
relationship between the feature vector x and response y in the presence of a teacher who
provides n examples. It is common to speak of “explaining” or predicting y on the basis of
x, where x is a vector of explanatory variables
explanatory
variables
.
An example of supervised learning is email spam detection. The goal is to train the
learner gT to accurately predict whether any future email, as represented by the feature
vector x, is spam or not. The training data consists of the feature vectors of a number
of different email examples as well as the corresponding labels (spam or not spam). For
instance, a feature vector could consist of the number of times sales-pitch words like “free”,
“sale”, or “miss out” occur within a given email.
As seen from the above discussion, most questions of interest in supervised learning
can be answered if we know the conditional pdf f(y | x), because we can then in principle
work out the function value g∗(x).
In contrast, unsupervised learning
unsupervised
learning
makes no distinction between response and explan-
atory variables, and the objective is simply to learn th

e structure of the unknown distribu-
tion of the data. In other words, we need to learn f(x). In this case the guess g(x) is an
approximation of f(x) and the risk is of the form
ℓ(g) = E Loss( f(X), g(X)).
An example of unsupervised learning is when we wish to analyze the purchasing be-
haviors of the customers of a grocery shop that has a total of, say, a hundred items on sale.
A feature vector here could be a binary vector x ∈{0, 1}100 representing the items bought
by a customer on a visit to the shop (a 1 in the k-th position if a customer bought item
k ∈{1, . . . , 100} and a 0 otherwise). Based on a training set τ = {x1, . . . , xn}, we wish to
find any interesting or unusual purchasing patterns. In general, it is difficult to know if an
unsupervised learner is doing a good job, because there is no teacher to provide examples
of accurate predictions.
The main methodologies for unsupervised learning include clustering, principal com-
ponent analysis, and kernel density estimation, which will be discussed in Chapter 4.
☞121
In the next three sections we will focus on supervised learning. The main super-
vised learning methodologies are regression and classification, to be discussed in detail in
Chapters 5 and 7. More advanced supervised learning techniques, including reproducing
☞167
☞251
kernel Hilbert spaces, tree methods, and deep learning, will be discussed in Chapters 6, 8,
and 9.
Statistical Learning
23
2.3
Training and Test Loss
Given an arbitrary prediction function g, it is typically not possible to compute its risk ℓ(g)
in (2.1). However, using the training sample T , we can approximate ℓ(g) via the empirical
(sample average) risk
ℓT(g) = 1
n
n
X
i=1
Loss(Yi, g(Xi)),
(2.3)
which we call the training loss
training loss
. The training loss is thus an unbiased estimator of the risk
(the expected loss) for a prediction function g, based on the training data.
To approximate the optimal prediction function g∗(the minimizer of the risk ℓ(g)) we
first select a 

suitable collection of approximating functions G and then take our learner to
be the function in G that minimizes the training loss; that is,
gG
T = argmin
g∈G
ℓT(g).
(2.4)
For example, the simplest and most useful G is the set of linear functions of x; that is, the
set of all functions g : x 7→β⊤x for some real-valued vector β.
We suppress the superscript G when it is clear which function class is used. Note that
minimizing the training loss over all possible functions g (rather than over all g ∈G) does
not lead to a meaningful optimization problem, as any function g for which g(Xi) = Yi for
all i gives minimal training loss. In particular, for a squared-error loss, the training loss will
be 0. Unfortunately, such functions have a poor ability to predict new (that is, independent
from T ) pairs of data. This poor generalization performance is called overfitting
overfitting
.
By choosing g a function that predicts the training data exactly (and is, for example,
0 otherwise), the squared-error training loss is zero. Minimizing the training loss is
not the ultimate goal!
The prediction accuracy of new pairs of data is measured by the generalization risk
generalization
risk
of
the learner. For a fixed training set τ it is defined as
ℓ(gG
τ ) = E Loss(Y, gG
τ (X)),
(2.5)
where (X, Y) is distributed according to f(x, y). In the discrete case the generalization risk
is therefore: ℓ(gG
τ ) = P
x,y Loss(y, gG
τ (x)) f(x, y) (replace the sum with an integral for the
continuous case). The situation is illustrated in Figure 2.1, where the distribution of (X, Y)
is indicated by the red dots. The training set (points in the shaded regions) determines a
fixed prediction function shown as a straight line. Three possible outcomes of (X, Y) are
shown (black dots). The amount of loss for each point is shown as the length of the dashed
lines. The generalization risk is the average loss over all possible pairs (x, y), weighted by
the corresponding f(x, y).
24
Training and Test Loss
x
x


x
y
y
y
Figure 2.1: The generalization risk for a fixed training set is the weighted-average loss over
all possible pairs (x, y).
For a random training set T , the generalization risk is thus a random variable that
depends on T (and G). If we average the generalization risk over all possible instances of
T , we obtain the expected generalization risk
expected
generalization
risk
:
E ℓ(gG
T) = E Loss(Y, gG
T(X)),
(2.6)
where (X, Y) in the expectation above is independent of T . In the discrete case, we have
Eℓ(gG
T) = P
x,y,x1,y1,...,xn,yn Loss(y, gG
τ (x)) f(x, y) f(x1, y1) · · · f(xn, yn). Figure 2.2 gives an il-
lustration.
y
x
y
y
x
x
Figure 2.2: The expected generalization risk is the weighted-average loss over all possible
pairs (x, y) and over all training sets.
For any outcome τ of the training data, we can estimate the generalization risk without
bias by taking the sample average
ℓT ′(gG
τ ) := 1
n′
n′
X
i=1
Loss(Y′
i , gG
τ (X′
i)),
(2.7)
where {(X′
1, Y′
1), . . . , (X′
n′, Y′
n′)} =: T ′ is a so-called test sample
test sample
. The test sample is com-
pletely separate from T , but is drawn in the same way as T ; that is, via independent draws
from f(x, y), for some sample size n′. We call the estimator (2.7) the test loss
test loss
. For a ran-
dom training set T we can define ℓT ′(gG
T) similarly. It is then crucial to assume that T is
independent of T ′. Table 2.1 summarizes the main definitions and notation for supervised
learning.
Statistical Learning
25
Table 2.1: Summary of definitions for supervised learning.
x
Fixed explanatory (feature) vector.
X
Random explanatory (feature) vector.
y
Fixed (real-valued) response.
Y
Random response.
f(x, y)
Joint pdf of X and Y, evaluated at (x, y).
f(y | x)
Conditional pdf of Y given X = x, evaluated at y.
τ or τn
Fixed training data {(xi, yi), i = 1, . . . , n}.
T or Tn
Random training data {(Xi, Yi), i = 1, . . . , n}.
X
Matrix of explanatory variables, with n rows x⊤
i , i = 1, . . . , n
and dim(x) feature c

olumns; one of the features may be the
constant 1.
y
Vector of response variables (y1, . . . , yn)⊤.
g
Prediction (guess) function.
Loss(y,by)
Loss incurred when predicting response y withby.
ℓ(g)
Risk for prediction function g; that is, E Loss(Y, g(X)).
g∗
Optimal prediction function; that is, argming ℓ(g).
gG
Optimal prediction function in function class G; that is,
argming∈G ℓ(g).
ℓτ(g)
Training loss for prediction function g; that is, the sample av-
erage estimate of ℓ(g) based on a fixed training sample τ.
ℓT(g)
The same as ℓτ(g), but now for a random training sample T .
gG
τ or gτ
The learner: argming∈G ℓτ(g). That is, the optimal prediction
function based on a fixed training set τ and function class G.
We suppress the superscript G if the function class is implicit.
gG
T or gT
The learner, where we have replaced τ with a random training
set T .
To compare the predictive performance of various learners in the function class G, as
measured by the test loss, we can use the same fixed training set τ and test set τ′ for all
learners. When there is an abundance of data, the “overall” data set is usually (randomly)
divided into a training and test set, as depicted in Figure 2.3. We then use the training data
to construct various learners gG1
τ , gG2
τ , . . ., and use the test data to select the best (with the
smallest test loss) among these learners. In this context the test set is called the validation
set
validation set
. Once the best learner has been chosen, a third “test” set can be used to assess the
predictive performance of the best learner. The training, validation, and test sets can again
be obtained from the overall data set via a random allocation. When the overall data set
is of modest size, it is customary to perform the validation phase (model selection) on the
training set only, using cross-validation. This is the topic of Section 2.5.2.
☞
38
26
Training and Test Loss

	

	


	


	
	
F

igure 2.3: Statistical learning algorithms often require the data to be divided into training
and test data. If the latter is used for model selection, a third set is needed for testing the
performance of the selected model.
We next consider a concrete example that illustrates the concepts introduced so far.
Example 2.1 (Polynomial Regression) In what follows, it will appear that we have ar-
bitrarily replaced the symbols x, g, G with u, h, H, respectively. The reason for this switch
of notation will become clear at the end of the example.
The data (depicted as dots) in Figure 2.4 are n = 100 points (ui, yi), i = 1, . . . , n drawn
from iid random points (Ui, Yi), i = 1, . . . , n, where the {Ui} are uniformly distributed on
the interval (0, 1) and, given Ui = ui, the random variable Yi has a normal distribution with
expectation 10 −140ui + 400u2
i −250u3
i and variance ℓ∗= 25. This is an example of a
polynomial regression model
polynomial
regression
model
. Using a squared-error loss, the optimal prediction function
h∗(u) = E[Y | U = u] is thus
h∗(u) = 10 −140u + 400u2 −250u3,
which is depicted by the dashed curve in Figure 2.4.
0.0
0.2
0.4
0.6
0.8
1.0
u
10
0
10
20
30
40
h * (u)
data points
true
Figure 2.4: Training data and the optimal polynomial prediction function h∗.
Statistical Learning
27
To obtain a good estimate of h∗(u) based on the training set τ = {(ui, yi), i = 1, . . . , n},
we minimize the outcome of the training loss (2.3):
ℓτ(h) = 1
n
n
X
i=1
(yi −h(ui))2,
(2.8)
over a suitable set H of candidate functions. Let us take the set Hp of polynomial functions
in u of order p −1:
h(u) := β1 + β2u + β3u2 + · · · + βpup−1
(2.9)
for p = 1, 2, . . . and parameter vector β = [β1, β2, . . . , βp]⊤. This function class contains the
best possible h∗(u) = E[Y | U = u] for p ⩾4. Note that optimization over Hp is a parametric
optimization problem, in that we need to find the best β. Optimization of (2.8) over Hp is
not straightforward, unless we notice that (2.9) is 

a linear function in β. In particular, if we
map each feature u to a feature vector x = [1, u, u2, . . . , up−1]⊤, then the right-hand side of
(2.9) can be written as the function
g(x) = x⊤β,
which is linear in x (as well as β). The optimal h∗(u) in Hp for p ⩾4 then corresponds
to the function g∗(x) = x⊤β∗in the set Gp of linear functions from Rp to R, where β∗=
[10, −140, 400, −250, 0, . . . , 0]⊤. Thus, instead of working with the set Hp of polynomial
functions we may prefer to work with the set Gp of linear functions. This brings us to a
very important idea in statistical learning:
Expand the feature space to obtain a linear prediction function.
Let us now reformulate the learning problem in terms of the new explanatory (feature)
variables xi = [1, ui, u2
i , . . . , up−1
i
]⊤, i = 1, . . . , n. It will be convenient to arrange these
feature vectors into a matrix X with rows x⊤
1 , . . . , x⊤
n :
X =

1
u1
u2
1
· · ·
up−1
1
1
u2
u2
2
· · ·
up−1
2
...
...
...
...
...
1
un
u2
n
· · ·
up−1
n

.
(2.10)
Collecting the responses {yi} into a column vector y, the training loss (2.3) can now be
written compactly as
1
n ∥y −Xβ∥2.
(2.11)
To find the optimal learner (2.4) in the class Gp we need to find the minimizer of (2.11):
bβ = argmin
β
∥y −Xβ∥2,
(2.12)
which is called the ordinary least-squares
ordinary
least-squares
solution. As is illustrated in Figure 2.5, to findbβ,
we choose Xbβ to be equal to the orthogonal projection of y onto the linear space spanned
by the columns of the matrix X; that is, Xbβ = Py, where P is the projection matrix
projection
matrix
.
28
Training and Test Loss
Span(X)
Xβ
Xbβ
y
Figure 2.5: Xbβ is the orthogonal projection of y onto the linear space spanned by the
columns of the matrix X.
According to Theorem A.4, the projection matrix is given by
☞362
P = X X+,
(2.13)
where the p × n matrix X+ in (2.13) is the pseudo-inverse of X. If X happens to be of full
☞360
pseudo-inverse
column rank (so that none of the

 columns can be expressed as a linear combination of the
☞356
other columns), then X+ = (X⊤X)−1X⊤.
In any case, from Xbβ = Py and PX = X, we can see that bβ satisfies the normal
equations
normal
equations
:
X⊤Xβ = X⊤Py = (PX)⊤y = X⊤y.
(2.14)
This is a set of linear equations, which can be solved very fast and whose solution can be
written explicitly as:
bβ = X+y.
(2.15)
Figure 2.6 shows the trained learners for various values of p:
h
Hp
τ (u) = g
Gp
τ (x) = x⊤bβ
0.0
0.2
0.4
0.6
0.8
1.0
u
10
0
10
20
30
40
h
p(u)
data points
true
p = 2, underfit
p = 4, correct
p = 16, overfit
Figure 2.6: Training data with fitted curves for p = 2, 4, and 16. The true cubic polynomial
curve for p = 4 is also plotted (dashed line).
Statistical Learning
29
We see that for p = 16 the fitted curve lies closer to the data points, but is further away
from the dashed true polynomial curve, indicating that we overfit. The choice p = 4 (the
true cubic polynomial) is much better than p = 16, or indeed p = 2 (straight line).
Each function class Gp gives a different learner g
Gp
τ , p = 1, 2, . . .. To assess which is
better, we should not simply take the one that gives the smallest training loss. We can
always get a zero training loss by taking p = n, because for any set of n points there exists
a polynomial of degree n −1 that interpolates all points!
Instead, we assess the predictive performance of the learners using the test loss (2.7),
computed from a test data set. If we collect all n′ test feature vectors in a matrix X′ and
the corresponding test responses in a vector y′, then, similar to (2.11), the test loss can be
written compactly as
ℓτ′(g
Gp
τ ) = 1
n′ ∥y′ −X′bβ∥2,
where bβ is given by (2.15), using the training data.
Figure 2.7 shows a plot of the test loss against the number of parameters in the vector
β; that is, p. The graph has a characteristic “bath-tub” shape and is at its lowest for p = 4,
correctly identifying the polynomial order 3 for the true model. Note that the test loss,

 as
an estimate for the generalization risk (2.7), becomes numerically unreliable after p = 16
(the graph goes down, where it should go up). The reader may check that the graph for
the training loss exhibits a similar numerical instability for large p, and in fact fails to
numerically decrease to 0 for large p, contrary to what it should do in theory. The numerical
problems arise from the fact that for large p the columns of the (Vandermonde) matrix X
are of vastly different magnitudes and so floating point errors quickly become very large.
Finally, observe that the lower bound for the test loss is here around 21, which corres-
ponds to an estimate of the minimal (squared-error) risk ℓ∗= 25.
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15 16 17 18
Number of parameters p
20
40
60
80
100
120
140
160
Test loss
Figure 2.7: Test loss as function of the number of parameters p of the model.
This script shows how the training data were generated and plotted in Python:
30
Training and Test Loss
polyreg1.py
import numpy as np
from numpy.random import rand , randn
from numpy.linalg import norm , solve
import matplotlib.pyplot as plt
def generate_data(beta , sig, n):
u = np.random.rand(n, 1)
y = (u ** np.arange(0, 4)) @ beta + sig * np.random.randn(n, 1)
return u, y
np.random.seed(12)
beta = np.array([[10, -140, 400, -250]]).T
n = 100
sig = 5
u, y = generate_data(beta , sig, n)
xx = np.arange(np.min(u), np.max(u)+5e-3, 5e-3)
yy = np.polyval(np.flip(beta), xx)
plt.plot(u, y, '.', markersize=8)
plt.plot(xx, yy, '--',linewidth=3)
plt.xlabel(r'$u$')
plt.ylabel(r'$h^*(u)$')
plt.legend(['data points','true'])
plt.show()
The following code, which imports the code above, fits polynomial models with p =
1, . . . , K = 18 parameters to the training data and plots a selection of fitted curves, as
shown in Figure 2.6.
polyreg2.py
from polyreg1 import *
max_p = 18
p_range = np.arange(1, max_p + 1, 1)
X = np.ones((n, 1))
betahat , trainloss = {}, {}
for p in p_range:
# p is the number of parameters
if p

 > 1:
X = np.hstack((X, u**(p-1)))
# add column to matrix
betahat[p] = solve(X.T @ X, X.T @ y)
trainloss[p] = (norm(y - X @ betahat[p])**2/n)
p = [2, 4, 16]
# select three curves
#replot the points and true line and store in the list "plots"
plots = [plt.plot(u, y, 'k.', markersize=8)[0],
plt.plot(xx, yy, 'k--',linewidth=3)[0]]
# add the three curves
for i in p:
yy = np.polyval(np.flip(betahat[i]), xx)
plots.append(plt.plot(xx, yy)[0])
Statistical Learning
31
plt.xlabel(r'$u$')
plt.ylabel(r'$h^{\mathcal{H}_p}_{\tau}(u)$')
plt.legend(plots ,('data points', 'true','$p=2$, underfit',
'$p=4$, correct','$p=16$, overfit'))
plt.savefig('polyfitpy.pdf',format='pdf')
plt.show()
The last code snippet which imports the previous code, generates the test data and plots the
graph of the test loss, as shown in Figure 2.7.
polyreg3.py
from polyreg2 import *
# generate test data
u_test, y_test = generate_data(beta, sig, n)
MSE = []
X_test = np.ones((n, 1))
for p in p_range:
if p > 1:
X_test = np.hstack((X_test, u_test**(p-1)))
y_hat = X_test @ betahat[p]
# predictions
MSE.append(np.sum((y_test - y_hat)**2/n))
plt.plot(p_range , MSE, 'b', p_range , MSE, 'bo')
plt.xticks(ticks=p_range)
plt.xlabel('Number of parameters $p$')
plt.ylabel('Test loss')
2.4
Tradeoffs in Statistical Learning
The art of machine learning in the supervised case is to make the generalization risk (2.5)
or expected generalization risk (2.6) as small as possible, while using as few computational
resources as possible. In pursuing this goal, a suitable class G of prediction functions has
to be chosen. This choice is driven by various factors, such as
• the complexity of the class (e.g., is it rich enough to adequately approximate, or even
contain, the optimal prediction function g∗?),
• the ease of training the learner via the optimization program (2.4),
• how accurately the training loss (2.3) estimates the risk (2.1) within class G,
• the feature types (categorical, continuous, etc.).
32
Tradeoffs in Statistical 

Learning
As a result, the choice of a suitable function class G usually involves a tradeoff between
conflicting factors. For example, a learner from a simple class G can be trained very
quickly, but may not approximate g∗very well, whereas a learner from a rich class G
that contains g∗may require a lot of computing resources to train.
To better understand the relation between model complexity, computational simplicity,
and estimation accuracy, it is useful to decompose the generalization risk into several parts,
so that the tradeoffs between these parts can be studied. We will consider two such decom-
positions: the approximation–estimation tradeoff and the bias–variance tradeoff.
We can decompose the generalization risk (2.5) into the following three components:
ℓ(gG
τ ) =
ℓ∗
|{z}
irreducible risk
+
ℓ(gG) −ℓ∗
|     {z     }
approximation error
+ ℓ(gG
τ ) −ℓ(gG)
|          {z          }
statistical error
,
(2.16)
where ℓ∗:= ℓ(g∗) is the irreducible risk
irreducible risk
and gG := argming∈G ℓ(g) is the best learner within
class G. No learner can predict a new response with a smaller risk than ℓ∗.
The second component is the approximation error
approximation
error
; it measures the difference between
the irreducible risk and the best possible risk that can be obtained by selecting the best
prediction function in the selected class of functions G. Determining a suitable class G and
minimizing ℓ(g) over this class is purely a problem of numerical and functional analysis,
as the training data τ are not present. For a fixed G that does not contain the optimal g∗, the
approximation error cannot be made arbitrarily small and may be the dominant component
in the generalization risk. The only way to reduce the approximation error is by expanding
the class G to include a larger set of possible functions.
The third component is the statistical (estimation) error
statistical
(estimation)
error
. It depends on the training
set τ and, in particular, on how well the learner gG
τ es

timates the best possible prediction
function, gG, within class G. For any sensible estimator this error should decay to zero (in
probability or expectation) as the training size tends to infinity.
☞439
The approximation–estimation tradeoff
approximation–
estimation
tradeoff
pits two competing demands against each
other. The first is that the class G has to be simple enough so that the statistical error is
not too large. The second is that the class G has to be rich enough to ensure a small approx-
imation error. Thus, there is a tradeoff between the approximation and estimation errors.
For the special case of the squared-error loss, the generalization risk is equal to ℓ(gG
τ ) =
E(Y −gG
τ (X))2; that is, the expected squared error1 between the predicted value gG
τ (X)
and the response Y. Recall that in this case the optimal prediction function is given by
g∗(x) = E[Y | X = x]. The decomposition (2.16) can now be interpreted as follows.
1. The first component, ℓ∗= E(Y −g∗(X))2, is the irreducible error, as no prediction
function will yield a smaller expected squared error.
2. The second component, the approximation error ℓ(gG) −ℓ(g∗), is equal to E(gG(X) −
g∗(X))2. We leave the proof (which is similar to that of Theorem 2.1) as an exercise;
see Exercise 2. Thus, the approximation error (defined as a risk difference) can here
be interpreted as the expected squared error between the optimal predicted value and
the optimal predicted value within the class G.
3. For the third component, the statistical error, ℓ(gG
τ ) −ℓ(gG) there is no direct inter-
pretation as an expected squared error unless G is the class of linear functions; that
1Colloquially called mean squared error.
Statistical Learning
33
is, g(x) = x⊤β for some vector β. In this case we can write (see Exercise 3) the
statistical error as ℓ(gG
τ ) −ℓ(gG) = E(gG
τ (X) −gG(X))2.
Thus, when using a squared-error loss, the generalization risk for a linear class G can
be decomposed as:
ℓ(gG
τ ) = E(gG
τ (X) −Y)2 =

 ℓ∗+ E(gG(X) −g∗(X))2
|                 {z                 }
approximation error
+ E(gG
τ (X) −gG(X))2
|                  {z                  }
statistical error
.
(2.17)
Note that in this decomposition the statistical error is the only term that depends on the
training set.
Example 2.2 (Polynomial Regression (cont.)) We continue Example 2.1. Here G =
Gp is the class of linear functions of x = [1, u, u2, . . . , up−1]⊤, and g∗(x) = x⊤β∗. Condi-
tional on X = x we have that Y = g∗(x) + ε(x), with ε(x) ∼N(0, ℓ∗), where ℓ∗= E(Y −
g∗(X))2 = 25 is the irreducible error. We wish to understand how the approximation and
statistical errors behave as we change the complexity parameter p.
First, we consider the approximation error. Any function g ∈Gp can be written as
g(x) = h(u) = β1 + β2u + · · · + βpup−1 = [1, u, . . . , up−1] β,
and so g(X) is distributed as [1, U, . . . , U p−1]β, where U ∼U(0, 1). Similarly, g∗(X) is
distributed as [1, U, U2, U3]β∗. It follows that an expression for the approximation error
is:
R 1
0

[1, u, . . . , up−1] β −[1, u, u2, u3] β∗2 du. To minimize this error, we set the gradient
with respect to β to zero and obtain the p linear equations
☞397
R 1
0

[1, u, . . . , up−1] β −[1, u, u2, u3] β∗
du = 0,
R 1
0

[1, u, . . . , up−1] β −[1, u, u2, u3 ]β∗
u du = 0,
...
R 1
0

[1, u, . . . , up−1] β −[1, u, u2, u3] β∗
up−1du = 0.
Let
Hp =
Z 1
0
[1, u, . . . , up−1]⊤[1, u, . . . , up−1] du
be the p × p Hilbert matrix
Hilbert matrix
, which has (i, j)-th entry given by
R 1
0 ui+j−2 du = 1/(i + j −1).
Then, the above system of linear equations can be written as Hpβ = eHβ∗, where eH is the
p × 4 upper left sub-block of Hep and ep = max{p, 4}. The solution, which we denote by βp,
is:
βp =

65
6 ,
p = 1,
[−20
3 , 35]⊤,
p = 2,
[−5
2, 10, 25]⊤,
p = 3,
[10, −140, 400, −250, 0, . . . , 0]⊤,
p ⩾4.
(2.18)
34
Tradeoffs in Statistical Learning
Hence, the approximation error E

gGp(X) −g∗(X)
2 is given by
Z 1
0

[1, u, . . . , up−1] βp −[

1, u, u2, u3] β∗2 du =

32225
252 ≈127.9,
p = 1,
1625
63 ≈25.8,
p = 2,
625
28 ≈22.3,
p = 3,
0,
p ⩾4.
(2.19)
Notice how the approximation error becomes smaller as p increases. In this particular
example the approximation error is in fact zero for p ⩾4. In general, as the class of ap-
proximating functions G becomes more complex, the approximation error goes down.
Next, we illustrate the typical behavior of the statistical error. Since gτ(x) = x⊤bβ, the
statistical error can be written as
Z 1
0

[1, . . . , up−1](bβ −βp)
2 du = (bβ −βp)⊤Hp(bβ −βp).
(2.20)
Figure 2.8 illustrates the decomposition (2.17) of the generalization risk for the same train-
ing set that was used to compute the test loss in Figure 2.7. Recall that test loss gives an
estimate of the generalization risk, using independent test data. Comparing the two figures,
we see that in this case the two match closely. The global minimum of the statistical error is
approximately 0.28, with minimizer p = 4. Since the approximation error is monotonically
decreasing to zero, p = 4 is also the global minimizer of the generalization risk.
0
2
4
6
8
10
12
14
16
18
0
50
100
150
approximation error
statistical error
irreducible error
generalization risk
Figure 2.8: The generalization risk for a particular training set is the sum of the irreducible
error, the approximation error, and the statistical error. The approximation error decreases
to zero as p increases, whereas the statistical error has a tendency to increase after p = 4.
Note that the statistical error depends on the estimate bβ, which in its turn depends on
the training set τ. We can obtain a better understanding of the statistical error by consid-
ering its expected behavior; that is, averaged over many training sets. This is explored in
Exercise 11.
Using again a squared-error loss, a second decomposition (for general G) starts from
ℓ(gG
τ ) = ℓ∗+ ℓ(gG
τ ) −ℓ(g∗),
Statistical Learning
35
where the statistical error and approximation

 error are combined. Using similar reasoning
as in the proof of Theorem 2.1, we have
ℓ(gG
τ ) = E(gG
τ (X) −Y)2 = ℓ∗+ E

gG
τ (X) −g∗(X)
2 = ℓ∗+ ED2(X, τ),
where D(x, τ) := gG
τ (x) −g∗(x). Now consider the random variable D(x, T ) for a random
training set T . The expectation of its square is:
E

gG
T(x) −g∗(x)
2 = ED2(x, T ) = (ED(x, T ))2 + Var D(x, T )
= (EgG
T (x) −g∗(x))2
|                 {z                 }
pointwise squared bias
+ Var gG
T(x)
|     {z     }
pointwise variance
.
(2.21)
If we view the learner gG
T(x) as a function of a random training set, then the pointwise
squared bias
pointwise
squared bias
term is a measure for how close gG
T(x) is on average to the true g∗(x),
whereas the pointwise variance term measures the deviation of gG
T(x) from its expected
pointwise
variance
value EgG
T(x). The squared bias can be reduced by making the class of functions G more
complex. However, decreasing the bias by increasing the complexity often leads to an in-
crease in the variance term. We are thus seeking learners that provide an optimal balance
between the bias and variance, as expressed via a minimal generalization risk. This is called
the bias–variance tradeoff
bias–variance
tradeoff
.
Note that the expected generalization risk (2.6) can be written as ℓ∗+ED2(X, T ), where
X and T are independent. It therefore decomposes as
E ℓ(gG
T) = ℓ∗+ E (E[gG
T(X) | X] −g∗(X))2
|                           {z                           }
expected squared bias
+ E[Var[gG
T(X) | X]]
|                 {z                 }
expected variance
.
(2.22)
2.5
Estimating Risk
The most straightforward way to quantify the generalization risk (2.5) is to estimate it via
the test loss (2.7). However, the generalization risk depends inherently on the training set,
and so different training sets may yield significantly different estimates. Moreover, when
there is a limited amount of data available, reserving a substantial proportion of the data
for testing rather than training ma

y be uneconomical. In this section we consider different
methods for estimating risk measures which aim to circumvent these difficulties.
2.5.1
In-Sample Risk
We mentioned that, due to the phenomenon of overfitting, the training loss of the learner,
ℓτ(gτ) (for simplicity, here we omit G from gG
τ ), is not a good estimate of the generalization
risk ℓ(gτ) of the learner. One reason for this is that we use the same data for both training
the model and assessing its risk. How should we then estimate the generalization risk or
expected generalization risk?
To simplify the analysis, suppose that we wish to estimate the average accuracy of the
predictions of the learner gτ at the n feature vectors x1, . . . , xn (these are part of the training
36
Estimating Risk
set τ). In other words, we wish to estimate the in-sample risk
in-sample risk
of the learner gτ:
ℓin(gτ) = 1
n
n
X
i=1
E Loss(Y′
i , gτ(xi)),
(2.23)
where each response Y′
i is drawn from f(y | xi), independently. Even in this simplified set-
ting, the training loss of the learner will be a poor estimate of the in-sample risk. Instead, the
proper way to assess the prediction accuracy of the learner at the feature vectors x1, . . . , xn,
is to draw new response values Y′
i ∼f(y | xi), i = 1, . . . , n, that are independent from the
responses y1, . . . , yn in the training data, and then estimate the in-sample risk of gτ via
1
n
n
X
i=1
Loss(Y′
i , gτ(xi)).
For a fixed training set τ, we can compare the training loss of the learner with the
in-sample risk. Their difference,
opτ = ℓin(gτ) −ℓτ(gτ),
is called the optimism (of the training loss), because it measures how much the training
loss underestimates (is optimistic about) the unknown in-sample risk. Mathematically, it is
simpler to work with the expected optimism
expected
optimism
:
E[opT | X1 = x1, . . . , Xn = xn] =: EX opT,
where the expectation is taken over a random training set T , conditional on Xi = xi,
i = 1, . . . , n. For ease of notation, we have abb

reviated the expected optimism to EX opT,
where EX denotes the expectation operator conditional on Xi = xi, i = 1, . . . , n. As in Ex-
ample 2.1, the feature vectors are stored as the rows of an n×p matrix X. It turns out that the
expected optimism for various loss functions can be expressed in terms of the (conditional)
covariance between the observed and predicted response.
Theorem 2.2: Expected Optimism
For the squared-error loss and 0–1 loss with 0–1 response, the expected optimism is
EX opT = 2
n
n
X
i=1
CovX(gT(xi), Yi).
(2.24)
Proof: In what follows, all expectations are taken conditional on X1 = x1, . . . , Xn = xn.
Let Yi be the response for xi and let bYi = gT(xi) be the predicted value. Note that the latter
depends on Y1, . . . , Yn. Also, let Y′
i be an independent copy of Yi for the same xi, as in
(2.23). In particular, Y′
i has the same distribution as Yi and is statistically independent of
all {Y j}, including Yi, and therefore is also independent of bYi. We have
EX opT = 1
n
n
X
i=1
EX
h
(Y′
i −bYi)2 −(Yi −bYi)2i
= 2
n
n
X
i=1
EX
h
(Yi −Y′
i )bYi
i
= 2
n
n
X
i=1

EX[YibYi] −EXYi EXbYi

= 2
n
n
X
i=1
CovX(bYi, Yi).
Statistical Learning
37
The proof for the 0–1 loss with 0–1 response is left as Exercise 4.
□
In summary, the expected optimism indicates how much, on average, the training loss
deviates from the expected in-sample risk. Since the covariance of independent random
variables is zero, the expected optimism is zero if the learner gT is statistically independent
from the responses Y1, . . . , Yn.
Example 2.3 (Polynomial Regression (cont.)) We continue Example 2.2, where the
components of the response vector Y = [Y1, . . . , Yn]⊤are independent and normally distrib-
uted with variance ℓ∗= 25 (the irreducible error) and expectations EXYi = g∗(xi) = x⊤
i β∗,
i = 1, . . . , n. Using the formula (2.15) for the least-squares estimator bβ, the expected op-
timism (2.24) is
2
n
n
X
i=1
CovX

x⊤
i bβ, Yi

= 2
ntr

CovX

Xbβ, Y

= 2
ntr  CovX
 XX

+Y, Y
= 2tr (XX+CovX (Y, Y))
n
= 2ℓ∗tr (XX+)
n
= 2ℓ∗p
n .
In the last equation we used the cyclic property of the trace (Theorem A.1): tr(XX+) =
☞357
tr(X+X) = tr(Ip), assuming that rank(X) = p. Therefore, an estimate for the in-sample risk
(2.23) is:
bℓin(gτ) = ℓτ(gτ) + 2ℓ∗p/n,
(2.25)
where we have assumed that the irreducible risk ℓ∗is known. Figure 2.9 shows that this
estimate is very close to the test loss from Figure 2.7. Hence, instead of computing the test
loss to assess the best model complexity p, we could simply have minimized the training
loss plus the correction term 2ℓ∗p/n. In practice, ℓ∗also has to be estimated somehow.
2
4
6
8
10
12
14
16
18
0
50
100
150
Figure 2.9: In-sample risk estimate bℓin(gτ) as a function of the number of parameters p of
the model. The test loss is superimposed as a blue dashed curve.
38
Estimating Risk
2.5.2
Cross-Validation
In general, for complex function classes G, it is very difficult to derive simple formulas of
the approximation and statistical errors, let alone for the generalization risk or expected
generalization risk. As we saw, when there is an abundance of data, the easiest way to
assess the generalization risk for a given training set τ is to obtain a test set τ′ and evaluate
the test loss (2.7). When a sufficiently large test set is not available but computational
☞24
resources are cheap, one can instead gain direct knowledge of the expected generalization
risk via a computationally intensive method called cross-validation
cross-validation
.
The idea is to make multiple identical copies of the data set, and to partition each copy
into different training and test sets, as illustrated in Figure 2.10. Here, there are four copies
of the data set (consisting of response and explanatory variables). Each copy is divided into
a test set (colored blue) and training set (colored pink). For each of these sets, we estimate
the model parameters using only training data and then predict the responses for the test
set. The a

verage loss between the predicted and observed responses is then a measure for
the predictive power of the model.


	













Figure 2.10: An illustration of four-fold cross-validation, representing four copies of the
same data set. The data in each copy is partitioned into a training set (pink) and a test
set (blue). The darker columns represent the response variable and the lighter ones the
explanatory variables.
In particular, suppose we partition a data set T of size n into K folds
folds
C1, . . . , CK of sizes
n1, . . . , nK (hence, n1 + · · · + nK = n). Typically nk ≈n/K, k = 1, . . . , K.
Let ℓCk be the test loss when using Ck as test data and all remaining data, denoted T−k,
as training data. Each ℓCk is an unbiased estimator of the generalization risk for training set
T−k; that is, for ℓ(gT−k).
The K-fold cross-validation
K-fold
cross-validation
loss is the weighted average of these risk estimators:
CVK =
K
X
k=1
nk
n ℓCk(gT−k)
= 1
n
K
X
k=1
X
i∈Ck
Loss(gT−k(xi), yi)
= 1
n
n
X
i=1
Loss(gT−κ(i)(xi), yi),
Statistical Learning
39
where the function κ : {1, . . . , n} 7→{1, . . . , K} indicates to which of the K folds each
of the n observations belongs. As the average is taken over varying training sets {T−k}, it
estimates the expected generalization risk E ℓ(gT), rather than the generalization risk ℓ(gτ)
for the particular training set τ.
Example 2.4 (Polynomial Regression (cont.)) For the polynomial regression ex-
ample, we can calculate a K-fold cross-validation loss with a nonrandom partitioning of the
training set using the following code, which imports the previous code for the polynomial
regression example. We omit the full plotting code.
polyregCV.py
from polyreg3 import *
K_vals = [5, 10, 100]
# number of folds
cv = np.zeros((len(K_vals), max_p))
# cv loss
X = np.ones((n, 1))
for p in p_range:
if p > 1:
X = np.hstack((X, u**(p-1)))
j = 0
for K in K_vals:
loss = []
for k in range(1, K+1):
# integer in

dices of test samples
test_ind = ((n/K)*(k-1) + np.arange(1,n/K+1)-1).astype('int')
train_ind = np.setdiff1d(np.arange(n), test_ind)
X_train , y_train = X[train_ind , :], y[train_ind , :]
X_test, y_test = X[test_ind , :], y[test_ind]
# fit model and evaluate test loss
betahat = solve(X_train.T @ X_train , X_train.T @ y_train)
loss.append(norm(y_test - X_test @ betahat) ** 2)
cv[j, p-1] = sum(loss)/n
j += 1
# basic plotting
plt.plot(p_range , cv[0, :], 'k-.')
plt.plot(p_range , cv[1, :], 'r')
plt.plot(p_range , cv[2, :], 'b--')
plt.show()
40
Modeling Data
2
4
6
8
10
12
14
16
18
Number of parameters p
50
100
150
200
250
300
K-fold cross-validation loss
K=5
K=10
K=100
Figure 2.11: K-fold cross-validation for the polynomial regression example.
Figure 2.11 shows the cross-validation loss for K ∈{5, 10, 100}. The case K = 100 cor-
responds to the leave-one-out cross-validation, which can be computed more efficiently
leave-one-out
cross-validation
using the formula in Theorem 5.1.
☞174
2.6
Modeling Data
The first step in any data analysis is to model
model
the data in one form or another. For example,
in an unsupervised learning setting with data represented by a vector x = [x1, . . . , xp]⊤, a
very general model is to assume that x is the outcome of a random vector X = [X1, . . . , Xp]⊤
with some unknown pdf f. The model can then be refined by assuming a specific form of
f.
When given a sequence of such data vectors x1, . . . , xn, one of the simplest models is to
assume that the corresponding random vectors X1, . . . , Xn are independent and identically
distributed (iid). We write
☞429
X1, . . . , Xn
iid∼f
or
X1, . . . , Xn
iid∼Dist,
to indicate that the random vectors form an iid sample from a sampling pdf f or sampling
distribution Dist. This model formalizes the notion that the knowledge about one variable
does not provide extra information about another variable. The main theoretical use of
independent data models is that the joint density of the random vectors X1, .

 . . , Xn is simply
the product of the marginal ones; see Theorem C.1. Specifically,
☞429
fX1, ...,Xn(x1, . . . , xn) = f(x1) · · · f(xn).
In most models of this kind, our approximation or model for the sampling distribution is
specified up to a small number of parameters. That is, g(x) is of the form g(x | β) which
is known up to some parameter vector β. Examples for the one-dimensional case (p = 1)
include the N(µ, σ2), Bin(n, p), and Exp(λ) distributions. See Tables C.1 and C.2 for other
☞425
Statistical Learning
41
common sampling distributions.
Typically, the parameters are unknown and must be estimated from the data. In a non-
parametric setting the whole sampling distribution would be unknown. To visualize the
underlying sampling distribution from outcomes x1, . . . , xn one can use graphical repres-
entations such as histograms, density plots, and empirical cumulative distribution func-
tions, as discussed in Chapter 1.
☞
11
If the order in which the data were collected (or their labeling) is not informative or
relevant, then the joint pdf of X1, . . . , Xn satisfies the symmetry:
fX1,...,Xn(x1, . . . , xn) = fXπ1,...,Xπn(xπ1, . . . , xπn)
(2.26)
for any permutation π1, . . . , πn of the integers 1, . . . , n. We say that the infinite sequence
X1, X2, . . . is exchangeable
exchangeable
if this permutational invariance (2.26) holds for any finite subset
of the sequence. As we shall see in Section 2.9 on Bayesian learning, it is common to
assume that the random vectors X1, . . . , Xn are a subset of an exchangeable sequence and
thus satisfy (2.26). Note that while iid random variables are exchangeable, the converse is
not necessarily true. Thus, the assumption of an exchangeable sequence of random vectors
is weaker than the assumption of iid random vectors.
Figure 2.12 illustrates the modeling tradeoffs. The keywords within the triangle repres-
ent various modeling paradigms. A few keywords have been highlighted, symbolizing their
importance in modeling. The s

pecific meaning of the keywords does not concern us here,
but the point is there are many models to choose from, depending on what assumptions are
made about the data.
Figure 2.12: Illustration of the modeling dilemma. Complex models are more generally
applicable, but may be difficult to analyze. Simple models may be highly tractable, but
may not describe the data accurately. The triangular shape signifies that there are a great
many specific models but not so many generic ones.
On the one hand, models that make few assumptions are more widely applicable, but at
the same time may not be very mathematically tractable or provide insight into the nature
of the data. On the other hand, very specific models may be easy to handle and interpret, but
42
Modeling Data
may not match the data very well. This tradeoff between the tractability and applicability of
the model is very similar to the approximation–estimation tradeoff described in Section 2.4.
In the typical unsupervised setting we have a training set τ = {x1, . . . , xn} that is viewed
as the outcome of n iid random variables X1, . . . , Xn from some unknown pdf f. The ob-
jective is then to learn or estimate f from the finite training data. To put the learning in
a similar framework as for supervised learning discussed in the preceding Sections 2.3–
2.5, we begin by specifying a class of probability density functions Gp := {g(· | θ), θ ∈Θ},
where θ is a parameter in some subset Θ of Rp. We now seek the best g in Gp to minimize
some risk. Note that Gp may not necessarily contain the true f even for very large p.
We stress that our notation g(x) has a different meaning in the supervised and unsu-
pervised case. In the supervised case, g is interpreted as a prediction function for a
response y; in the unsupervised setting, g is an approximation of a density f.
For each x we measure the discrepancy between the true model f(x) and the hypothes-
ized model g(x | θ) using the loss function
Loss(f(x), g(x | θ)) = ln
f(x)
g

(x | θ) = ln f(x) −ln g(x | θ).
The expected value of this loss (that is, the risk) is thus
ℓ(g) = E ln
f(X)
g(X | θ) =
Z
f(x) ln
f(x)
g(x | θ) dx.
(2.27)
The integral in (2.27) provides a fundamental way to measure the distance between two
densities and is called the Kullback–Leibler (KL) divergence2
Kullback–
Leibler
divergence
between f and g(· | θ). Note
that the KL divergence is not symmetric in f and g(· | θ). Moreover, it is always greater
than or equal to 0 (see Exercise 15) and equal to 0 when f = g(· | θ).
Using similar notation as for the supervised learning setting in Table 2.1, define gGp as
the global minimizer of the risk in the class Gp; that is, gGp = argming∈Gp ℓ(g). If we define
θ∗= argmin
θ
E Loss( f(X), g(X | θ)) = argmin
θ
Z   ln f(x) −ln g(x | θ) f(x) dx
= argmax
θ
Z
f(x) ln g(x | θ) dx = argmax
θ
E ln g(X | θ),
then gGp = g(· | θ∗) and learning gGp is equivalent to learning (or estimating) θ∗. To learn θ∗
from a training set τ = {x1, . . . , xn} we then minimize the training loss,
1
n
n
X
i=1
Loss(f(xi), g(xi | θ)) = −1
n
n
X
i=1
ln g(xi | θ) + 1
n
n
X
i=1
ln f(xi),
giving:
bθn := argmax
θ
1
n
n
X
i=1
ln g(xi | θ).
(2.28)
2Sometimes called cross-entropy distance.
Statistical Learning
43
As the logarithm is an increasing function, this is equivalent to
bθn := argmax
θ
n
Y
i=1
g(xi | θ),
where Qn
i=1 g(xi | θ) is the likelihood of the data; that is, the joint density of the {Xi} eval-
uated at the points {xi}. We therefore have recovered the classical maximum likelihood
estimate of θ∗.
maximum
likelihood
estimate
☞456
When the risk ℓ(g(· | θ)) is convex in θ over a convex set Θ, we can find the maximum
likelihood estimator by setting the gradient of the training loss to zero; that is, we solve
−1
n
n
X
i=1
S(xi | θ) = 0,
where S(x | θ) := ∂ln g(x | θ)
∂θ
is the gradient of ln g(x | θ) with respect to θ and is often called
the score
score
.
Example 2.5 (Exponential Model) Suppose we have the training data τn = {x1, . . . , xn},
which is modeled

 as a realization of n positive iid random variables: X1, . . . , Xn ∼iid f(x).
We select the class of approximating functions G to be the parametric class {g : g(x | θ) =
θ exp(−x θ), x > 0, θ > 0}. In other words, we look for the best gG within the family of
exponential distributions with unknown parameter θ > 0. The likelihood of the data is
n
Y
i=1
g(xi | θ) =
n
Y
i=1
θ exp(−θxi) = exp(−θ n xn + n ln θ)
and the score is S (x | θ) = −x+θ−1. Thus, maximizing the likelihood with respect to θ is the
same as maximizing −θ n xn + n ln θ or solving −Pn
i=1 S (xi | θ)/n = xn −θ−1 = 0. In other
words, the solution to (2.28) is the maximum likelihood estimate bθn = 1/xn.
In a supervised setting, where the data is represented by a vector x of explanatory
variables and a response y, the general model is that (x, y) is an outcome of (X, Y) ∼f
for some unknown f. And for a training sequence (x1, y1), . . . , (xn, yn) the default model
assumption is that (X1, Y1), . . . , (Xn, Yn) ∼iid f. As explained in Section 2.2, the analysis
primarily involves the conditional pdf f(y | x) and in particular (when using the squared-
error loss) the conditional expectation g∗(x) = E[Y | X = x]. The resulting representation
(2.2) allows us to then write the response at X = x as a function of the feature x plus an
error term: Y = g∗(x) + ε(x).
This leads to the simplest and most important model for supervised learning, where we
choose a linear class G of prediction or guess functions and assume that it is rich enough
to contain the true g∗. If we further assume that, conditional on X = x, the error term ε
does not depend on x, that is, E ε = 0 and Var ε = σ2, then we obtain the following model.
44
Modeling Data
Definition 2.1: Linear Model
In a linear model
linear model
the response Y depends on a p-dimensional explanatory variable
x = [x1, . . . , xp]⊤via the linear relationship
Y = x⊤β + ε,
(2.29)
where E ε = 0 and Var ε = σ2.
Note that (2.29) is a model for a single pair (x, Y). The model f

or the training set
{(xi, Yi)} is simply that each Yi satisfies (2.29) (with x = xi) and that the {Yi} are independ-
ent. Gathering all responses in the vector Y = [Y1, . . . , Yn]⊤, we can write
Y = Xβ + ε,
(2.30)
where ε = [ε1, . . . , εn]⊤is a vector of iid copies of ε and X is the so-called model matrix
model matrix
,
with rows x⊤
1 , . . . , x⊤
n . Linear models are fundamental building blocks of statistical learning
algorithms. For this reason, a large part of Chapter 5 is devoted to linear regression models.
☞167
Example 2.6 (Polynomial Regression (cont.)) For our running Example 2.1, we see
☞26
that the data is described by a linear model of the form (2.30), with model matrix X given
in (2.10).
Statistical Learning
45
Before we discuss a few other models in the following sections, we would like to em-
phasize a number of points about modeling.
• Any model for data is likely to be wrong. For example, real data (as opposed to
computer-generated data) are often assumed to come from a normal distribution,
which is never exactly true. However, an important advantage of using a normal
distribution is that it has many nice mathematical properties, as we will see in Sec-
tion 2.7.
• Most data models depend on a number of unknown parameters, which need to be
estimated from the observed data.
• Any model for real-life data needs to be checked for suitability. An important cri-
terion is that data simulated from the model should resemble the observed data, at
least for a certain choice of model parameters.
Here are some guidelines for choosing a model. Think of the data as a spreadsheet or
data frame, as in Chapter 1, where rows represent the data units and the columns the data
features (variables, groups).
• First establish the type of the features (quantitative, qualitative, discrete, continuous,
etc.).
• Assess whether the data can be assumed to be independent across rows or columns.
• Decide on the level of generality of the model. For example, should we use a simp

le
model with a few unknown parameters or a more generic model that has a large
number of parameters? Simple specific models are easier to fit to the data (low es-
timation error) than more general models, but the fit itself may not be accurate (high
approximation error). The tradeoffs discussed in Section 2.4 play an important role
here.
• Decide on using a classical (frequentist) or Bayesian model. Section 2.9 gives a short
introduction to Bayesian learning.
☞
48
2.7
Multivariate Normal Models
A standard model for numerical observations x1, . . . , xn (forming, e.g., a column in a
spreadsheet or data frame) is that they are the outcomes of iid normal random variables
X1, . . . , Xn
iid∼N(µ, σ2).
It is helpful to view a normally distributed random variable as a simple transformation
of a standard normal random variable. To wit, if Z has a standard normal distribution, then
X = µ + σZ has a N(µ, σ2) distribution. The generalization to n dimensions is discussed
in Appendix C.7. We summarize the main points: Let Z1, . . . , Zn
iid∼N(0, 1). The pdf of
☞434
Z = [Z1, . . . , Zn]⊤(that is, the joint pdf of Z1, . . . , Zn) is given by
fZ(z) =
n
Y
i=1
1
√
2π
e−1
2z2
i = (2π)−n
2 e−1
2 z⊤z,
z ∈Rn.
(2.31)
46
Multivariate Normal Models
We write Z ∼N(0, In) and say that Z has a standard normal distribution in Rn. Let
X = µ + B Z
(2.32)
for some m×n matrix B and m-dimensional vector µ. Then X has expectation vector µ and
covariance matrix Σ = BB⊤; see (C.20) and (C.21). This leads to the following definition.
☞432
Definition 2.2: Multivariate Normal Distribution
An m-dimensional random vector X that can be written in the form (2.32) for some
m-dimensional vector µ and m × n matrix B, with Z ∼N(0, In), is said to have a
multivariate normal
multivariate
normal
or multivariate Gaussian distribution with mean vector µ and
covariance matrix Σ = BB⊤. We write X ∼N(µ, Σ).
The m-dimensional density of a multivariate normal distribution has a very similar form
to the density of the one-d

imensional normal distribution and is given in the next theorem.
We leave the proof as an exercise; see Exercise 5.
☞60
Theorem 2.3: Density of a Multivariate Random Vector
Let X ∼N(µ, Σ), where the m × m covariance matrix Σ is invertible. Then X has pdf
fX(x) =
1
√(2π)m |Σ|
e−1
2 (x−µ)⊤Σ−1(x−µ),
x ∈Rm.
(2.33)
Figure 2.13 shows the pdfs of two bivariate (that is, two-dimensional) normal distribu-
tions. In both cases the mean vector is µ = [0, 0]⊤and the variances (the diagonal elements
of Σ) are 1. The correlation coefficients (or, equivalently here, the covariances) are respect-
ively ϱ = 0 and ϱ = 0.8.
0
0.1
2
0.2
0
-2
2
0
-2
0
0.1
2
0.2
0
-2
2
0
-2
Figure 2.13: Pdfs of bivariate normal distributions with means zero, variances 1, and cor-
relation coefficients 0 (left) and 0.8 (right).
Statistical Learning
47
The main reason why the multivariate normal distribution plays an important role in
data science and machine learning is that it satisfies the following properties, the details
and proofs of which can be found in Appendix C.7:
☞434
1. Affine combinations are normal.
2. Marginal distributions are normal.
3. Conditional distributions are normal.
2.8
Normal Linear Models
Normal linear models combine the simplicity of the linear model with the tractability of
the Gaussian distribution. They are the principal model for traditional statistics, and include
the classic linear regression and analysis of variance models.
Definition 2.3: Normal Linear Model
In a normal linear model
normal linear
model
the response Y depends on a p-dimensional explanatory
variable x = [x1, . . . , xp]⊤, via the linear relationship
Y = x⊤β + ε,
(2.34)
where ε ∼N(0, σ2).
Thus, a normal linear model is a linear model (in the sense of Definition 2.1) with
normal error terms. Similar to (2.30), the corresponding normal linear model for the whole
training set {(xi, Yi)} has the form
Y = Xβ + ε,
(2.35)
where X is the model matrix comprised of rows x⊤
1 , . . . , x⊤
n and ε ∼N(0, σ2In). Con-
se

quently, Y can be written as Y = Xβ + σZ, where Z ∼N(0, In), so that Y ∼N(Xβ, σ2In).
It follows from (2.33) that its joint density is given by
☞
46
g(y | β, σ2, X) = (2πσ2)−n
2 e−
1
2σ2 ||y−Xβ||2.
(2.36)
Estimation of the parameter β can be performed via the least-squares method, as discussed
in Example 2.1. An estimate can also be obtained via the maximum likelihood method.
This simply means finding the parameters σ2 and β that maximize the likelihood of the
outcome y, given by the right-hand side of (2.36). It is clear that for every value of σ2
the likelihood is maximal when ∥y −Xβ∥2 is minimal. As a consequence, the maximum
likelihood estimate for β is the same as the least-squares estimate (2.15). We leave it as an
exercise (see Exercise 18) to show that the maximum likelihood estimate of σ2 is equal to
☞
64
c
σ2 = ∥y −Xbβ∥2
n
,
(2.37)
where bβ is the maximum likelihood estimate (least squares estimate in this case) of β.
48
Bayesian Learning
2.9
Bayesian Learning
In Bayesian unsupervised learning, we seek to approximate the unknown joint density
f(x1, . . . , xn) of the training data Tn = {X1, . . . , Xn} via a joint pdf of the form
Z 
n
Y
i=1
g(xi | θ)
w(θ) dθ,
(2.38)
where g(· | θ) belongs to a family of parametric densities Gp := {g(· | θ), θ ∈Θ} (viewed
as a family of pdfs conditional on a parameter θ in some set Θ ⊂Rp) and w(θ) is a pdf
that belongs to a (possibly different) family of densities Wp. Note how the joint pdf (2.38)
satisfies the permutational invariance (2.26) and can thus be useful as a model for training
data which is part of an exchangeable sequence of random variables.
Following standard practice in a Bayesian context, instead of writing fX(x) and
fX | Y(x | y) for the pdf of X and the conditional pdf of X given Y, one simply writes
f(x) and f(x | y). If Y is a different random variable, its pdf (at y) is thus denoted by
f(y).
Thus, we will use the same symbol g for different (conditional) approximating probab-
ility densiti

es and f for the different (conditional) true and unknown probability densities.
Using Bayesian notation, we can write g(τ | θ) = Qn
i=1 g(xi | θ) and thus the approximating
joint pdf (2.38) can then be written as
R
g(τ | θ) w(θ) dθ and the true unknown joint pdf as
f(τ) = f(x1, . . . , xn).
Once Gp and Wp are specified, selecting an approximating function g(x) of the form
g(x) =
Z
g(x | θ) w(θ) dθ
is equivalent to selecting a suitable w from Wp. Similar to (2.27), we can use the Kullback–
Leibler risk to measure the discrepancy between the proposed approximation (2.38) and the
true f(τ):
ℓ(g) = E ln
f(T )
R
g(T | θ) w(θ) dθ
=
Z
f(τ) ln
f(τ)
R
g(τ | θ) w(θ) dθ
dτ.
(2.39)
The main difference with (2.27) is that since the training data is not necessarily iid (it may
be exchangeable, for example), the expectation must be with respect to the joint density of
☞41
T , not with respect to the marginal f(x) (as in the iid case).
Minimizing the training loss is equivalent to maximizing the likelihood of the training
data τ; that is, solving the optimization problem
max
w∈Wp
Z
g(τ | θ) w(θ) dθ,
where the maximization is over an appropriate class Wp of density functions that is be-
lieved to result in the smallest KL risk.
Statistical Learning
49
Suppose that we have a rough guess, denoted w0(θ), for the best w ∈Wp that min-
imizes the Kullback–Leibler risk. We can always increase the resulting likelihood L0 :=
R
g(τ | θ) w0(θ) dθ by instead using the density w1(θ) := w0(θ) g(τ | θ)/L0, giving a likeli-
hood L1 :=
R
g(τ | θ) w1(θ) dθ. To see this, write L0 and L1 as expectations with respect to
w0. In particular, we can write
L0 = Ew0 g(τ | θ)
and
L1 = Ew1 g(τ | θ) = Ew0g2(τ | θ)/L0.
It follows that
L1 −L0 = 1
L0
Ew0
h
g2(τ | θ) −L2
0
i
= 1
L0
Varw0[g(τ | θ)] ⩾0.
(2.40)
We may thus expect to obtain better predictions using w1 instead of w0, because w1 has
taken into account the observed data τ and increased the likelihood of the model. In fact,
if we iterate this process (see 

Exercise 20) and create a sequence of densities w1, w2, . . .
such that wt(θ) ∝wt−1(θ) g(τ | θ), then wt(θ) concentrates more and more of its probability
mass at the maximum likelihood estimator bθ (see (2.28)) and in the limit equals a (degen-
erate) point-mass pdf atbθ. In other words, in the limit we recover the maximum likelihood
method: gτ(x) = g(x |bθ). Thus, unless the class of densities Wp is restricted to be non-
degenerate, maximizing the likelihood as much as possible leads to a degenerate choice
for w(θ).
In many situations, the maximum likelihood estimate g(τ |bθ) is either not an ap-
propriate approximation to f(τ) (see Example 2.9), or simply fails to exist (see Exer-
cise 10 in Chapter 4). In such cases, given an initial non-degenerate guess w0(θ) = g(θ),
☞162
one can obtain a more appropriate and non-degenerate approximation to f(τ) by taking
w(θ) = w1(θ) ∝g(τ | θ) g(θ) in (2.38), giving the following Bayesian learner of f(x):
gτ(x) :=
Z
g(x | θ)
g(τ | θ) g(θ)
R
g(τ | ϑ) g(ϑ) dϑ
dθ,
(2.41)
where
R
g(τ | ϑ) g(ϑ) dϑ = g(τ). Using Bayes’ formula for probability densities,
☞428
g(θ | τ) = g(τ | θ) g(θ)
g(τ)
,
(2.42)
we can write w1(θ) = g(θ | τ). With this notation, we have the following definitions.
Definition 2.4: Prior, Likelihood, and Posterior
Let τ and Gp := {g(· | θ), θ ∈Θ} be the training set and family of approximating
functions.
• A pdf g(θ) that reflects our a priori beliefs about θ is called the prior
prior
pdf.
• The conditional pdf g(τ | θ) is called the likelihood
likelihood
.
• Inference about θ is given by the posterior
posterior
pdf g(θ | τ), which is proportional
to the product of the prior and the likelihood:
g(θ | τ) ∝g(τ | θ) g(θ).
50
Bayesian Learning
Remark 2.1 (Early Stopping) Bayes iteration is an example of an “early stopping”
heuristic for maximum likelihood optimization, where we exit after only one step. As ob-
served above, if we keep iterating, we obtain the maximum likelihood estimate (MLE). In
a sense the Bayes rule pro

vides a regularization of the MLE. Regularization is discussed in
more detail in Chapter 6; see also Example 2.9. The early stopping rule is also of benefit
in regularization; see Exercise 20 in Chapter 6.
On the one hand, the initial guess g(θ) conveys the a priori (prior to training the
Bayesian learner) information about the optimal density in Wp that minimizes the KL risk.
Using this prior g(θ), the Bayesian approximation to f(x) is the prior predictive density
prior predictive
density
:
g(x) =
Z
g(x | θ) g(θ) dθ.
On the other hand, the posterior pdf conveys improved knowledge about this optimal dens-
ity in Wp after training with τ. Using the posterior g(θ | τ), the Bayesian learner of f(x) is
the posterior predictive density
posterior
predictive
density
:
gτ(x) = g(x | τ) =
Z
g(x | θ) g(θ | τ) dθ,
where we have assumed that g(x | θ, τ) = g(x | θ); that is, the likelihood depends on τ only
through the parameter θ.
The choice of the prior is typically governed by two considerations:
1. the prior should be simple enough to facilitate the computation or simulation of the
posterior pdf;
2. the prior should be general enough to model ignorance of the parameter of interest.
Priors that do not convey much knowledge of the parameter are said to be uninformat-
ive. The uniform or flat prior in Example 2.9 (to follow) is frequently used.
uninformative
prior
For the purpose of analytical and numerical computations, we can view θ as a ran-
dom vector with prior density g(θ), which after training is updated to the posterior
density g(θ | τ).
The above thinking allows us to write g(x | τ) ∝
R
g(x | θ) g(τ | θ) g(θ) dθ, for example,
thus ignoring any constants that do not depend on the argument of the densities.
Example 2.7 (Normal Model) Suppose that the training data T = {X1, . . . , Xn} is
modeled using the likelihood g(x | θ) that is the pdf of
X | θ ∼N(µ, σ2),
where θ := [µ, σ2]⊤. Next, we need to specify the prior distribution of θ to complete
the model. We can specify 

prior distributions for µ and σ2 separately and then take their
Statistical Learning
51
product to obtain the prior for vector θ (assuming independence). A possible prior distri-
bution for µ is
µ ∼N(ν, ϕ2).
(2.43)
It is typical to refer to any parameters of the prior density as hyperparameters
hyperparamet-
ers
of the
Bayesian model. Instead of giving directly a prior for σ2 (or σ), it turns out to be con-
venient to give the following prior distribution to 1/σ2:
1
σ2 ∼Gamma(α, β).
(2.44)
The smaller α and β are, the less informative is the prior. Under this prior, σ2 is said to have
an inverse gamma
inverse gamma
3 distribution. If 1/Z ∼Gamma(α, β), then the pdf of Z is proportional
to exp (−β/z) /zα+1 (Exercise 19). The Bayesian posterior is then given by:
☞
64
g(µ, σ2 | τ) ∝g(µ) × g(σ2) × g(τ | µ, σ2)
∝exp
(
−(µ −ν)2
2ϕ2
)
×
exp
n
−β/σ2o
(σ2)α+1
×
exp
n
−P
i(xi −µ)2/(2σ2)
o
(σ2)n/2
∝(σ2)−n/2−α−1 exp
(
−(µ −ν)2
2ϕ2
−β
σ2 −(µ −xn)2 + S 2
n
2σ2/n
)
,
where S 2
n := 1
n
P
i x2
i −x2
n = 1
n
P
i(xi −xn)2 is the (scaled) sample variance. All inference
about (µ, σ2) is then represented by the posterior pdf. To facilitate computations it is helpful
to find out if the posterior belongs to a recognizable family of distributions. For example,
the conditional pdf of µ given σ2 and τ is
g(µ | σ2, τ) ∝exp
(
−(µ −ν)2
2ϕ2
−(µ −xn)2
2σ2/n
)
,
which after simplification can be recognized as the pdf of
(µ | σ2, τ) ∼N

γnxn + (1 −γn)ν, γn σ2/n

,
(2.45)
where we have defined the weight parameter: γn :=
n
σ2
. 
1
ϕ2 + n
σ2

. We can then see that the
posterior mean E[µ | σ2, τ] = γnxn + (1 −γn)ν is a weighted linear combination of the prior
mean ν and the sample average xn. Further, as n →∞, the weight γn →1 and thus the
posterior mean approaches the maximum likelihood estimate xn.
It is sometimes possible to use a prior g(θ) that is not a bona fide probability density, in the
sense that
R
g(θ) dθ = ∞, as long as the resulting posterior g(θ | τ) ∝g(τ | θ)g(θ) is a proper
pdf. Su

ch a prior is called an improper prior
improper prior
.
Example 2.8 (Normal Model (cont.)) An example of an improper prior is obtained
from (2.43) when we let ϕ →∞(the larger ϕ is, the more uninformative is the prior).
3Reciprocal gamma distribution would have been a better name.
52
Bayesian Learning
Then, g(µ) ∝1 is a flat prior, but
R
g(µ) dµ = ∞, making it an improper prior. Neverthe-
less, the posterior is a proper density, and in particular the conditional posterior of (µ | σ2, τ)
simplifies to
(µ | σ2, τ) ∼N

xn, σ2/n

,
because the weight parameter γn goes to 1 as ϕ →∞. The improper prior g(µ) ∝1 also
allows us to simplify the posterior marginal for σ2:
g(σ2 | τ) =
Z
g(µ, σ2 | τ) dµ ∝(σ2)−(n−1)/2−α−1 exp
(
−β + nS 2
n/2
σ2
)
,
which we recognize as the density corresponding to
1
σ2
 τ ∼Gamma
 
α + n −1
2
, β + n
2S 2
n
!
.
In addition to g(µ) ∝1, we can also use an improper prior for σ2. If we take the limit α →0
and β →0 in (2.44), then we also obtain the improper prior g(σ2) ∝1/σ2 (or equivalently
g(1/σ2) ∝1/σ2). In this case, the posterior marginal density for σ2 implies that:
nS 2
n
σ2
 τ ∼χ2
n−1
and the posterior marginal density for µ implies that:
µ −xn
S n/
√
n −1
 τ ∼tn−1.
(2.46)
In general, deriving a simple formula for the posterior density of θ is either impossible
or too tedious. Instead, the Monte Carlo methods in Chapter 3 can be used to simulate
(approximately) from the posterior for the purposes of inference and prediction.
One way in which a distributional result such as (2.46) can be useful is in the construc-
tion of a 95% credible interval
credible
interval
I for the parameter µ; that is, an interval I such that the
probability P[µ ∈I | τ] is equal to 0.95. For example, the symmetric 95% credible interval
is
I =
"
xn −
S n
√
n −1
γ, xn +
S n
√
n −1
γ
#
,
where γ is the 0.975-quantile of the tn−1 distribution. Note that the credible interval is
not a random object and that the parameter µ is interpreted as a random variable w

ith a
distribution. This is unlike the case of classical confidence intervals, where the parameter
is nonrandom, but the interval is (the outcome of) a random object.
☞457
As a generalization of the 95% Bayesian credible interval we can define a 1−α credible
region
credible region
, which is any set R satisfying
P[θ ∈R | τ] =
Z
θ∈R
g(θ | τ) dθ ⩾1 −α.
(2.47)
Statistical Learning
53
Example 2.9 (Bayesian Regularization of Maximum Likelihood) Consider model-
ing the number of deaths during birth in a maternity ward. Suppose that the hospital data
consists of τ = {x1, . . . , xn}, with xi = 1 if the i-th baby has died during birth and xi = 0
otherwise, for i = 1, . . . , n. A possible Bayesian model for the data is θ ∼U(0, 1) (uniform
prior) with (X1, . . . , Xn | θ)
iid∼Ber(θ). The likelihood is therefore
g(τ | θ) =
n
Y
i=1
θxi(1 −θ)1−xi = θs (1 −θ)n−s,
where s = x1 + · · · + xn is the total number of deaths. Since g(θ) = 1, the posterior pdf is
g(θ | τ) ∝θs (1 −θ)n−s,
θ ∈[0, 1],
which is the pdf of the Beta(s + 1, n −s + 1) distribution. The normalization constant is
(n + 1)
n
s

. The posterior pdf is shown in Figure 2.14 for (s, n) = (0, 100). It is not difficult
Figure 2.14: Posterior pdf for θ, with n = 100 and s = 0.
to see that the maximum a posteriori
maximum a
posteriori
(MAP) estimate of θ (the mode or maximizer of the
posterior density) is
argmax
θ
g(θ | τ) = s
n,
which agrees with the maximum likelihood estimate. Figure 2.14 also shows that the left
one-sided 95% credible interval for θ is [0, 0.0292], where 0.0292 is the 0.95 quantile
(rounded) of the Beta(1, 101) distribution.
Observe that when (s, n) = (0, 100) the maximum likelihood estimate bθ = 0 infers that
deaths at birth are not possible. We know that this inference is wrong — the probability of
death can never be zero, it is simply (and fortunately) too small to be inferred accurately
from a sample size of n = 100. In contrast to the maximum likelihood estimate, the pos-
terior mean E[θ | τ] = (

s + 1)/(n + 2) is not zero for (s, n) = (0, 100) and provides the more
reasonable point estimate of 0.0098 for the probability of death.
54
Bayesian Learning
In addition, while computing a Bayesian credible interval poses no conceptual diffi-
culties, it is not simple to derive a confidence interval for the maximum likelihood estimate
of bθ, because the likelihood as a function of θ is not differentiable at θ = 0. As a result of
this lack of smoothness, the usual confidence intervals based on the normal approximation
cannot be used.
We now return to the unsupervised learning setting of Section 2.6, but consider this
from a Bayesian perspective. Recall from (2.39) that the Kullback–Leibler risk for an ap-
proximating function g is
ℓ(g) =
Z
f(τ′
n)[ln f(τ′
n) −ln g(τ′
n)] dτ′
n,
where τ′
n denotes the test data. Since
R
f(τ′
n) ln f(τ′
n) dτ′
n plays no role in minimizing the
risk, we consider instead the cross-entropy risk, defined as
☞122
ℓ(g) = −
Z
f(τ′
n) ln g(τ′
n) dτ′
n.
Note that the smallest possible cross-entropy risk is ℓ∗
n = −
R
f(τ′
n) ln f(τ′
n) dτ′
n. The expec-
ted generalization risk of the Bayesian learner can then be decomposed as
E ℓ(gTn) = ℓ∗
n +
Z
f(τ′
n) ln
f(τ′
n)
E g(τ′
n | Tn) dτ′
n
|                            {z                            }
“bias” component
+ E
Z
f(τ′
n) ln E g(τ′
n | Tn)
g(τ′
n | Tn) dτ′
n
|                               {z                               }
“variance” component
,
where gTn(τ′
n) = g(τ′
n | Tn) =
R
g(τ′
n | θ) g(θ | Tn) dθ is the posterior predictive density after
observing Tn.
Assuming that the sets Tn and T ′
n are comprised of 2n iid random variables with density
f, we can show (Exercise 23) that the expected generalization risk simplifies to
E ℓ(gTn) = E ln g(Tn) −E ln g(T2n),
(2.48)
where g(τn) and g(τ2n) are the prior predictive densities of τn and τ2n, respectively.
Let θn = argmaxθ g(θ | Tn) be the MAP estimator of θ∗:= argmaxθ E ln g(X | θ). As-
suming that θn converges to θ∗(with probability one) an

d 1
nE ln g(Tn | θn) = E ln g(X | θ∗) +
O(1/n), we can use the following large-sample approximation of the expected generaliza-
tion risk.
Theorem 2.4: Approximating the Bayesian Cross-Entropy Risk
For n →∞, the expected cross-entropy generalization risk satisfies:
Eℓ(gTn) ≃−E ln g(Tn) −p
2 ln n,
(2.49)
where (with p the dimension of the parameter vector θ and θn the MAP estimator):
E ln g(Tn) ≃E ln g(Tn | θn) −p
2 ln n.
(2.50)
Statistical Learning
55
Proof: To show (2.50), we apply Theorem C.21 to ln
R
e−nrn(θ)g(θ) dθ, where
☞450
rn(θ) := −1
n ln g(Tn | θ) = −1
n
n
X
i=1
ln g(Xi | θ)
a.s.
−→−E ln g(X | θ) =: r(θ) < ∞.
This gives (with probability one)
ln
Z
g(Tn | θ) g(θ) dθ ≃−nr(θ∗) −p
2 ln(n).
Taking expectations on both sides and using nr(θ∗) = nE[rn(θn)] + O(1), we deduce (2.50).
To demonstrate (2.49), we derive the asymptotic approximation of E ln g(T2n) by repeating
the argument for (2.50), but replacing n with 2n, where necessary. Thus, we obtain:
E ln g(T2n) ≃−2nr(θ∗) −p
2 ln(2n).
Then, (2.49) follows from the identity (2.48).
□
The results of Theorem 2.4 have two major implications for model selection and assess-
ment. First, (2.49) suggests that −ln g(Tn) can be used as a crude (leading-order) asymp-
totic approximation to the expected generalization risk for large n and fixed p. In this
context, the prior predictive density g(Tn) is usually called the model evidence
model evidence
or marginal
likelihood for the class Gp. Since the integral
R
g(Tn | θ) g(θ) dθ is rarely available in closed
form, the exact computation of the model evidence is typically not feasible and may require
Monte Carlo estimation methods.
☞
78
Second, when the model evidence is difficult to compute via Monte Carlo methods or
otherwise, (2.50) suggests that we can use the following large-sample approximation:
−2E ln g(Tn) ≃−2 ln g(Tn | θn) + p ln(n).
(2.51)
The asymptotic approximation on the right-hand side of (2.51) is called the Bayesian in-
formation criterion
Bayesian
information


criterion
(BIC). We prefer the class Gp with the smallest BIC. The BIC is typic-
ally used when the model evidence is difficult to compute and n is sufficiently larger than
p. For a fixed p, and as n becomes larger and larger, the BIC becomes a more and more
accurate estimator of −2E ln g(Tn). Note that the BIC approximation is valid even when the
true density f < Gp. The BIC provides an alternative to the Akaike information criterion
(AIC) for model selection. However, while the BIC approximation does not assume that
☞126
the true model f belongs to the parametric class under consideration, the AIC assumes
that f ∈Gp. Thus, the AIC is merely a heuristic approximation based on the asymptotic
approximations in Theorem 4.1.
Although the above Bayesian theory has been presented in an unsupervised learn-
ing setting, it can be readily extended to the supervised case. We only need to relabel
the training set Tn. In particular, when (as is typical for regression models) the train-
ing responses Y1, . . . , Yn are considered as random variables but the corresponding fea-
ture vectors x1, . . . , xn are viewed as being fixed, then Tn is the collection of random re-
sponses {Y1, . . . , Yn}. Alternatively, we can simply identify Tn with the response vector
Y = [Y1, . . . , Yn]⊤. We will adopt this notation in the next example.
56
Bayesian Learning
Example 2.10 (Polynomial Regression (cont.)) Consider Example 2.2 once again, but
now in a Bayesian framework, where the prior knowledge on (σ2, β) is specified by
g(σ2) = 1/σ2 and β | σ2 ∼N(0, σ2D), and D is a (matrix) hyperparameter. Let Σ :=
(X⊤X + D−1)−1. Then the posterior can be written as:
g(β, σ2 | y) =
exp

−∥y−Xβ∥2
2σ2

(2πσ2)n/2
×
exp

−β⊤D−1β
2σ2

(2πσ2)p/2 |D|1/2 × 1
σ2
,
g(y)
=
(σ2)−(n+p)/2−1
(2π)(n+p)/2 |D|1/2 exp
 
−∥Σ−1/2(β −β)∥2
2σ2
−(n + p + 2) σ2
2σ2
! ,
g(y),
where β := ΣX⊤y and σ2 := y⊤(I −XΣX⊤)y/(n + p + 2) are the MAP estimates of β and
σ2, and g(y) is the model evidence for Gp:
g(y) =
"
g(β, σ2, y) dβ 

dσ2
=
|Σ|1/2
(2π)n/2|D|1/2
Z ∞
0
exp

−(n+p+2)σ
2
2σ2

(σ2)n/2+1
dσ2
=
|Σ|1/2Γ(n/2)
|D|1/2(π(n + p + 2) σ2)n/2.
Therefore, based on (2.49), we have
2Eℓ(gTn) ≃−2 ln g(y) = n ln
h
π(n + p + 2) σ2i
−2 ln Γ(n/2) + ln |D| −ln |Σ|.
On the other hand, the minus of the log-likelihood of Y can be written as
−ln g(y | β, σ2) = ∥y −Xβ∥2
2σ2
+ n
2 ln(2πσ2)
= ∥Σ−1/2(β −β)∥2
2σ2
+ (n + p + 2) σ2
2σ2
+ n
2 ln(2πσ2).
Therefore, the BIC approximation (2.51) is
−2 ln g(y | β, σ2) + (p + 1) ln(n) = n[ln(2π σ2) + 1] + (p + 1) ln(n) + (p + 2),
(2.52)
where the extra ln(n) term in (p + 1) ln(n) is due to the inclusion of σ2 in θ = (σ2, β).
Figure 2.15 shows the model evidence and its BIC approximation, where we used a hyper-
parameter D = 104 × Ip for the prior density of β. We can see that both approximations
exhibit a pronounced minimum at p = 4, thus identifying the true polynomial regression
model. Compare the overall qualitative shape of the cross-entropy risk estimate with the
shape of the square-error risk estimate in Figure 2.11.
Statistical Learning
57
1
2
3
4
5
6
7
8
9
10
600
650
700
750
800
Figure 2.15: The BIC and marginal likelihood used for model selection.
It is possible to give the model complexity parameter p a Bayesian treatment, in which
we define a prior density on the set of all models under consideration. For example, let
g(p), p = 1, . . . , m be a prior density on m candidate models. Treating the model com-
plexity index p as an additional parameter to θ ∈Rp, and applying Bayes’ formula, the
posterior for (θ, p) can be written as:
g(θ, p | τ) = g(θ | p, τ) × g(p | τ)
= g(τ | θ, p) g(θ | p)
g(τ | p)
|               {z               }
posterior of θ given model p
× g(τ | p) g(p)
g(τ)
|        {z        }
posterior of model p
.
The model evidence for a fixed p is now interpreted as the prior predictive density of τ,
conditional on the model p:
g(τ | p) =
Z
g(τ | θ, p) g(θ | p) dθ,
and the quantity g(τ) = Pm
p=1 g(τ | p) g(p) is interpreted as the marginal likelihoo

d of all the
m candidate models. Finally, a simple method for model selection is to pick the index bp
with the largest posterior probability:
bp = argmax
p
g(p | τ) = argmax
p
g(τ | p) g(p).
Example 2.11 (Polynomial Regression (cont.)) Let us revisit Example 2.10 by giving
the parameter p = 1, . . . , m, with m = 10, a Bayesian treatment. Recall that we used the
notation τ = y in that example. We assume that the prior g(p) = 1/m is flat and uninform-
ative so that the posterior is given by
g(p | y) ∝g(y | p) =
|Σ|1/2 Γ(n/2)
|D|1/2(π(n + p + 2) σ2)n/2,
58
Bayesian Learning
where all quantities in g(y | p) are computed using the first p columns of X. Figure 2.16
shows the resulting posterior density g(p | y). The figure also shows the posterior density
bg(y | p) P10
p=1bg(y | p), where
bg(y | p) := exp
 
−n[ln(2π σ2) + 1] + (p + 1) ln(n) + (p + 2)
2
!
is derived from the BIC approximation (2.52). In both cases, there is a clear maximum at
p = 4, suggesting that a third-degree polynomial is the most appropriate model for the
data.
1
2
3
4
5
6
7
8
9
10
0
0.2
0.4
0.6
0.8
1
Figure 2.16: Posterior probabilities for each polynomial model of degree p −1.
Suppose that we wish to compare two models, say model p = 1 and model p = 2.
Instead of computing the posterior g(p | τ) explicitly, we can compare the posterior odds
ratio:
g(p = 1 | τ)
g(p = 2 | τ) = g(p = 1)
g(p = 2) × g(τ | p = 1)
g(τ | p = 2)
|        {z        }
Bayes factor B1 | 2
.
This gives rise to the Bayes factor
Bayes factor
Bi | j, whose value signifies the strength of the evidence
in favor of model i over model j. In particular Bi | j > 1 means that the evidence in favor for
model i is larger.
Example 2.12 (Savage–Dickey Ratio) Suppose that we have two models. Model p =
2 has a likelihood g(τ | µ, ν, p = 2), depending on two parameters. Model p = 1 has the
same functional form for the likelihood but now ν is fixed to some (known) ν0; that
is, g(τ | µ, p = 1) = g(τ | µ, ν = ν0, p = 2). We also assume that the 

prior information on µ
Statistical Learning
59
for model 1 is the same as that for model 2, conditioned on ν = ν0. That is, we assume
g(µ | p = 1) = g(µ | ν = ν0, p = 2). As model 2 contains model 1 as a special case, the latter
is said to be nested inside model 2. We can formally write (see also Exercise 26):
g(τ | p = 1) =
Z
g(τ | µ, p = 1) g(µ | p = 1) dµ
=
Z
g(τ | µ, ν = ν0, p = 2) g(µ | ν = ν0, p = 2) dµ
= g(τ | ν = ν0, p = 2) = g(τ, ν = ν0 | p = 2)
g(ν = ν0 | p = 2) .
Hence, the Bayes factor simplifies to
B1 | 2 = g(τ | p = 1)
g(τ | p = 2) = g(τ, ν = ν0 | p = 2)
g(ν = ν0 | p = 2)

g(τ | p = 2) = g(ν = ν0 | τ, p = 2)
g(ν = ν0 | p = 2) .
In other words, B1 | 2 is the ratio of the posterior density to the prior density of ν, evaluated at
ν = ν0 and both under the unrestricted model p = 2. This ratio of posterior to prior densities
is called the Savage–Dickey density ratio
Savage–Dickey
density ratio
.
Whether to use a classical (frequentist) or Bayesian model is largely a question of con-
venience. Classical inference is useful because it comes with a huge repository of ready-
to-use results, and requires no (subjective) prior information on the parameters. Bayesian
models are useful because the whole theory is based on the elegant Bayes’ formula, and
uncertainty in the inference (e.g., confidence intervals) can be quantified much more nat-
urally (e.g., credible intervals). A usual practice is to “Bayesify” a classical model, simply
by adding some prior information on the parameters.
Further Reading
A popular textbook on statistical learning is [55]. Accessible treatments of mathematical
statistics can be found, for example, in [69], [74], and [124]. More advanced treatments
are given in [10], [25], and [78]. A good overview of modern-day statistical inference
is given in [36]. Classical references on pattern classification and machine learning are
[12] and [35]. For advanced learning theory including information theory and Rademacher
complexity, we refer to [2

8] and [109]. An applied reference for Bayesian inference is [46].
For a survey of numerical techniques relevant to computational statistics, see [90].
Exercises
1. Suppose that the loss function is the piecewise linear function
Loss(y,by) = α (by −y)+ + β (y −by)+,
α, β > 0,
where c+ is equal to c if c > 0, and zero otherwise. Show that the minimizer of the risk
ℓ(g) = E Loss(Y, g(X)) satisfies
P[Y < g∗(x) | X = x] =
β
α + β.
In other words, g∗(x) is the β/(α + β) quantile of Y, conditional on X = x.
60
Exercises
2. Show that, for the squared-error loss, the approximation error ℓ(gG) −ℓ(g∗) in (2.16), is
equal to E(gG(X) −g∗(X))2. [Hint: expand ℓ(gG) = E(Y −g∗(X) + g∗(X) −gG(X))2.]
3. Suppose G is the class of linear functions. A linear function evaluated at a feature x can
be described as g(x) = β⊤x for some parameter vector β of appropriate dimension. Denote
gG(x) = x⊤βG and gG
τ (x) = x⊤bβ. Show that
E

gG
τ (X) −g∗(X)
2 = E

X⊤bβ −X⊤βG2 + E

X⊤βG −g∗(X)
2 .
Hence, deduce that the statistical error in (2.16) is ℓ(gG
τ ) −ℓ(gG) = E (gG
τ (X) −gG(X))2.
4. Show that formula (2.24) holds for the 0–1 loss with 0–1 response.
5. Let X be an n-dimensional normal random vector with mean vector µ and covariance
matrix Σ, where the determinant of Σ is non-zero. Show that X has joint probability density
fX(x) =
1
√(2π)n |Σ|
e−1
2 (x−µ)⊤Σ−1(x−µ),
x ∈Rn.
6. Let bβ = A+y. Using the defining properties of the pseudo-inverse, show that for any
☞360
β ∈Rp,
∥Abβ −y∥⩽∥Aβ −y∥.
7. Suppose that in the polynomial regression Example 2.1 we select the linear class of
functions Gp with p ⩾4. Then, g∗∈Gp and the approximation error is zero, because
gGp(x) = g∗(x) = x⊤β, where β = [10, −140, 400, −250, 0, . . . , 0]⊤∈Rp. Use the tower
property to show that the learner gτ(x) = x⊤bβ with bβ = X+y, assuming rank(X) ⩾4, is
☞431
unbiased
unbiased
:
E gT(x) = g∗(x).
8. (Exercise 7 continued.) Observe that the learner gT can be written as a linear combina-
tion of the response variable: gT(x

) = x⊤X+Y. Prove that for any learner of the form x⊤Ay,
where A ∈Rp×n is some matrix and that satisfies EX[x⊤AY] = g∗(x), we have
VarX[x⊤X+Y] ⩽VarX[x⊤AY],
where the equality is achieved for A = X+. This is called the Gauss–Markov inequality
Gauss–Markov
inequality
.
Hence, using the Gauss–Markov inequality deduce that for the unconditional variance:
Var gT(x) ⩽Var[x⊤AY].
Deduce that A = X+ also minimizes the expected generalization risk.
9. Consider again the polynomial regression Example 2.1. Use the fact that EXbβ = X+h∗(u),
where h∗(u) = E[Y | U = u] = [h∗(u1), . . . , h∗(un)]⊤, to show that the expected in-sample
risk is:
EX ℓin(gT) = ℓ∗+ ∥h∗(u)∥2 −∥XX+h∗(u)∥2
n
+ ℓ∗p
n .
Also, use Theorem C.2 to show that the expected statistical error is:
☞430
EX (bβ −β)⊤Hp(bβ −β) = ℓ∗tr(X+(X+)⊤Hp) + (X+h∗(u) −β)⊤Hp(X+h∗(u) −β).
Statistical Learning
61
10. Consider the setting of the polynomial regression in Example 2.2. Use Theorem C.19
to prove that
☞449
√n (bβn −βp)
d
−→N

0, ℓ∗H−1
p + H−1
p MpH−1
p

,
(2.53)
where Mp := E[XX⊤(g∗(X) −gGp(X))2] is the matrix with (i, j)-th entry:
Z 1
0
ui+j−2(hHp(u) −h∗(u))2 du,
and H−1
p is the p × p inverse Hilbert matrix
inverse Hilbert
matrix
with (i, j)-th entry:
(−1)i+j(i + j −1)
 p + i −1
p −j
! p + j −1
p −i
! i + j −2
i −1
!2
.
Observe that Mp = 0 for p ⩾4, so that the matrix Mp term is due to choosing a restrictive
class Gp that does not contain the true prediction function.
11. In Example 2.2 we saw that the statistical error can be expressed (see (2.20)) as
Z 1
0

[1, . . . , up−1](bβ −βp)
2 du = (bβ −βp)⊤Hp(bβ −βp).
By Exercise 10 the random vector Zn := √n(bβn −βp) has asymptotically a multivariate
normal distribution with mean vector 0 and covariance matrix V := ℓ∗H−1
p + H−1
p MpH−1
p .
Use Theorem C.2 to show that the expected statistical error is asymptotically
☞430
E (bβ −βp)⊤Hp(bβ −βp) ≃ℓ∗p
n +
tr(MpH−1
p )
n
,
n →∞.
(2.54)
Plot this large-sample approximation of the expected statistical error and compare it with
the 

outcome of the statistical error.
We note a subtle technical detail: In general, convergence in distribution does not imply
convergence in Lp-norm (see Example C.6), and so here we have implicitly assumed that
☞442
∥Zn∥
d
−→Dist. ⇒∥Zn∥
L2
−→constant := limn↑∞E∥Zn∥.
12. Consider again Example 2.2. The result in (2.53) suggests that Ebβ →βp as n →∞,
where βp is the solution in the class Gp given in (2.18). Thus, the large-sample approxim-
ation of the pointwise bias of the learner g
Gp
T (x) = x⊤bβ at x = [1, . . . , up−1]⊤is
E g
Gp
T (x) −g∗(x) ≃[1, . . . , up−1] βp −[1, u, u2, u3] β∗,
n →∞.
Use Python to reproduce Figure 2.17, which shows the (large-sample) pointwise squared
bias of the learner for p ∈{1, 2, 3}. Note how the bias is larger near the endpoints u = 0
and u = 1. Explain why the areas under the curves correspond to the approximation errors.
62
Exercises
0
0.2
0.4
0.6
0.8
1
0
50
100
150
200
250
Figure 2.17: The large-sample pointwise squared bias of the learner for p = 1, 2, 3. The
bias is zero for p ⩾4.
13. For our running Example 2.2 we can use (2.53) to derive a large-sample approximation
of the pointwise variance of the learner gT(x) = x⊤bβn. In particular, show that for large n
Var gT(x) ≃
ℓ∗x⊤H−1
p x
n
+
x⊤H−1
p MpH−1
p x
n
,
n →∞.
(2.55)
Figure 2.18 shows this (large-sample) variance of the learner for different values of the
predictor u and model index p. Observe that the variance ultimately increases in p and that
it is smaller at u = 1/2 than closer to the endpoints u = 0 or u = 1. Since the bias is also
9
7
5
1
2
0.05
3
3
4
0.5
1
0.95
Figure 2.18: The pointwise variance of the learner for various pairs of p and u.
larger near the endpoints, we deduce that the pointwise mean squared error (2.21) is larger
near the endpoints of the interval [0, 1] than near its middle. In other words, the error is
much smaller in the center of the data cloud than near its periphery.
Statistical Learning
63
14. Let h : x 7→R be a convex function and let X be a ran

dom variable. Use the subgradi-
ent definition of convexity to prove Jensen’s inequality:
☞403
Jensen’s
inequality
E h(X) ⩾h(EX).
(2.56)
15. Using Jensen’s inequality, show that the Kullback–Leibler divergence between prob-
ability densities f and g is always positive; that is,
E ln f(X)
g(X) ⩾0,
where X ∼f.
16. The purpose of this exercise is to prove the following Vapnik–Chernovenkis bound
Vapnik–
Chernovenkis
bound
: for
any finite class G (containing only a finite number |G| of possible functions) and a general
bounded loss function, l ⩽Loss ⩽u, the expected statistical error is bounded from above
according to:
E ℓ(gG
Tn) −ℓ(gG) ⩽(u −l) √2 ln(2|G|)
√n
.
(2.57)
Note how this bound conveniently does not depend on the distribution of the training set
Tn (which is typically unknown), but only on the complexity (i.e., cardinality) of the class
G. We can break up the proof of (2.57) into the following four parts:
(a) For a general function class G, training set T , risk function ℓ, and training loss ℓT,
we have, by definition, ℓ(gG) ⩽ℓ(g) and ℓT(gG
T) ⩽ℓT(g) for all g ∈G. Show that
ℓ(gG
T) −ℓ(gG) ⩽sup
g∈G
|ℓT(g) −ℓ(g)| + ℓT(gG) −ℓ(gG),
where we used the notation sup (supremum) for the least upper bound. Since
EℓT(g) = Eℓ(g), we obtain, after taking expectations on both sides of the inequal-
ity above:
E ℓ(gG
T) −ℓ(gG) ⩽E sup
g∈G
|ℓT(g) −ℓ(g)|.
(b) If X is a zero-mean random variable taking values in the interval [l, u], then the fol-
lowing Hoeffding’s inequality
Hoeffding’s
inequality
states that the moment generating function satisfies
E etX ⩽exp
 t2(u −l)2
8
!
,
t ∈R.
(2.58)
Prove this result by using the fact that the line segment joining points (l, exp(tl)) and
(u, exp(tu)) bounds the convex function x 7→exp(tx) for x ∈[l, u]; that is:
etx ⩽etlu −x
u −l + etu x −l
u −l,
x ∈[l, u].
(c) Let Z1, . . . , Zn be (possibly dependent and non-identically distributed) zero-mean ran-
dom variables with moment generating functions that satisfy E exp(tZk) ⩽exp(t2η2/2)
for all

 k and some parameter η. Use Jensen’s inequality (2.56) to prove that for any
☞427
64
Exercises
t > 0,
E max
k
Zk = 1
t E ln max
k
etZk ⩽1
t ln n + tη2
2 .
From this derive that
E max
k
Zk ⩽η
√
2 ln n.
Finally, show that this last inequality implies that
E max
k
|Zk| ⩽η
p
2 ln(2n).
(2.59)
(d) Returning to the objective of this exercise, denote the elements of G by g1, . . . , g|G|,
and let Zk = ℓTn(gk) −ℓ(gk). By part (a) it is sufficient to bound E maxk |Zk|. Show that
the {Zk} satisfy the conditions of (c) with η = (u −l)/√n. For this you will need to
apply part (b) to the random variable Loss(g(X), Y) −ℓ(g), where (X, Y) is a generic
data point. Now complete the proof of (2.57).
17. Consider the problem in Exercise 16a above. Show that
|ℓT(gG
T) −ℓ(gG)| ⩽2 sup
g∈G
|ℓT(g) −ℓ(g)| + ℓT(gG) −ℓ(gG).
From this, conclude:
E |ℓT(gG
T) −ℓ(gG)| ⩽2E sup
g∈G
|ℓT(g) −ℓ(g)|.
The last bound allows us to assess how close the training loss ℓT(gG
T) is to the optimal risk
ℓ(gG) within class G.
18. Show that for the normal linear model Y ∼N(Xβ, σ2In), the maximum likelihood es-
timator of σ2 is identical to the method of moments estimator (2.37).
19. Let X ∼Gamma(α, λ). Show that the pdf of Z = 1/X is equal to
λα(z)−α−1e−λ (z)−1
Γ(α)
,
z > 0.
20. Consider the sequence w0, w1, . . ., where w0 = g(θ) is a non-degenerate initial guess
and wt(θ) ∝wt−1(θ)g(τ | θ), t > 1. We assume that g(τ | θ) is not the constant function (with
respect to θ) and that the maximum likelihood value
g(τ |bθ) = max
θ
g(τ | θ) < ∞
exists (is bounded). Let
lt :=
Z
g(τ | θ)wt(θ) dθ.
Show that {lt} is a strictly increasing and bounded sequence. Hence, conclude that its limit
is g(τ |bθ).
Statistical Learning
65
21. Consider the Bayesian model for τ = {x1, . . . , xn} with likelihood g(τ | µ) such that
(X1, . . . , Xn | µ) ∼iid N(µ, 1) and prior pdf g(µ) such that µ ∼N(ν, 1) for some hyperpara-
meter ν. Define a sequence of densities wt(µ), t ⩾2 via wt(µ) ∝wt−1(µ) g(τ | µ), start-
ing with w1(µ) = g(µ). Let at a

nd bt denote the mean and precision4 of µ under the
posterior gt(µ | τ) ∝g(τ | µ)wt(µ). Show that gt(µ | τ) is a normal density with precision
bt = bt−1 + n, b0 = 1 and mean at = (1 −γt)at−1 + γtxn, a0 = ν, where γt := n/(bt−1 + n).
Hence, deduce that gt(µ | τ) converges to a degenerate density with a point-mass at xn.
22. Consider again Example 2.8, where we have a normal model with improper prior
g(θ) = g(µ, σ2) ∝1/σ2. Show that the prior predictive pdf is an improper density g(x) ∝1,
but that the posterior predictive density is
g(x | τ) ∝
 
1 + (x −xn)2
(n + 1)S 2
n
!−n/2
.
Deduce that
X−xn
S n
√(n+1)/(n−1) ∼tn−1.
23. Assuming that X1, . . . , Xn
iid∼f, show that (2.48) holds and that ℓ∗
n = −n E ln f(X).
24. Suppose that τ = {x1, . . . , xn} are observations of iid continuous and strictly positive
random variables, and that there are two possible models for their pdf. The first model
p = 1 is
g(x | θ, p = 1) = θ exp (−θx)
and the second p = 2 is
g(x | θ, p = 2) =
 2θ
π
!1/2
exp
 
−θx2
2
!
.
For both models, assume that the prior for θ is a gamma density
g(θ) = bt
Γ(t)θt−1 exp (−bθ) ,
with the same hyperparameters b and t. Find a formula for the Bayes factor, g(τ | p =
1)/g(τ | p = 2), for comparing these models.
25. Suppose that we have a total of m possible models with prior probabilities g(p), p =
1, . . . , m. Show that the posterior probability of model g(p | τ) can be expressed in terms of
all the p(p −1) Bayes factors:
g(p = i | τ) =
1 +
X
j,i
g(p = j)
g(p = i) Bj | i

−1
.
4The precision is the reciprocal of the variance.
66
Exercises
26. Given the data τ = {x1, . . . , xn}, suppose that we use the likelihood (X | θ) ∼N(µ, σ2)
with parameter θ = (µ, σ2)⊤and wish to compare the following two nested models.
(a) Model p = 1, where σ2 = σ2
0 is known and this is incorporated via the prior
g(θ | p = 1) = g(µ | σ2, p = 1) g(σ2 | p = 1) =
1
√
2πσ
e−(µ−x0)2
2σ2
× δ(σ2 −σ2
0).
(b) Model p = 2, where both mean and variance are unknown with prior
g(θ

 | p = 2) = g(µ | σ2) g(σ2) =
1
√
2πσ
e−(µ−x0)2
2σ2
× bt(σ2)−t−1e−b/σ2
Γ(t)
.
Show that the prior g(θ | p = 1) can be viewed as the limit of the prior g(θ | p = 2) when
t →∞and b = tσ2
0. Hence, conclude that
g(τ | p = 1) = lim
t→∞
b=tσ2
0
g(τ | p = 2)
and use this result to calculate B1 | 2. Check that the formula for B1 | 2 agrees with the Savage–
Dickey density ratio:
g(τ | p = 1)
g(τ | p = 2) = g(σ2 = σ2
0 | τ)
g(σ2 = σ2
0) ,
where g(σ2 | τ) and g(σ2) are the posterior and prior, respectively, under model p = 2.
CHAPTER3
MONTE CARLO METHODS
Many algorithms in machine learning and data science make use of Monte Carlo
techniques. This chapter gives an introduction to the three main uses of Monte Carlo
simulation: to (1) simulate random objects and processes in order to observe their beha-
vior, (2) estimate numerical quantities by repeated sampling, and (3) solve complicated
optimization problems through randomized algorithms.
3.1
Introduction
Briefly put, Monte Carlo simulation
Monte Carlo
simulation
is the generation of random data by means of a com-
puter. These data could arise from simple models, such as those described in Chapter 2,
or from very complicated models describing real-life systems, such as the positions of
vehicles on a complex road network, or the evolution of security prices in the stock mar-
ket. In many cases, Monte Carlo simulation simply involves random sampling from certain
probability distributions. The idea is to repeat the random experiment that is described by
the model many times to obtain a large quantity of data that can be used to answer questions
about the model. The three main uses of Monte Carlo simulation are:
Sampling. Here the objective is to gather information about a random object by observing
many realizations of it. For instance, this could be a random process that mimics the
behavior of some real-life system such as a production line or telecommunications
network. Another usage is found in Bayesian statistics, where Mark

ov chains are
often used to sample from a posterior distribution.
☞
49
Estimation. In this case the emphasis is on estimating certain numerical quantities related
to a simulation model. An example is the evaluation of multidimensional integrals
via Monte Carlo techniques. This is achieved by writing the integral as the expecta-
tion of a random variable, which is then approximated by the sample mean. Appeal-
ing to the Law of Large Numbers guarantees that this approximation will eventually
☞446
converge when the sample size becomes large.
Optimization. Monte Carlo simulation is a powerful tool for the optimization of complic-
ated objective functions. In many applications these functions are deterministic and
67
68
Monte Carlo Sampling
randomness is introduced artificially in order to more efficiently search the domain of
the objective function. Monte Carlo techniques are also used to optimize noisy func-
tions, where the function itself is random; for example, when the objective function
is the output of a Monte Carlo simulation.
The Monte Carlo method dramatically changed the way in which statistics is used in
today’s analysis of data. The ever-increasing complexity of data requires radically different
statistical models and analysis techniques from those that were used 20 to 100 years ago.
By using Monte Carlo techniques, the data analyst is no longer restricted to using basic
(and often inappropriate) models to describe data. Now, any probabilistic model that can
be simulated on a computer can serve as the basis for statistical analysis. This Monte Carlo
revolution has had an impact on both Bayesian and frequentist statistics. In particular, in
frequentist statistics, Monte Carlo methods are often referred to as resampling techniques.
An important example is the well-known bootstrap method [37], where statistical quantit-
ies such as confidence intervals and P-values for statistical tests can simply be determined
by simulation without the need of a sophisticated

 analysis of the underlying probability
distributions; see, for example, [69] for basic applications. The impact on Bayesian statist-
ics has been even more profound, through the use of Markov chain Monte Carlo (MCMC)
techniques [87, 48]. MCMC samplers construct a Markov process which converges in dis-
tribution to a desired (often high-dimensional) density. This convergence in distribution
justifies using a finite run of the Markov process as an approximate random realization
from the target density. The MCMC approach has rapidly gained popularity as a versat-
ile heuristic approximation, partly due to its simple computer implementation and inbuilt
mechanism to tradeoff between computational cost and accuracy; namely, the longer one
runs the Markov process, the better the approximation. Nowadays, MCMC methods are
indispensable for analyzing posterior distributions for inference and model selection; see
also [50, 99].
The following three sections elaborate on these three uses of Monte Carlo simulation
in turn.
3.2
Monte Carlo Sampling
In this section we describe a variety of Monte Carlo sampling methods, from the building
block of simulating uniform random numbers to MCMC samplers.
3.2.1
Generating Random Numbers
At the heart of any Monte Carlo method is a random number generator: a procedure that
random number
generator
produces a stream of uniform random numbers on the interval (0,1). Since such numbers
are usually produced via deterministic algorithms, they are not truly random. However, for
most applications all that is required is that such pseudo-random numbers are statistically
indistinguishable from genuine random numbers U1, U2, . . . that are uniformly distributed
on the interval (0,1) and are independent of each other; we write U1, U2, . . . ∼iid U(0, 1).
For example, in Python the rand method of the numpy.random module is widely used for
this purpose.
Monte Carlo Methods
69
Most random number generators at present are based on linear recurrence relations

.
One of the most important random number generators is the multiple-recursive generator
multiple-
recursive
generator
(MRG) of order k, which generates a sequence of integers Xk, Xk+1, . . . via the linear recur-
rence
Xt = (a1Xt−1 + · · · + akXt−k) mod m,
t = k, k + 1, . . .
(3.1)
for some modulus m and multipliers {ai, i = 1, . . . , k}. Here “mod” refers to the modulo op-
modulus
multipliers
eration: n mod m is the remainder when n is divided by m. The recurrence is initialized by
specifying k “seeds”, X0, . . . , Xk−1. To yield fast algorithms, all but a few of the multipliers
should be 0. When m is a large integer, one can obtain a stream of pseudo-random numbers
Uk, Uk+1, . . . between 0 and 1 from the sequence Xk, Xk+1, . . ., simply by setting Ut = Xt/m.
It is also possible to set a small modulus, in particular m = 2. The output function for such
modulo 2 generators is then typically of the form
modulo 2
generators
Ut =
w
X
i=1
Xtw+i−12−i
for some w ⩽k, e.g., w = 32 or 64. Examples of modulo 2 generators are the feedback shift
register generators, the most popular of which are the Mersenne twisters; see, for example,
feedback shift
register
Mersenne
twisters
[79] and [83]. MRGs with excellent statistical properties can be implemented efficiently
by combining several simpler MRGs and carefully choosing their respective moduli and
multipliers. One of the most successful is L’Ecuyer’s MRG32k3a generator; see [77]. From
now on, we assume that the reader has a sound random number generator available.
3.2.2
Simulating Random Variables
Simulating a random variable X from an arbitrary (that is, not necessarily uniform) distri-
bution invariably involves the following two steps:
1. Simulate uniform random numbers U1, . . . , Uk on (0, 1) for some k = 1, 2, . . ..
2. Return X = g(U1, . . . , Uk), where g is some real-valued function.
The construction of suitable functions g is as much of an art as a science. Many
simulation methods may be found, for example, in [71] 

and the accompanying website
www.montecarlohandbook.org. Two of the most useful general procedures for gen-
erating random variables are the inverse-transform method and the acceptance–rejection
method. Before we discuss these, we show one possible way to simulate standard normal
random variables. In Python we can generate standard normal random variables via the
randn method of the numpy.random module.
Example 3.1 (Simulating Standard Normal Random Variables) If X and Y are in-
dependent standard normally distributed random variables (that is, X, Y ∼iid N(0, 1)), then
their joint pdf is
f(x, y) = 1
2πe−1
2(x2+y2),
(x, y) ∈R2,
which is a radially symmetric function. In Example C.2 we see that, in polar coordin-
☞433
ates, the angle Θ that the random vector [X, Y]⊤makes with the positive x-axis is U(0, 2π)
70
Monte Carlo Sampling
distributed (as would be expected from the radial symmetry) and the radius R has pdf
fR(r) = r e−r2/2, r > 0. Moreover, R and Θ are independent. We will see shortly, in Ex-
ample 3.4, that R has the same distribution as
√
−2 ln U with U ∼U(0, 1). So, to sim-
☞72
ulate X, Y ∼iid N(0, 1), the idea is to first simulate R and Θ independently and then return
X = R cos(Θ) and Y = R sin(Θ) as a pair of independent standard normal random variables.
This leads to the Box–Muller approach for generating standard normal random variables.
Algorithm 3.2.1: Normal Random Variable Simulation: Box–Muller Approach
output: Independent standard normal random variables X and Y.
1 Simulate two independent random variables, U1 and U2, from U(0, 1).
2 X ←(−2 ln U1)1/2 cos(2πU2)
3 Y ←(−2 ln U1)1/2 sin(2πU2)
4 return X, Y
Once a standard normal number generator is available, simulation from any n-
dimensional normal distribution N(µ, Σ) is relatively straightforward. The first step is to
find an n × n matrix B that decomposes Σ into the matrix product BB⊤. In fact there exist
many such decompositions. One of the more important ones is the Cholesky decomposition,
Chol

esky
decomposition
which is a special case of the LU decomposition; see Section A.6.1 for more information
☞368
on such decompositions. In Python, the function cholesky of numpy.linalg can be used
to produce such a matrix B.
Once the Cholesky factorization is determined, it is easy to simulate X ∼N(µ, Σ) as,
by definition, it is the affine transformation µ + BZ of an n-dimensional standard normal
random vector.
Algorithm 3.2.2: Normal Random Vector Simulation
input: µ, Σ
output: X ∼N(µ, Σ)
1 Determine the Cholesky factorization Σ = BB⊤.
2 Simulate Z = [Z1, . . . , Zn]⊤by drawing Z1, . . . , Zn ∼iid N(0, 1).
3 X ←µ + BZ
4 return X
Example 3.2 (Simulating from a Bivariate Normal Distribution) The Python code
below draws N = 1000 iid samples from the two bivariate (n = 2) normal pdfs in Fig-
ure 2.13. The resulting point clouds are given in Figure 3.1.
☞46
bvnormal.py
import numpy as np
from numpy.random import randn
import matplotlib.pyplot as plt
N = 1000
r = 0.0
#change to 0.8 for other plot
Sigma = np.array([[1, r], [r, 1]])
Monte Carlo Methods
71
B = np.linalg.cholesky(Sigma)
x = B @ randn(2,N)
plt.scatter([x[0,:]],[x[1,:]], alpha =0.4, s = 4)
2
0
2
3
2
1
0
1
2
3
2
0
2
3
2
1
0
1
2
3
Figure 3.1: 1000 realizations of bivariate normal distributions with means zero, variances
1, and correlation coefficients 0 (left) and 0.8 (right).
In some cases, the covariance matrix Σ has special structure which can be exploited to
create even faster generation algorithms, as illustrated in the following example.
Example 3.3 (Simulating Normal Vectors in O(n2) Time) Suppose that the random
vector X = [X1, . . . , Xn]⊤represents the values at times t0 + kδ, k = 0, . . . , n −1 of a zero-
mean Gaussian process (X(t), t ⩾0) that is weakly stationary, meaning that Cov(X(s), X(t))
☞238
depends only on t−s. Then clearly the covariance matrix of X, say An, is a symmetric Toep-
litz matrix. Suppose for simplicity that Var X(t) = 1. Then the covariance matrix is in fact
☞379
a correlation m

atrix, and will have the following structure:
An :=

1
a1
. . .
an−2
an−1
a1
1
...
an−2
...
...
...
...
...
an−2
...
...
a1
an−1
an−2
· · ·
a1
1

.
Using the Levinson–Durbin algorithm we can compute a lower diagonal matrix Ln and
a diagonal matrix Dn in O(n2) time such that Ln An L⊤
n = Dn; see Theorem A.14. If we
☞383
simulate Zn ∼N(0, In), then the solution X of the linear system:
Ln X = D1/2
n
Zn
has the desired distribution N(0, An). The linear system is solved in O(n2) time via forward
substitution.
72
Monte Carlo Sampling
3.2.2.1
Inverse-Transform Method
Let X be a random variable with cumulative distribution function (cdf) F. Let F−1 denote
the inverse1 of F and U ∼U(0, 1). Then,
P[F−1(U) ⩽x] = P[U ⩽F(x)] = F(x).
(3.2)
This leads to the following method to simulate a random variable X with cdf F:
Algorithm 3.2.3: Inverse-Transform Method
input: Cumulative distribution function F.
output: Random variable X distributed according to F.
1 Generate U from U(0, 1).
2 X ←F−1(U)
3 return X
The inverse-transform method works both for continuous and discrete distribu-
tions. After importing numpy as np, simulating numbers 0, . . . , k −1 according to
probabilities p0, . . . , pk−1 can be done via np.min(np.where(np.cumsum(p) >
np.random.rand())), where p is the vector of the probabilities.
Example 3.4 (Example 3.1 (cont.)) One remaining issue in Example 3.1 was how to
simulate the radius R when we only know its density fR(r) = r e−r2/2, r > 0. We can use the
inverse-transform method for this, but first we need to determine its cdf. The cdf of R is,
by integration of the pdf,
FR(r) = 1 −e−1
2r2,
r > 0,
and its inverse is found by solving u = FR(r) in terms of r, giving
F−1
R (u) =
p
−2 ln(1 −u),
u ∈(0, 1).
Thus R has the same distribution as √−2 ln(1 −U), with U ∼U(0, 1). Since 1−U also has
a U(0, 1) distribution, R has also the same distribution as
√
−2 ln U.
3.2.2.2
Acceptance–Rejection Method
The acceptance–rejectio

n method is used to sample from a “difficult” probability density
function (pdf) f(x) by generating instead from an “easy” pdf g(x) satisfying f(x) ⩽C g(x)
for some constant C ⩾1 (for example, via the inverse-transform method), and then ac-
cepting or rejecting the drawn sample with a certain probability. Algorithm 3.2.4 gives the
pseudo-code.
The idea of the algorithm is to generate uniformly a point (X, Y) under the graph of the
function Cg, by first drawing X ∼g and then Y ∼U(0,Cg(X)). If this point lies under the
graph of f, then we accept X as a sample from f; otherwise, we try again. The efficiency
of the acceptance–rejection method is usually expressed in terms of the probability of
acceptance, which is 1/C.
1Every cdf has a unique inverse function defined by F−1(u) = inf{x : F(x) ⩾u}. If, for each u, the
equation F(x) = u has a unique solution x, this definition coincides with the usual interpretation of the
inverse function.
Monte Carlo Methods
73
Algorithm 3.2.4: Acceptance–Rejection Method
input: Pdf g and constant C such that Cg(x) ⩾f(x) for all x.
output: Random variable X distributed according to pdf f.
1 found ←false
2 while not found do
3
Generate X from g.
4
Generate U from U(0, 1) independently of X.
5
Y ←UCg(X)
6
if Y ⩽f(X) then found ←true
7 return X
Example 3.5 (Simulating Gamma Random Variables) Simulating random variables
from a Gamma(α, λ) distribution is generally done via the acceptance–rejection method.
Consider, for example, the Gamma distribution with α = 1.3 and λ = 5.6. Its pdf,
☞425
f(x) = λαxα−1e−λx
Γ(α)
,
x ⩾0,
where Γ is the gamma function Γ(α) :=
R ∞
0 e−xxα−1 dx, α > 0, is depicted by the blue solid
curve in Figure 3.2.
0
0.5
1
1.5
2
0
1
2
3
4
5
Figure 3.2: The pdf g of the Exp(4) distribution multiplied by C = 1.2 dominates the pdf f
of the Gamma(1.3, 5.6) distribution.
This pdf happens to lie completely under the graph of Cg(x), where C = 1.2 and
g(x) = 4 exp(−4x), x ⩾0 is the pdf of the exponential distribution Exp(4). Hence, 

we
can simulate from this particular Gamma distribution by accepting or rejecting a sample
from the Exp(4) distribution according to Step 6 of Algorithm 3.2.4. Simulating from the
☞425
Exp(4) distribution can be done via the inverse-transform method: simulate U ∼U(0, 1)
and return X = −ln(U)/4. The following Python code implements Algorithm 3.2.4 for this
example.
74
Monte Carlo Sampling
accrejgamma.py
from math import exp, gamma, log
from numpy.random import rand
alpha = 1.3
lam = 5.6
f = lambda x: lam**alpha * x**(alpha -1) * exp(-lam*x)/gamma(alpha)
g = lambda x: 4*exp(-4*x)
C = 1.2
found = False
while not found:
x = - log(rand())/4
if C*g(x)*rand() <= f(x):
found = True
print(x)
3.2.3
Simulating Random Vectors and Processes
Techniques for generating random vectors and processes are as diverse as the class of
random processes themselves; see, for example, [71]. We highlight a few general scenarios.
When X1, . . . , Xn are independent random variables with pdfs fi, i = 1, . . . , n, so that
their joint pdf is f(x) = f1(x1) · · · fn(xn), the random vector X = [X1, . . . , Xn]⊤can be
☞429
simply simulated by drawing each component Xi ∼fi individually — for example, via the
inverse-transform method or acceptance–rejection.
For dependent components X1, . . . , Xn, we can, as a consequence of the product rule of
probability, represent the joint pdf f(x) as
☞431
f(x) = f(x1, . . . , xn) = f1(x1) f2(x2 | x1) · · · fn(xn | x1, . . . , xn−1),
(3.3)
where f1(x1) is the marginal pdf of X1 and fk(xk | x1, . . . , xk−1) is the conditional pdf of Xk
given X1 = x1, X2 = x2, . . . , Xk−1 = xk−1. Provided the conditional pdfs are known, one can
generate X by first generating X1, then, given X1 = x1, generate X2 from f2(x2 | x1), and so
on, until generating Xn from fn(xn | x1, . . . , xn−1).
The latter method is particularly applicable for generating Markov chains. Recall from
Section C.10 that a Markov chain is a stochastic process {Xt, t = 0, 1, 2, . . .} that satisfies
☞451
Mark

ov chain
the Markov property; meaning that for all t and s the conditional distribution of Xt+s given
Xu, u ⩽t, is the same as that of Xt+s given only Xt. As a result, each conditional density
ft(xt | x1, . . . , xt−1) can be written as a one-step transition density qt(xt | xt−1); that is, the
probability density to go from state xt−1 to state xt in one step. In many cases of interest
the chain is time-homogeneous, meaning that the transition density qt does not depend on
t. Such Markov chains can be generated sequentially, as given in Algorithm 3.2.5.
Monte Carlo Methods
75
Algorithm 3.2.5: Simulate a Markov Chain
input: Number of steps N, initial pdf f0, transition density q.
1 Draw X0 from the initial pdf f0.
2 for t = 1 to N do
3
Draw Xt from the distribution corresponding to the density q(· | Xt−1)
4 return X0, . . . , XN
Example 3.6 (Markov Chain Simulation) For time-homogeneous Markov chains
with a discrete state space, we can visualize the one-step transitions by means of a trans-
ition graph
transition
graph
, where arrows indicate possible transitions between states and the labels de-
scribe the corresponding probabilities. Figure 3.3 shows (on the left) the transition graph
of the Markov chain {Xt, t = 0, 1, 2, . . .} with state space {1, 2, 3, 4} and one-step transition
matrix
P =

0
0.2
0.5
0.3
0.5
0
0.5
0
0.3
0.7
0
0
0.1
0
0
0.9

.
1
2
3
4
0.2
0.7
0.5
0.9
0.5
0.3
0.5
0.3
0.1
0
20
40
60
80
100
1
2
3
4
Figure 3.3: The transition graph (left) and a typical path (right) of the Markov chain.
In the same figure (on the right) a typical outcome (path) of the Markov chain is
shown. The path was simulated using the Python program below. In this implementation
the Markov chain always starts in state 1. We will revisit Markov chains, and in particular
Markov chains with continuous state spaces, in Section 3.2.5.
☞
78
MCsim.py
import numpy as np
import matplotlib.pyplot as plt
n = 101
P = np.array([[0, 0.2, 0.5, 0.3],
[0.5, 0, 0.5, 0]

,
[0.3, 0.7, 0, 0],
[0.1, 0, 0, 0.9]])
x = np.array(np.ones(n, dtype=int))
x[0] = 0
76
Monte Carlo Sampling
for t in range(0,n-1):
x[t+1] = np.min(np.where(np.cumsum(P[x[t],:]) >
np.random.rand()))
x = x + 1
#add 1 to all elements of the vector x
plt.plot(np.array(range(0,n)),x, 'o')
plt.plot(np.array(range(0,n)),x, '--')
plt.show()
3.2.4
Resampling
The idea behind resampling is very simple: an iid sample τ := {x1, . . . , xn} from some
resampling
unknown cdf F represents our best knowledge of F if we make no further a priori as-
sumptions about it. If it is not possible to simulate more samples from F, the best way to
“repeat” the experiment is to resample from the original data by drawing from the empir-
ical cdf Fn; see (1.2). That is, we draw each xi with equal probability and repeat this N
☞11
times, according to Algorithm 3.2.6 below. As we draw here “with replacement”, multiple
instances of the original data points may occur in the resampled data.
Algorithm 3.2.6: Sampling from an Empirical Cdf.
input: Original iid sample x1, . . . , xn and sample size N.
output: Iid sample X∗
1, . . . , X∗
N from the empirical cdf.
1 for t = 1 to N do
2
Draw U ∼U(0, 1)
3
Set I ←⌈nU⌉
4
Set X∗
t ←xI
5 return X∗
1, . . . , X∗
N
In Step 3, ⌈nU⌉returns the ceiling of nU; that is, it is the smallest integer larger than
or equal to nU. Consequently, I is drawn uniformly at random from the set of indices
{1, . . . , n}.
By sampling from the empirical cdf we can thus (approximately) repeat the experiment
that gave us the original data as many times as we like. This is useful if we want to assess
the properties of certain statistics obtained from the data. For example, suppose that the
original data τ gave the statistic t(τ). By resampling we can gain information about the
distribution of the corresponding random variable t(T ).
Example 3.7 (Quotient of Uniforms) Let U1, . . . , Un, V1, . . . , Vn be iid U(0, 1) random
variables and define Xi = Ui/Vi, i = 1, . . . , n. Suppose we wish

 to investigate the distribu-
tion of the sample median eX and sample mean X of the (random) data T := {X1, . . . , Xn}.
Since we know the model for T exactly, we can generate a large number, N say, of inde-
pendent copies of it, and for each of these copies evaluate the sample medians eX1, . . . , eXN
and sample means X1, . . . , XN. For n = 100 and N = 1000 the empirical cdfs might look
like the left and right curves in Figure 3.4, respectively. Contrary to what you might have
expected, the distributions of the sample median and sample mean do not match at all. The
sample median is quite concentrated around 1, whereas the distribution of the sample mean
is much more spread out.
Monte Carlo Methods
77
0
1
2
3
4
5
6
7
0
0.5
1
Figure 3.4: Empirical cdfs of the medians of the resampled data (left curve) and sample
means (right curve) of the resampled data.
Instead of sampling completely new data, we could also reuse the original data by
resampling them via Algorithm 3.2.6. This gives independent copies eX∗
1, . . . , eX∗
N and
X
∗
1, . . . , X
∗
N, for which we can again plot the empirical cdf. The results will be similar
to the previous case. In fact, in Figure 3.4 the cdf of the resampled sample medians and
sample means are plotted. The corresponding Python code is given below. The essential
point of this example is that resampling of data can greatly add to the understanding of the
probabilistic properties of certain measurements on the data, even if the underlying model
is not known. See Exercise 12 for a further investigation of this example.
☞117
quotunif.py
import numpy as np
from numpy.random import rand, choice
import matplotlib.pyplot as plt
from statsmodels.distributions.empirical_distribution import ECDF
n = 100
N = 1000
x = rand(n)/rand(n)
# data
med = np.zeros(N)
ave = np.zeros(N)
for i in range(0,N):
s = choice(x, n, replace=True) # resampled data
med[i] = np.median(s)
ave[i] = np.mean(s)
med_cdf = ECDF(med)
ave_cdf = ECDF(ave)
plt.plot(med_cdf.x, med_c

df.y)
plt.plot(ave_cdf.x, ave_cdf.y)
plt.show()
78
Monte Carlo Sampling
3.2.5
Markov Chain Monte Carlo
Markov chain Monte Carlo (MCMC) is a Monte Carlo sampling technique for (approxim-
Markov chain
Monte Carlo
ately) generating samples from an arbitrary distribution — often referred to as the target
target
distribution. The basic idea is to run a Markov chain long enough such that its limiting
distribution is close to the target distribution. Often such a Markov chain is constructed to
be reversible, so that the detailed balance equations (C.43) can be used. Depending on the
☞453
starting position of the Markov chain, the initial random variables in the Markov chain may
have a distribution that is significantly different from the target (limiting) distribution. The
random variables that are generated during this burn-in period
burn-in period
are often discarded. The
remaining random variables form an approximate and dependent sample from the target
distribution.
In the next two sections we discuss two popular MCMC samplers: the Metropolis–
Hastings sampler and the Gibbs sampler.
3.2.5.1
Metropolis–Hastings Sampler
The Metropolis–Hastings sampler [87] is similar to the acceptance–rejection method in
☞72
that it simulates a trial state, which is then accepted or rejected according to some random
mechanism. Specifically, suppose we wish to sample from a target pdf f(x), where x takes
values in some d-dimensional set. The aim is to construct a Markov chain {Xt, t = 0, 1, . . .}
in such a way that its limiting pdf is f. Suppose the Markov chain is in state x at time t. A
transition of the Markov chain from state x is carried out in two phases. First a proposal
proposal
state Y is drawn from a transition density q(· | x). This state is accepted as the new state,
with acceptance probability
acceptance
probability
α(x, y) = min
( f(y) q(x | y)
f(x) q(y | x), 1
)
,
(3.4)
or rejected otherwise. In the latter case the chain remains in state x. The algorithm just
described can

 be summarized as follows.
Algorithm 3.2.7: Metropolis–Hastings Sampler
input: Initial state X0, sample size N, target pdf f(x), proposal function q(y | x).
output: X1, . . . , XN (dependent), approximately distributed according to f(x).
1 for t = 0 to N −1 do
2
Draw Y ∼q(y | Xt)
// draw a proposal
3
α ←α(Xt, Y)
// acceptance probability as in (3.4)
4
Draw U ∼U(0, 1)
5
if U ⩽α then Xt+1 ←Y
6
else Xt+1 ←Xt
7 return X1, . . . , XN
The fact that the limiting distribution of the Metropolis–Hastings Markov chain is equal
to the target distribution (under general conditions) is a consequence of the following result.
Monte Carlo Methods
79
Theorem 3.1: Local Balance for the Metropolis–Hastings Sampler
The transition density of the Metropolis–Hastings Markov chain satisfies
☞453
the de-
tailed balance equations.
Proof:
We prove the theorem for the discrete case only. Because a transition of the
Metropolis–Hastings Markov chain consists of two steps, the one-step transition probabil-
ity to go from x to y is not q(y | x) but
eq(y | x) =

q(y | x) α(x, y),
if y , x,
1 −P
z,x q(z | x) α(x, z),
if y = x.
(3.5)
We thus need to show that
f(x)eq(y | x) = f(y)eq(x | y)
for all x, y.
(3.6)
With the acceptance probability as in (3.4), we need to check (3.6) for three cases:
(a) x = y,
(b) x , y and f(y)q(x | y) ⩽f(x)q(y | x), and
(c) x , y and f(y)q(x | y) > f(x)q(y | x).
Case (a) holds trivially. For case (b), α(x, y) = f(y)q(x | y)/( f(x)q(y | x)) and α(y, x) = 1.
Consequently,
eq(y | x) = f(y)q(x | y)/ f(x)
and
eq(x | y) = q(x | y),
so that (3.6) holds. Similarly, for case (c) we have α(x, y) = 1 and α(y, x) = f(x)q(y | x)/
( f(y)q(x | y)). It follows that,
eq(y | x) = q(y | x)
and
eq(x | y) = f(x)q(y | x)/ f(y),
so that (3.6) holds again.
□
Thus if the Metropolis–Hastings Markov chain is ergodic, then its limiting pdf is f(x).
☞452
A fortunate property of the algorithm, which is important in many applications, is that in
order to evaluate the acceptance probability α(x, y)

 in (3.4), one only needs to know the
target pdf f(x) up to a constant; that is f(x) = c f(x) for some known function f(x) but
unknown constant c.
The efficiency of the algorithm depends of course on the choice of the proposal trans-
ition density q(y | x). Ideally, we would like q(y | x) to be “close” to the target f(y), irre-
spective of x. We discuss two common approaches.
1. Choose the proposal transition density q(y | x) independent of x; that is, q(y | x) =
g(y) for some pdf g(y). An MCMC sampler of this type is called an independence
sampler
independence
sampler
. The acceptance probability is thus
α(x, y) = min
( f(y) g(x)
f(x) g(y), 1
)
.
80
Monte Carlo Sampling
2. If the proposal transition density is symmetric (that is, q(y | x) = q(x | y)), then the
acceptance probability has the simple form
α(x, y) = min
( f(y)
f(x), 1
)
,
(3.7)
and the MCMC algorithm is called a random walk sampler. A typical example is
random walk
sampler
when, for a given current state x, the proposal state Y is of the form Y = x + Z,
where Z is generated from some spherically symmetric distribution, such as N(0, I).
We now give an example illustrating the second approach.
Example 3.8 (Random Walk Sampler) Consider the two-dimensional pdf
f(x1, x2) = c e−1
4
√
x2
1+x2
2

sin

2
q
x2
1 + x2
2

+ 1

,
−2π < x1 < 2π, −2π < x2 < 2π,
(3.8)
where c is an unknown normalization constant. The graph of this pdf (unnormalized) is
depicted in the left panel of Figure 3.5.
-6
-4
-2
0
2
4
6
-6
-4
-2
0
2
4
6
Figure 3.5: Left panel: the two-dimensional target pdf. Right panel: points from the random
walk sampler are approximately distributed according to the target pdf.
The following Python program implements a random walk sampler to (approximately)
draw N = 104 dependent samples from the pdf f. At each step, given a current state x,
a proposal Y is drawn from the N(x, I) distribution. That is, Y = x + Z, with Z bivariate
standard normal. We see in the right panel of Figure 3.5 that the sampler 

works correctly.
The starting point for the Markov chain is chosen as (0, 0). Note that the normalization
constant c is never required to be specified in the program.
rwsamp.py
import numpy as np
import matplotlib.pyplot as plt
from numpy import pi, exp, sqrt, sin
from numpy.random import rand, randn
Monte Carlo Methods
81
N = 10000
a = lambda x: -2*pi < x
b = lambda x: x < 2*pi
f = lambda x1, x2: exp(-sqrt(x1**2+x2**2)/4)*(
sin(2*sqrt(x1**2+x2**2))+1)*a(x1)*b(x1)*a(x2)*b(x2)
xx = np.zeros((N,2))
x = np.zeros((1,2))
for i in range(1,N):
y = x + randn(1,2)
alpha = np.amin((f(y[0][0],y[0][1])/f(x[0][0],x[0][1]) ,1))
r = rand() < alpha
x = r*y + (1-r)*x
xx[i,:] = x
plt.scatter(xx[:,0], xx[:,1], alpha =0.4,s =2)
plt.axis('equal')
plt.show()
3.2.5.2
Gibbs Sampler
The Gibbs sampler
Gibbs sampler
[48] uses a somewhat different methodology from the Metropolis–
Hastings algorithm and is particularly useful for generating n-dimensional random vectors.
The key idea of the Gibbs sampler is to update the components of the random vector
one at a time, by sampling them from conditional pdfs. Thus, Gibbs sampling can be
advantageous if it is easier to sample from the conditional distributions than from the joint
distribution.
Specifically, suppose that we wish to sample a random vector X = [X1, . . . , Xn]⊤ac-
cording to a target pdf f(x). Let f(xi | x1, . . . , xi−1, xi+1, . . . , xn) represent the conditional
pdf2 of the i-th component, Xi, given the other components x1, . . . , xi−1, xi+1, . . . , xn. The
Gibbs sampling algorithm is as follows.
Algorithm 3.2.8: Gibbs Sampler
input: Initial point X0, sample size N, and target pdf f.
output: X1, . . . , XN approximately distributed according to f.
1 for t = 0 to N −1 do
2
Draw Y1 from the conditional pdf f(y1 | Xt,2, . . . , Xt,n).
3
for i = 2 to n do
4
Draw Yi from the conditional pdf f(yi | Y1, . . . , Yi−1, Xt,i+1, . . . , Xt,n).
5
Xt+1 ←Y
6 return X1, . . . , XN
There exist many variants of the Gibbs sampler, depending on the 

steps required to
update Xt to Xt+1 — called the cycle of the Gibbs algorithm. In the algorithm above, the
cycle
2In this section we employ a Bayesian notation style, using the same letter f for different (conditional)
densities.
82
Monte Carlo Sampling
cycle consists of Steps 2–5, in which the components are updated in a fixed order 1 →2 →
· · · →n. For this reason Algorithm 3.2.8 is also called the systematic Gibbs sampler.
systematic
Gibbs sampler
In the random-order Gibbs sampler, the order in which the components are updated
random-order
Gibbs sampler
in each cycle is a random permutation of {1, . . . , n} (see Exercise 9). Other modifications
☞115
are to update the components in blocks (i.e., several at the same time), or to update only
a random selection of components. The variant where in each cycle only a single random
component is updated is called the random Gibbs sampler. In the reversible Gibbs sampler
random Gibbs
sampler
reversible
Gibbs sampler
a cycle consists of the coordinate-wise updating 1 →2 →· · · →n −1 →n →n −1 →
· · · →2 →1. In all cases, except for the systematic Gibbs sampler, the resulting Markov
chain {Xt, t = 1, 2, . . .} is reversible and hence its limiting distribution is precisely f(x).
☞452
Unfortunately, the systematic Gibbs Markov chain is not reversible and so the detailed
balance equations are not satisfied. However, a similar result holds, due to Hammersley and
Clifford, under the so-called positivity condition: if at a point x = (x1, . . . , xn) all marginal
densities f(xi) > 0, i = 1, . . . , n, then the joint density f(x) > 0.
Theorem 3.2: Hammersley–Clifford Balance for the Gibbs Sampler
Let q1→n(y | x) denote the transition density of the systematic Gibbs sampler, and let
qn→1(x | y) be the transition density of the reverse move, in the order n →n −1 →
· · · →1. Then, if the positivity condition holds,
f(x) q1→n(y | x) = f(y) qn→1(x | y).
(3.9)
Proof: For the forward move we have:
q1→n(y | x) = f(y1 | x2, . . . , xn)f(y2 |

 y1, x3, . . . , xn) · · · f(yn | y1, . . . , yn−1),
and for the reverse move:
qn→1(x | y) = f(xn | y1, . . . , yn−1) f(xn−1 | y1, . . . , yn−2, xn) · · · f(x1 | x2, . . . , xn).
Consequently,
q1→n(y | x)
qn→1(x | y)
=
n
Y
i=1
f(yi | y1, . . . , yi−1, xi+1, . . . , xn)
f(xi | y1, . . . , yi−1, xi+1, . . . , xn)
=
n
Y
i=1
f(y1, . . . , yi, xi+1, . . . , xn)
f(y1, . . . , yi−1, xi, . . . , xn)
=
f(y) Qn−1
i=1 f(y1, . . . , yi, xi+1, . . . , xn)
f(x) Qn
j=2 f(y1, . . . , y j−1, xj, . . . , xn)
=
f(y) Qn−1
i=1 f(y1, . . . , yi, xi+1, . . . , xn)
f(x) Qn−1
j=1 f(y1, . . . , y j, x j+1, . . . , xn)
= f(y)
f(x).
The result follows by rearranging the last identity. The positivity condition ensures that we
do not divide by 0 along the line.
□
Intuitively, the long-run proportion of transitions x →y for the “forward move” chain
is equal to the long-run proportion of transitions y →x for the “reverse move” chain.
Monte Carlo Methods
83
To verify that the Markov chain X0, X1, . . . for the systematic Gibbs sampler indeed has
limiting pdf f(x), we need to check that the global balance equations (C.42) hold. By
☞452
integrating (in the continuous case) both sides in (3.9) with respect to x, we see that indeed
Z
f(x) q1→n(y | x) dx = f(y).
Example 3.9 (Gibbs Sampler for the Bayesian Normal Model) Gibbs samplers are
often applied in Bayesian statistics, to sample from the posterior pdf. Consider for instance
the Bayesian normal model
☞
51
f(µ, σ2) = 1/σ2
(x | µ, σ2) ∼N(µ1, σ2I).
Here the prior for (µ, σ2) is improper.
improper prior
That is, it is not a pdf in itself, but by obstinately
applying Bayes’ formula it does yield a proper posterior pdf. In some sense this prior
conveys the least amount of information about µ and σ2. Following the same procedure as
in Example 2.8, we find the posterior pdf:
f(µ, σ2 | x) ∝

σ2−n/2−1 exp
(
−1
2
P
i(xi −µ)2
σ2
)
.
(3.10)
Note that µ and σ2 here are the “variables” and x is a fixed data vector. To simulate samples
µ and σ2 from (3.10) using

 the Gibbs sampler, we need the distributions of both (µ | σ2, x)
and (σ2 | µ, x). To find f(µ | σ2, x), view the right-hand side of (3.10) as a function of µ
only, regarding σ2 as a constant. This gives
f(µ | σ2, x) ∝exp
(
−nµ2 −2µ P
i xi
2σ2
)
= exp
(
−µ2 −2µx
2(σ2/n)
)
∝exp
(
−1
2
(µ −x)2
σ2/n
)
.
(3.11)
This shows that (µ | σ2, x) has a normal distribution with mean x and variance σ2/n.
Similarly, to find f(σ2 | µ, x), view the right-hand side of (3.10) as a function of σ2,
regarding µ as a constant. This gives
f(σ2 | µ, x) ∝(σ2)−n/2−1 exp
−1
2
n
X
i=1
(xi −µ)2/σ2
,
(3.12)
showing that (σ2 | µ, x) has an inverse-gamma distribution with parameters n/2 and
☞425
Pn
i=1(xi −µ)2/2. The Gibbs sampler thus involves the repeated simulation of
(µ | σ2, x) ∼N

x, σ2/n

and
(σ2 | µ, x) ∼InvGamma
n/2,
n
X
i=1
(xi −µ)2/2
.
Simulating X ∼InvGamma(α, λ) is achieved by first generating Z ∼Gamma(α, λ) and
then returning X = 1/Z.
84
Monte Carlo Sampling
In our parameterization of the Gamma(α, λ) distribution, λ is the rate parameter.
Many software packages instead use the scale parameter c = 1/λ. Be aware of this
when simulating Gamma random variables.
The Python script below defines a small data set of size n = 10 (which was randomly
simulated from a standard normal distribution), and implements the systematic Gibbs
sampler to simulate from the posterior distribution, using N = 105 samples.
gibbsamp.py
import numpy as np
import matplotlib.pyplot as plt
x = np.array([[-0.9472, 0.5401, -0.2166, 1.1890, 1.3170,
-0.4056, -0.4449, 1.3284, 0.8338, 0.6044]])
n=x.size
sample_mean = np.mean(x)
sample_var = np.var(x)
sig2 = np.var(x)
mu=sample_mean
N=10**5
gibbs_sample = np.array(np.zeros((N, 2)))
for k in range(N):
mu=sample_mean + np.sqrt(sig2/n)*np.random.randn()
V=np.sum((x-mu)**2)/2
sig2 = 1/np.random.gamma(n/2, 1/V)
gibbs_sample[k,:]= np.array([mu, sig2])
plt.scatter(gibbs_sample[:,0], gibbs_sample[:,1],alpha =0.1,s =1)
plt.plot(np.mean(x), np.var(x),'w

o')
plt.show()
-1
-0.5
0
0.5
1
1.5
2
7
0
0.5
1
1.5
^f(7 j x)
Figure 3.6: Left: approximate draws from the posterior pdf f(µ, σ2 | x) obtained via the
Gibbs sampler. Right: estimate of the posterior pdf f(µ | x).
Monte Carlo Methods
85
The left panel of Figure 3.6 shows the (µ, σ2) points generated by the Gibbs sampler.
Also shown, via the white circle, is the point (x, s2), where x = 0.3798 is the sample mean
and s2 = 0.6810 the sample variance. This posterior point cloud visualizes the considerable
uncertainty in the estimates. By projecting the (µ, σ2) points onto the µ-axis — that is,
by ignoring the σ2 values — one obtains (approximate) samples from the posterior pdf
of µ; that is, f(µ | x). The right panel of Figure 3.6 shows a kernel density estimate (see
Section 4.4) of this pdf. The corresponding 0.025 and 0.975 sample quantiles were found
☞134
to be −0.2054 and 0.9662, respectively, giving the 95% credible interval (−0.2054, 0.9662)
for µ, which contains the true expectation 0. Similarly, an estimated 95% credible interval
for σ2 is (0.3218, 2.2485), which contains the true variance 1.
3.3
Monte Carlo Estimation
In this section we describe how Monte Carlo simulation can be used to estimate complic-
ated integrals, probabilities, and expectations. A number of variance reduction techniques
are introduced as well, including the recent cross-entropy method.
3.3.1
Crude Monte Carlo
The most common setting for Monte Carlo estimation is the following: Suppose we wish to
compute the expectation µ = EY of some (say continuous) random variable Y with pdf f,
but the integral EY =
R
yf(y) dy is difficult to evaluate. For example, if Y is a complicated
function of other random variables, it would be difficult to obtain an exact expression for
f(y). The idea of crude Monte Carlo — sometimes abbreviated as CMC — is to approx-
crude Monte
Carlo
imate µ by simulating many independent copies Y1, . . . , YN of Y and then take their sample
mean Y as an estimator of µ. All that

 is needed is an algorithm to simulate such copies.
By the Law of Large Numbers, Y converges to µ as N →∞, provided the expectation
☞446
of Y exists. Moreover, by the Central Limit Theorem, Y approximately has a N(µ, σ2/N)
☞447
distribution for large N, provided that the variance σ2 = VarY < ∞. This enables the con-
struction of an approximate (1 −α) confidence interval for µ:
confidence
interval
 
Y −z1−α/2
S
√
N
,
Y + z1−α/2
S
√
N
!
,
(3.13)
where S is the sample standard deviation of the {Yi} and zγ denotes the γ-quantile of the
N(0, 1) distribution; see also Section C.13. Instead of specifying the confidence interval,
☞457
one often reports only the sample mean and the estimated standard error: S/
√
N, or the
estimated
standard error
estimated relative error: S/(Y
√
N). The basic estimation procedure for independent data
estimated
relative error
is summarized in Algorithm 3.3.1 below.
It is often the case that the output Y is a function of some underlying random vector or
stochastic process; that is, Y = H(X), where H is a real-valued function and X is a random
vector or process. The beauty of Monte Carlo for estimation is that (3.13) holds regardless
of the dimension of X.
86
Monte Carlo Estimation
Algorithm 3.3.1: Crude Monte Carlo for Independent Data
input: Simulation algorithm for Y ∼f, sample size N, confidence level 1 −α.
output: Point estimate and approximate (1 −α) confidence interval for µ = EY.
1 Simulate Y1, . . . , YN
iid∼f.
2 Y ←1
N
PN
i=1 Yi
3 S 2 ←
1
N−1
PN
i=1(Yi −Y)2
4 return Y and the interval (3.13).
Example 3.10 (Monte Carlo Integration) In Monte Carlo integration, simulation is
Monte Carlo
integration
used to evaluate complicated integrals. Consider, for example, the integral
µ =
Z ∞
−∞
Z ∞
−∞
Z ∞
−∞
p
|x1 + x2 + x3| e−(x2
1+x2
2+x2
3)/2 dx1 dx2 dx3.
Defining Y = |X1 + X2 + X3|1/2(2π)3/2, with X1, X2, X3
iid∼N(0, 1), we can write µ = EY.
Using the following Python program, with a sample size of N = 106, we obtained an
estimate Y = 17.031 wi

th an approximate 95% confidence interval (17.017, 17.046).
mcint.py
import numpy as np
from numpy import pi
c = (2*pi)**(3/2)
H = lambda x: c*np.sqrt(np.abs(np.sum(x,axis=1)))
N = 10**6
z = 1.96
x = np.random.randn(N,3)
y = H(x)
mY = np.mean(y)
sY = np.std(y)
RE = sY/mY/np.sqrt(N)
print('Estimate = {:3.3f}, CI = ({:3.3f},{:3.3f})'.format(
mY, mY*(1-z*RE), mY*(1+z*RE)))
Estimate = 17.031, CI = (17.017,17.046)
Example 3.11 (Example 2.1 (cont.)) We return to the bias–variance tradeoff in Ex-
ample 2.1. Figure 2.7 gives estimates of the (squared-error) generalization risk (2.5) as
☞26
☞29
☞23
a function of the number of parameters in the model. But how accurate are these estim-
ates? Because we know in this case the exact model for the data, we can use Monte Carlo
simulation to estimate the generalization risk (for a fixed training set) and the expected
generalization risk (averaged over all training sets) precisely. All we need to do is repeat
the data generation, fitting, and validation steps many times and then take averages of the
results. The following Python code repeats 100 times:
1. Simulate the training set of size n = 100.
Monte Carlo Methods
87
2. Fit models up to size k = 8.
3. Estimate the test loss using a test set with the same sample size n = 100.
Figure 3.7 shows that there is some variation in the test losses, due to the randomness in
both the training and test sets. To obtain an accurate estimate of the expected generalization
risk (2.6), take the average of the test losses. We see that for k ⩽8 the estimate in Figure 2.7
is close to the true expected generalization risk.
1
2
3
4
5
6
7
8
Number of parameters p
25
50
75
100
125
150
175
200
Test loss
Figure 3.7: Independent estimates of the test loss show some variability.
CMCtestloss.py
import numpy as np, matplotlib.pyplot as plt
from numpy.random import rand, randn
from numpy.linalg import solve
def generate_data(beta, sig, n):
u = rand(n, 1)
y = (u ** np.arange(0, 4)) @ beta + sig * randn(n, 1)
ret

urn u, y
beta = np.array([[10, -140, 400, -250]]).T
n = 100
sig = 5
betahat = {}
plt.figure(figsize=[6,5])
totMSE = np.zeros(8)
max_p = 8
p_range = np.arange(1, max_p + 1, 1)
88
Monte Carlo Estimation
for N in range(0,100):
u, y = generate_data(beta, sig, n)
#training data
X = np.ones((n, 1))
for p in p_range:
if p > 1:
X = np.hstack((X, u**(p-1)))
betahat[p] = solve(X.T @ X, X.T @ y)
u_test, y_test = generate_data(beta, sig, n)
#test data
MSE = []
X_test = np.ones((n, 1))
for p in p_range:
if p > 1:
X_test = np.hstack((X_test, u_test**(p-1)))
y_hat = X_test @ betahat[p] # predictions
MSE.append(np.sum((y_test - y_hat)**2/n))
totMSE = totMSE + np.array(MSE)
plt.plot(p_range , MSE,'C0',alpha=0.1)
plt.plot(p_range ,totMSE/N,'r-o')
plt.xticks(ticks=p_range)
plt.xlabel('Number of parameters $p$')
plt.ylabel('Test loss')
plt.tight_layout()
plt.savefig('MSErepeatpy.pdf',format='pdf')
plt.show()
3.3.2
Bootstrap Method
The bootstrap method [37] combines CMC estimation with the resampling procedure of
Section 3.2.4. The idea is as follows: Suppose we wish to estimate a number µ via some
☞76
estimator Y = H(T ), where T := {X1, . . . , Xn} is an iid sample from some unknown cdf
F. It is assumed that Y does not depend on the order of the {Xi}. To assess the quality (for
example, accuracy) of the estimator Y, one could draw independent replications T1, . . . , TN
of T and find sample estimates for quantities such as the variance VarY, the bias EY −µ,
and the mean squared error E(Y −µ)2. However, it may be too time-consuming or simply
not feasible to obtain such replications. An alternative is to resample the original data.
To reiterate, given an outcome τ = {x1, . . . , xn} of T , we simulate an iid sample T ∗:=
{X∗
1, . . . , X∗
n} from the empirical cdf Fn, via Algorithm 3.2.6 (hence the resampling size is
☞76
N = n here).
The rationale is that the empirical cdf Fn is close to the actual cdf F and gets closer as
n gets larger. Hence, any quantities depending on F, such as EFg

(Y), where g is a function,
can be approximated by EFng(Y). The latter is usually still difficult to evaluate, but it can
be simply estimated via CMC as
1
K
K
X
i=1
g(Y∗
i ),
Monte Carlo Methods
89
where Y∗
1, . . . , Y∗
K are independent random variables, each distributed as Y∗= H(T ∗). This
seemingly self-referent procedure is called bootstrapping — alluding to Baron von Mün-
bootstrapping
chausen, who pulled himself out of a swamp by his own bootstraps. As an example, the
bootstrap estimate of the expectation of Y is
c
EY = Y
∗= 1
K
K
X
i=1
Y∗
i ,
which is simply the sample mean of {Y∗
i }. Similarly, the bootstrap estimate for VarY is the
sample variance
[
VarY =
1
K −1
K
X
i=1
(Y∗
i −Y
∗)2.
(3.14)
Bootstrap estimators for the bias and MSE are Y
∗−Y and 1
K
PK
i=1(Y∗
i −Y)2, respectively.
Note that for these estimators the unknown quantity µ is replaced with its original estimator
Y. Confidence intervals can be constructed in the same fashion. We mention two variants:
the normal method and the percentile method. In the normal method, a 1 −α confidence
normal method
percentile
method
interval for µ is given by
(Y ± z1−α/2S ∗),
where S ∗is the bootstrap estimate of the standard deviation of Y; that is, the square root
of (3.14). In the percentile method, the upper and lower bounds of the 1 −α confidence
interval for µ are given by the 1 −α/2 and α/2 quantiles of Y, which in turn are estimated
via the corresponding sample quantiles of the bootstrap sample {Y∗
i }.
The following example illustrates the usefulness of the bootstrap method for ratio es-
timation and also introduces the renewal reward process model for data.
Example 3.12 (Bootstrapping the Ratio Estimator) A common scenario in stochastic
simulation is that the output of the simulation consists of independent pairs of data
(C1, R1), (C2, R2), . . ., where each C is interpreted as the length of a period of time — a so-
called cycle — and R is the reward obtained during that cycle. Such a collection of ran

dom
variables {(Ci, Ri)} is called a renewal reward process
renewal
reward process
. Typically, the reward Ri depends on
the cycle length Ci. Let At be the average reward earned by time t; that is, At = PNt
i=1 Ri/t,
where Nt = max{n : C1 + · · · + Cn ⩽t} counts the number of complete cycles at time t. It
can be shown, see Exercise 20, that if the expectations of the cycle length and reward are
☞119
finite, then At converges to the constant ER/EC. This ratio can thus be interpreted as the
long-run average reward
long-run
average reward
.
Estimation of the ratio ER/EC from data (C1, R1), . . . , (Cn, Rn) is easy: take the ratio
estimator
ratio estimator
A = R
C
.
However, this estimator A is not unbiased and it is not obvious how to derive confidence
intervals. Fortunately, the bootstrap method can come to the rescue: simply resample the
pairs {(Ci, Ri)}, obtain ratio estimators A∗
1, . . . , A∗
K, and from these compute quantities of
interest such as confidence intervals.
As a concrete example, let us return to the Markov chain in Example 3.6. Recall that
☞
75
the chain starts at state 1 at time 0. After a certain amount of time T1, the process returns
90
Monte Carlo Estimation
to state 1. The time steps 0, . . . , T1 −1 form a natural “cycle” for this process, as from
time T1 onwards the process behaves probabilistically exactly the same as when it started,
independently of X0, . . . , XT1−1. Thus, if we define T0 = 0, and let Ti be the i-th time that
the chain returns to state 1, then we can break up the time interval into independent cycles
of lengths Ci = Ti −Ti−1, i = 1, 2, . . .. Now suppose that during the i-th cycle a reward
Ri =
Ti−1
X
t=Ti−1
ϱt−Ti−1 r(Xt)
is received, where r(i) is some fixed reward for visiting state i ∈{1, 2, 3, 4} and ϱ ∈(0, 1)
is a discounting factor. Clearly, {(Ci, Ri)} is a renewal reward process. Figure 3.8 shows the
outcomes of 1000 pairs (C, R), using r(1) = 4, r(2) = 3, r(3) = 10, r(4) = 1, and ϱ = 0.9.
0
10
20
30
40
50
60
70
Cyc

le length
0
10
20
30
40
50
60
Reward
Figure 3.8: Each circle represents a (cycle length, reward) pair. The varying circle sizes
indicate the number of occurrences for a given pair. For example, (2,15.43) is the most
likely pair here, occurring 186 out of a 1000 times. It corresponds to the cycle path 1 →
3 →2 →1.
The long-run average reward is estimated as 2.50 for our data. But how accurate is this
estimate? Figure 3.9 shows a density plot of the bootstrapped ratio estimates, where we
independently resampled the data pairs 1000 times.
2.2
2.4
2.6
2.8
long-run average reward
0
2
4
density
Figure 3.9: Density plot of the bootstrapped ratio estimates for the Markov chain renewal
reward process.
Monte Carlo Methods
91
Figure 3.9 indicates that the true long-run average reward lies between 2.2 and 2.8
with high confidence. More precisely, the 99% bootstrap confidence interval (percentile
method) is here (2.27, 2.77). The following Python script spells out the procedure.
ratioest.py
import numpy as np, matplotlib.pyplot as plt, seaborn as sns
from numba import jit
np.random.seed(123)
n = 1000
P = np.array([[0, 0.2, 0.5, 0.3],
[0.5 ,0, 0.5, 0],
[0.3, 0.7, 0, 0],
[0.1, 0, 0, 0.9]])
r = np.array([4,3,10,1])
Corg = np.array(np.zeros((n,1)))
Rorg = np.array(np.zeros((n,1)))
rho=0.9
@jit()
#for speed-up; see Appendix
def generate_cyclereward(n):
for i in range(n):
t=1
xreg = 1
#regenerative state
(out of 1,2,3,4)
reward = r[0]
x= np.amin(np.argwhere(np.cumsum(P[xreg -1,:]) > np.random.
rand())) + 1
while x != xreg:
t += 1
reward += rho**(t-1)*r[x-1]
x = np.amin(np.where(np.cumsum(P[x-1,:]) > np.random.rand
())) + 1
Corg[i] = t
Rorg[i] = reward
return Corg, Rorg
Corg, Rorg = generate_cyclereward(n)
Aorg = np.mean(Rorg)/np.mean(Corg)
K = 5000
A = np.array(np.zeros((K,1)))
C = np.array(np.zeros((n,1)))
R = np.array(np.zeros((n,1)))
for i in range(K):
ind = np.ceil(n*np.random.rand(1,n)).astype(int)[0]-1
C = Corg[ind]
R = Rorg[ind]
A[i] = np.mean(R)/np.mean(C)
plt.xlabel('long-run

 average reward')
plt.ylabel('density')
sns.kdeplot(A.flatten(),shade=True)
plt.show()
92
Monte Carlo Estimation
3.3.3
Variance Reduction
The estimation of performance measures in Monte Carlo simulation can be made more
efficient by utilizing known information about the simulation model. Variance reduction
techniques include antithetic variables, control variables, importance sampling, conditional
Monte Carlo, and stratified sampling; see, for example, [71, Chapter 9]. We shall only deal
with control variables and importance sampling here.
Suppose Y is the output of a simulation experiment. A random variable eY, obtained
from the same simulation run, is called a control variable
control
variable
for Y if Y and eY are correlated
(negatively or positively) and the expectation of eY is known. The use of control variables
for variance reduction is based on the following theorem. We leave its proof to Exercise 21.
☞119
Theorem 3.3: Control Variable Estimation
Let Y1, . . . , YN be the output of N independent simulation runs and let eY1, . . . , eYN be
the corresponding control variables, with EeYk = eµ known. Let ϱY,eY be the correlation
coefficient between each Yk and eYk. For each α ∈R the estimator
bµ(c) = 1
N
N
X
k=1
h
Yk −α
eYk −eµ
i
(3.15)
is an unbiased estimator for µ = EY. The minimal variance of bµ(c) is
Var bµ(c) = 1
N (1 −ϱ2
Y,eY) Var Y,
(3.16)
which is obtained for α = ϱY,eY
q
VarY/VareY.
From (3.16) we see that, by using the optimal α in (3.15), the variance of the control
variate estimator is a factor 1 −ϱ2
Y,eY smaller than the variance of the crude Monte Carlo
estimator. Thus, if eY is highly correlated with Y, a significant variance reduction can be
achieved. The optimal α is usually unknown, but it can be easily estimated from the sample
covariance matrix of {(Yk, eYk)}.
☞456
In the next example, we estimate the multiple integral in Example 3.10 using control
variables.
Example 3.13 (Monte Carlo Integration (cont.))
☞86
The random variable Y = |X1 +X

2 +
X3|1/2(2π)3/2 is positively correlated with the random variable eY = X2
1 + X2
2 + X2
3, for the
same choice of X1, X2, X3
iid∼N(0, 1). As EeY = Var(X1 + X2 + X3) = 3, we can use it as a
control variable to estimate the expectation of Y. The following Python program is based
on Theorem 3.3. It imports the crude Monte Carlo sampling code from Example 3.10.
Monte Carlo Methods
93
mcintCV.py
from mcint import *
Yc = np.sum(x**2, axis=1) # control variable data
yc = 3
# true expectation of control variable
C = np.cov(y,Yc) # sample covariance matrix
cor = C[0][1]/np.sqrt(C[0][0]*C[1][1])
alpha = C[0][1]/C[1][1]
est = np.mean(y-alpha*(Yc-yc))
RECV = np.sqrt((1-cor**2)*C[0][0]/N)/est
#relative error
print('Estimate = {:3.3f}, CI = ({:3.3f},{:3.3f}), Corr = {:3.3f}'.
format(est, est*(1-z*RECV), est*(1+z*RECV),cor))
Estimate = 17.045, CI = (17.032,17.057), Corr = 0.480
A typical estimate of the correlation coefficient ϱY,eY is 0.48, which gives a reduction of
the variance with a factor 1−0.482 ≈0.77 — a simulation speed-up of 23% compared with
crude Monte Carlo. Although the gain is small in this case, due to the modest correlation
between Y and eY, little extra work was required to achieve this variance reduction.
One of the most important variance reduction techniques is importance sampling
importance
sampling
. This
technique is especially useful for the estimation of very small probabilities. The standard
setting is the estimation of a quantity
µ = Ef H(X) =
Z
H(x) f(x) dx,
(3.17)
where H is a real-valued function and f the probability density of a random vector X,
called the nominal pdf. The subscript f is added to the expectation operator to indicate that
nominal pdf
it is taken with respect to the density f.
Let g be another probability density such that g(x) = 0 implies that H(x) f(x) = 0.
Using the density g we can represent µ as
µ =
Z
H(x) f(x)
g(x) g(x) dx = Eg
"
H(X) f(X)
g(X)
#
.
(3.18)
Consequently, if X1, . . . , XN ∼iid g, then
bµ = 1
N
N
X
k=1
H(Xk) f(X

k)
g(Xk)
(3.19)
is an unbiased estimator of µ. This estimator is called the importance sampling estimator
importance
sampling
estimator
and g is called the importance sampling density. The ratio of densities, f(x)/g(x), is called
the likelihood ratio. The importance sampling pseudo-code is given in Algorithm 3.3.2.
likelihood ratio
94
Monte Carlo Estimation
Algorithm 3.3.2: Importance Sampling Estimation
input: Function H, importance sampling density g such that g(x) = 0 for all x for
which H(x)f(x) = 0, sample size N, confidence level 1 −α.
output: Point estimate and approximate (1 −α) confidence interval for
µ = EH(X), where X ∼f.
1 Simulate X1, . . . , XN
iid∼g and let Yi = H(Xi)f(Xi)/g(Xi), i = 1, . . . , N.
2 Estimate µ via bµ = Y and determine an approximate (1 −α) confidence interval as
I :=
 
bµ −z1−α/2
S
√
N
, bµ + z1−α/2
S
√
N
!
,
where zγ denotes the γ-quantile of the N(0, 1) distribution and S is the sample
standard deviation of Y1, . . . , YN.
3 return bµ and the interval I.
Example 3.14 (Importance Sampling) Let us examine the workings of importance
sampling by estimating the area, µ say, under the graph of the function
M(x1, x2) = e−1
4
√
x2
1+x2
2

sin

2
q
x2
1 + x2
2

+ 1

,
(x1, x2) ∈R2.
(3.20)
We saw a similar function in Example 3.8 (but note the different domain). A natural ap-
☞80
proach to estimate the area is to truncate the domain to the square [−b, b]2, for large enough
b, and to estimate the integral
µb =
Z b
−b
Z b
−b
(2b)2M(x)
|      {z      }
H(x)
f(x) dx = Ef H(X)
via crude Monte Carlo, where f(x) = 1/(2b)2, x ∈[−b, b]2, is the pdf of the uniform distri-
bution on [−b, b]2. Here is the Python code which does just that.
impsamp1.py
import numpy as np
from numpy import exp, sqrt, sin, pi, log, cos
from numpy.random import rand
b = 1000
H = lambda x1, x2: (2*b)**2 * exp(-sqrt(x1**2+x2**2)/4)*(sin(2*sqrt(
x1**2+x2**2))+1)*(x1**2 + x2**2 < b**2)
f = 1/((2*b)**2)
N = 10**6
X1 = -b + 2*b*rand(N,1)
X2 = -b + 2*b*rand(N,1)
Z = H(X1,X2)
est

CMC = np.mean(Z).item()
# to obtain scalar
RECMC = np.std(Z)/estCMC/sqrt(N).item()
print('CI = ({:3.3f},{:3.3f}), RE = {: 3.3f}'.format(estCMC*(1-1.96*
RECMC), estCMC*(1+1.96*RECMC),RECMC))
Monte Carlo Methods
95
CI = (82.663,135.036), RE =
0.123
For a truncation level of b = 1000 and a sample size of N = 106, a typical estimate is
108.8, with an estimated relative error of 0.123. We have two sources of error here. The
first is the error in approximating µ by µb. However, as the function H decays exponentially
fast, b = 1000 is more than enough to ensure this error is negligible. The second type of
error is the statistical error, due to the estimation process itself. This can be quantified by
the estimated relative error, and can be reduced by increasing the sample size.
Let us now consider an importance sampling approach in which the importance
sampling pdf g is radially symmetric and decays exponentially in the radius, similar to the
function H. In particular, we simulate (X1, X2) in a way akin to Example 3.1, by first gen-
☞
69
erating a radius R ∼Exp(λ) and an angle Θ ∼U(0, 2π), and then returning X1 = R cos(Θ)
and X2 = R sin(Θ). By the Transformation Rule (Theorem C.4) we then have
☞433
g(x) = fR,Θ(r, θ)1
r = λ e−λr 1
2π
1
r = λe−λ√
x2
1+x2
2
2π
q
x2
1 + x2
2
,
x ∈R2 \ {0}.
The following code, which imports the one given above, implements the importance
sampling steps, using the parameter λ = 0.1.
impsamp2.py
from impsamp1 import *
lam = 0.1;
g = lambda x1, x2: lam*exp(-sqrt(x1**2 + x2**2)*lam)/sqrt(x1**2 + x2
**2)/(2*pi);
U = rand(N,1); V = rand(N,1)
R = -log(U)/lam
X1 = R*cos(2*pi*V)
X2 = R*sin(2*pi*V)
Z = H(X1,X2)*f/g(X1,X2)
estIS = np.mean(Z).item()
# obtain scalar
REIS = np.std(Z)/estIS/sqrt(N).item()
print('CI = ({:3.3f},{:3.3f}), RE = {: 3.3f}'.format(estIS*(1-1.96*
REIS), estIS*(1+1.96*REIS),REIS))
CI = (100.723,101.077), RE =
0.001
A typical estimate is 100.90 with an estimated relative error of 1 · 10−4, which gives
a substantial variance reduction. I

n terms of approximate 95% confidence intervals, we
have (82.7,135.0) in the CMC case versus (100.7,101.1) in the importance sampling case.
Of course, we could have reduced the truncation level b to improve the performance of
CMC, but then the approximation error might become more significant. For the importance
sampling case, the relative error is hardly affected by the threshold level, but does depend
on the choice of λ. We chose λ such that the decay rate is slower than the decay rate of the
function H, which is 0.25.
As illustrated in the above example, a main difficulty in importance sampling is how to
choose the importance sampling distribution. A poor choice of g may seriously affect the
96
Monte Carlo for Optimization
accuracy of both the estimate and the confidence interval. The theoretically optimal choice
g∗for the importance sampling density minimizes the variance of bµ and is therefore the
solution to the functional minimization program
min
g Varg
 
H(X) f(X)
g(X)
!
.
(3.21)
It is not difficult to show, see also Exercise 22, that if either H(x) ⩾0 or H(x) ⩽0 for all
☞119
x, then the optimal importance sampling pdf
optimal
importance
sampling pdf
is
g∗(x) = H(x) f(x)
µ
.
(3.22)
Namely, in this case Varg∗bµ = Varg∗(H(X)f(X)/g(X)) = Varg∗µ = 0, so that the estimatorbµ
is constant under g∗. An obvious difficulty is that the evaluation of the optimal importance
sampling density g∗is usually not possible, since g∗(x) in (3.22) depends on the unknown
quantity µ. Nevertheless, one can typically choose a good importance sampling density g
“close” to the minimum variance density g∗.
One of the main considerations for choosing a good importance sampling pdf is that
the estimator (3.19) should have finite variance. This is equivalent to the requirement
that
Eg
"
H2(X) f 2(X)
g2(X)
#
= Ef
"
H2(X) f(X)
g(X)
#
< ∞.
(3.23)
This suggests that g should not have lighter tails than f and that, preferably, the
likelihood ratio, f/g, should be bounded.
3.4
Monte Carlo for Op

timization
In this section we describe several Monte Carlo methods for optimization. Such random-
ized algorithms can be useful for solving optimization problems with many local optima
and complicated constraints, possibly involving a mix of continuous and discrete variables.
Randomized algorithms are also used to solve noisy optimization problems, in which the
objective function is unknown and has to be obtained via Monte Carlo simulation.
3.4.1
Simulated Annealing
Simulated annealing is a Monte Carlo technique for minimization that emulates the phys-
Simulated
annealing
ical state of atoms in a metal when the metal is heated up and then slowly cooled down.
When the cooling is performed very slowly, the atoms settle down to a minimum-energy
state. Denoting the state as x and the energy of a state as S (x), the probability distribution
of the (random) states is described by the Boltzmann pdf
f(x) ∝e−S (x)
kT ,
x ∈X,
where k is Boltzmann’s constant and T is the temperature.
Monte Carlo Methods
97
Going beyond the physical interpretation, suppose that S (x) is an arbitrary function to
be minimized, with x taking values in some discrete or continuous set X. The Gibbs pdf
Gibbs pdf
corresponding to S (x) is defined as
fT(x) = e−S (x)
T
zT
,
x ∈X,
provided that the normalization constant zT := P
x exp(−S (x)/T) is finite. Note that this
is simply the Boltzmann pdf with the Boltzmann constant k removed. As T →0, the pdf
becomes more and more peaked around the set of global minimizers of S .
The idea of simulated annealing is to create a sequence of points X1, X2, . . . that are ap-
proximately distributed according to pdfs fT1(x), fT2(x), . . ., where T1, T2, . . . is a sequence
of “temperatures” that decreases (is “cooled”) to 0 — known as the annealing schedule. If
annealing
schedule
each Xt were sampled exactly from fTt, then Xt would converge to a global minimum of
S (x) as Tt →0. However, in practice sampling is approximate and convergence to a global
minimum is not 

assured. A generic simulated annealing algorithm is as follows.
Algorithm 3.4.1: Simulated Annealing
input: Annealing schedule T0, T1, . . . ,, function S , initial value x0.
output: Approximations to the global minimizer x∗and minimum value S (x∗).
1 Set X0 ←x0 and t ←1.
2 while not stopping do
3
Approximately simulate Xt from fTt(x).
4
t ←t + 1
5 return Xt, S (Xt)
A popular annealing schedule is geometric cooling, where Tt = β Tt−1, t = 1, 2, . . ., for
geometric
cooling
a given initial temperature T0 and a cooling factor β ∈(0, 1). Appropriate values for T0
cooling factor
and β are problem-dependent and this has traditionally required tuning on the part of the
user. A possible stopping criterion is to stop after a fixed number of iterations, or when the
temperature is “small enough”.
Approximate sampling from a Gibbs distribution is most often carried out via Markov
chain Monte Carlo. For each iteration t, the Markov chain should theoretically run for a
large number of steps to accurately sample from the Gibbs pdf fTt. However, in practice,
one often only runs a single step of the Markov chain, before updating the temperature, as
in Algorithm 3.4.2 below.
To sample from a Gibbs distribution fT, this algorithm uses a random walk Metropolis–
Hastings sampler. From (3.7), the acceptance probability of a proposal y is thus
☞
80
α(x, y) = min

e−1
T S (y)
e−1
T S (x), 1
= min
n
e−1
T (S (y)−S (x)), 1
o
.
Hence, if S (y) < S (x), then the proposal is aways accepted. Otherwise, the proposal is
accepted with probability exp(−1
T (S (y) −S (x))).
98
Monte Carlo for Optimization
Algorithm 3.4.2: Simulated Annealing with a Random Walk Sampler
input: Objective function S , starting state X0, initial temperature T0, number of
iterations N, symmetric proposal density q(y | x), constant β.
output: Approximate minimizer and minimum value of S .
1 for t = 0 to N −1 do
2
Simulate a new state Y from the symmetric proposal q(y | Xt).
3
if S (Y) < S (Xt) then
4
Xt+1 ←Y


5
else
6
Draw U ∼U(0, 1).
7
if U ⩽e−(S (Y)−S (Xt))/Tt then
8
Xt+1 ←Y
9
else
10
Xt+1 ←Xt
11
Tt+1 ←β Tt
12 return XN and S (XN)
Example 3.15 (Simulated Annealing for Minimization) Let us minimize the “wig-
gly” function depicted in the bottom panel of Figure 3.10 and given by:
S (x) =

−e−x2/100 sin(13x −x4)5 sin(1 −3x2)2,
if −2 ⩽x ⩽2,
∞,
otherwise.
0
1
2
3
4
5
f(x) = e!S(x)=T
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
x
-1
0
1
S(x)
T = 1
T = 0:4
T = 0:2
Figure 3.10: Lower panel: the “wiggly” function S (x). Upper panel: three (normalized)
Gibbs pdfs for temperatures T = 1, 0.4, 0.2. As the temperature decreases, the Gibbs pdf
converges to the pdf that has all its mass concentrated at the minimizer of S .
Monte Carlo Methods
99
The function has many local minima and maxima, with a global minimum around 1.4.
The figure also illustrates the relationship between S and the (unnormalized) Gibbs pdf fT.
The following Python code implements a slight variant of Algorithm 3.4.2 where, in-
stead of stopping after a fixed number of iterations, the algorithm stops when the temper-
ature is lower than some threshold (here 10−3).
Instead of stopping after a fixed number N of iterations or when the temperature
is low enough, it is useful to stop when consecutive function values are closer than
some distance ε to each other, or when the best found function value has not changed
over a fixed number d of iterations.
For a “current” state x, the proposal state Y is here drawn from the N(x, 0.52) distri-
bution. We use geometric cooling with decay parameter β = 0.999 and initial temperature
T0 = 1. We set the initial state to x0 = 0. Figure 3.11 depicts a realization of the sequence
of states xt for t = 0, 1, . . .. After initially fluctuating wildly, the sequence settles down
to a value around 1.37, with S (1.37) = −0.92, corresponding to the global optimizer and
minimum, respectively.
simann.py
import numpy as np
import matplotlib.pyplot as plt
def wiggly(x):
y = -np.exp(x**2/100)*np.sin(13*

x-x**4)**5*np.sin(1-3*x**2)**2
ind = np.vstack((np.argwhere(x<-2),np.argwhere(x>2)))
y[ind]=float('inf')
return y
S = wiggly
beta = 0.999
sig = 0.5
T=1
x= np.array([0])
xx=[]
Sx=S(x)
while T>10**(-3):
T=beta*T
y = x+sig*np.random.randn()
Sy = S(y)
alpha = np.amin((np.exp(-(Sy-Sx)/T),1))
if np.random.uniform()<alpha:
x=y
Sx=Sy
xx=np.hstack((xx,x))
print('minimizer = {:3.3f}, minimum ={:3.3f}'.format(x[0],Sx[0]))
plt.plot(xx)
plt.show()
minimizer = 1.365, minimum = -0.958
100
Monte Carlo for Optimization
0
1000
2000
3000
4000
5000
6000
7000
number of iterations
2.0
1.5
1.0
0.5
0.0
0.5
1.0
1.5
2.0
state
Figure 3.11: Typical states generated by the simulated annealing algorithm.
3.4.2
Cross-Entropy Method
The cross-entropy (CE) method [103] is a simple Monte Carlo algorithm that can be used
cross-entropy
for both optimization and estimation.
The basic idea of the CE method for minimizing a function S on a set X is to define
a parametric family of probability densities {f(· | v), v ∈V} on X and to iteratively update
the parameter v so that f(· | v) places more mass on states x that have smaller S values than
on the previous iteration. In particular, the CE algorithm has two basic phases:
• Sampling: Samples X1, . . . , XN are drawn independently according to f(· | v). The
objective function S is evaluated at these points.
• Updating: A new parameter v′ is selected on the basis of those Xi for which S (Xi) ⩽
γ for some level γ. These {Xi} form the elite sample set, E.
elite sample
At each iteration the level parameter γ is chosen as the worst of the Nelite := ⌈ϱN⌉
best performing samples, where ϱ ∈(0, 1) is the rarity parameter
rarity
parameter
— typically, ϱ = 0.1 or
ϱ = 0.01. The parameter v is updated as a smoothed average αv′+(1−α)v, where α ∈(0, 1)
is the smoothing parameter
smoothing
parameter
and
v′ := argmax
v∈V
X
X∈E
ln f(X | v).
(3.24)
The updating rule (3.24) is the result of minimizing the Kullback–Leibler divergence
between the conditional density of X ∼f(x |

 v) given S (X) ⩽γ, and f(x; v); see [103].
Note that (3.24) yields the maximum likelihood estimator (MLE) of v based on the elite
☞456
samples. Hence, for many specific families of distributions, explicit solutions can be found.
An important example is where X ∼N(µ, diag(σ2)); that is, X has independent Gaussian
Monte Carlo Methods
101
components. In this case, the mean vector µ and the vector of variances σ2 are simply
updated via the sample mean and sample variance of the elite samples. This is known as
normal updating. A generic CE procedure for minimization is given in Algorithm 3.4.3.
normal
updating
Algorithm 3.4.3: Cross-Entropy Method for Minimization
input: Function S, initial sampling parameter v0, sample size N, rarity parameter
ϱ, smoothing parameter α.
output: Approximate minimum of S and optimal sampling parameter v.
1 Initialize v0, set Nelite ←⌈ϱN⌉and t ←0.
2 while a stopping criterion is not met do
3
t ←t + 1
4
Simulate an iid sample X1, . . . , XN from the density f(· | vt−1).
5
Evaluate the performances S (X1), . . . , S (XN) and sort them from smallest to
largest: S (1), . . . , S (N).
6
Let γt be the sample ϱ-quantile of the performances:
γt ←S (Nelite).
(3.25)
7
Determine the set of elite samples Et = {Xi : S (Xi) ⩽γt}.
8
Let v′
t be the MLE of the elite samples:
v′
t ←argmax
v
X
X∈Et
ln f(X | v).
(3.26)
9
Update the sampling parameter as
vt ←αv′
t + (1 −α)vt−1.
(3.27)
10 return γt, vt
The CE algorithm produces a sequence of pairs (γ1, v1), (γ2, v2), . . . , such that γt con-
verges (approximately) to the minimal function value, and f(· | vt) to a degenerate pdf that
(approximately) concentrates all its mass at a minimizer of S , as t →∞. A possible stop-
ping condition is to stop when the sampling distribution f(· | vt) is sufficiently close to a
degenerate distribution. For normal updating this means that the standard deviation is suf-
ficiently small.
The output of the CE algorithm could also include the overall best function value
and corr

esponding solution.
In the following example, we minimize the same function as in Example 3.15, but
☞
97
instead use the CE algorithm.
Example 3.16 (Cross-Entropy Method for Minimization) In this case we take the
family of normal distributions {N(µ, σ2)} for the sampling step (Step 4 of Algorithm 3.4.3),
starting with µ = 0 and σ = 3. The choice of the initial parameter is quite arbitrary, as long
as σ is large enough to sample a wide range of points. We take N = 100 samples at each it-
eration, set ϱ = 0.1, and keep the Nelite = 10 = ⌈Nϱ⌉smallest ones as the elite samples. The
parameters µ and σ are then updated via the sample mean and sample standard deviation
102
Monte Carlo for Optimization
of the elite samples. In this case we do not use any smoothing (α = 1). In the following
Python code the 100 × 2 matrix Sx stores the x-values in the first column and the func-
tion values in the second column. The rows of this matrix are sorted in ascending order
according to the function values, giving the matrix sortSx. The first Nelite = 10 rows of
this sorted matrix correspond to the elite samples and their function values. The updating
of µ and σ is done in Lines 14 and 15. Figure 3.12 shows how the pdfs of the N(µt, σ2
t )
sampling distributions degenerate to the point mass at the global minimizer 1.366.
CEmethod.py
from simann import wiggly
import numpy as np
np.set_printoptions(precision=3)
mu, sigma = 0, 3
N, Nel = 100, 10
eps = 10**-5
S = wiggly
while sigma > eps:
X = np.random.randn(N,1)*sigma + np.array(np.ones((N,1)))*mu
Sx = np.hstack((X, S(X)))
sortSx = Sx[Sx[:,1].argsort(),]
Elite = sortSx[0:Nel,:-1]
mu = np.mean(Elite, axis=0)
sigma = np.std(Elite, axis=0)
print('S(mu)= {}, mu: {}, sigma: {}\n'.format(S(mu), mu, sigma))
S(mu)= [0.071], mu: [0.414], sigma: [0.922]
S(mu)= [0.063], mu: [0.81], sigma: [0.831]
S(mu)= [-0.033], mu: [1.212], sigma: [0.69]
S(mu)= [-0.588], mu: [1.447], sigma: [0.117]
S(mu)= [-0.958], mu: [1.366], sigma: [0.007]
S(mu)= [-0.958], mu: 

[1.366], sigma: [0.]
S(mu)= [-0.958], mu: [1.366], sigma: [3.535e-05]
S(mu)= [-0.958], mu: [1.366], sigma: [2.023e-06]
Monte Carlo Methods
103
-2
-1
0
1
2
3
x
0
0.5
1
1.5
2
f(x; 7; <)
5 
4 
2 
 1 
iteration 0
3 
Figure 3.12: The normal pdfs of the first six sampling distributions, truncated to the interval
[−2, 3]. The initial sampling distribution is N(0, 32).
3.4.3
Splitting for Optimization
Minimizing a function S (x), x ∈X is closely related to drawing a random sample from a
level set of the form {x ∈X : S (x) ⩽γ}. Suppose S has minimum value γ∗attained at x∗.
level set
As long as γ ⩾γ∗, this level set contains the minimizer. Moreover, if γ is close to γ∗, the
volume of this level set will be small. So, a randomly selected point from this set is expected
to be close to x∗. Thus, by gradually decreasing the level parameter γ, the level sets will
gradually shrink towards the set {x∗}. Indeed, the CE method was developed with exactly
this connection in mind; see, e.g., [102]. Note that the CE method employs a parametric
sampling distribution to obtain samples from the level sets (the elite samples). In [34]
a non-parametric sampling mechanism is introduced that uses an evolving collection of
particles. The resulting optimization algorithm, called splitting for continuous optimization
splitting for
continuous
optimization
(SCO), provides a fast and accurate way to optimize complicated continuous functions. The
details of SCO are given in Algorithm 3.4.4.
At iteration t = 0, the algorithm starts with a population of particles Y0 = {Y1, . . . , YN}
that are uniformly generated on some bounded region B, which is large enough to contain
a global minimizer. The function values of all particles in Y0 are sorted, and the best
Nelite = ⌈Nϱ⌉form the elite particle set X1, exactly as in the CE method. Next, the elite
particles are “split” into ⌊N/Nelite⌋children particles, adding one extra child to some of
the elite particles to ensure that the total number of children is aga

in N. The purpose of
Line 4 is to randomize which elite particles receive an extra child. Lines 8–15 describe
how the children of the i-th elite particle are generated. First, in Line 9, we select one
of the other elite particles uniformly at random. The same line defines an n-dimensional
vector σi whose components are the absolute differences between the vectors X(i) and X(I),
104
Monte Carlo for Optimization
Algorithm 3.4.4: Splitting for Continuous Optimization (SCO)
input: Objective function S , sample size N, rarity parameter ϱ, scale factor w,
bounded region B ⊂X that is known to contain a global minimizer, and
maximum number of attempts MaxTry.
output: Final iteration number t and sequence (Xbest,1, b1), . . . , (Xbest,t, bt) of best
solutions and function values at each iteration.
1 Simulate Y0 = {Y1, . . . , YN} uniformly on B. Set t ←0 and Nelite ←⌈Nϱ⌉.
2 while stopping condition is not satisfied do
3
Determine the Nelite smallest values, S (1) ⩽· · · ⩽S (Nelite), of {S (X), X ∈Yt},
and store the corresponding vectors, X(1), . . . , X(Nelite), in Xt+1. Set bt+1 ←S (1)
and Xbest,t+1 ←X(1).
4
Draw Bi ∼Bernoulli( 1
2), i = 1, . . . , Nelite, with PNelite
i=1 Bi = N mod Nelite.
5
for i = 1 to Nelite do
6
Ri ←
j
N
Nelite
k
+ Bi
// random splitting factor
7
Y ←X(i); Y′ ←Y
8
for j = 1 to Ri do
9
Draw I ∈{1, . . . , Nelite} \ {i} uniformly and let σi ←w|X(i) −X(I)|.
10
Simulate a uniform permutation π = (π1, . . . , πn) of (1, . . . , n).
11
for k = 1 to n do
12
for Try = 1 to MaxTry do
13
Y′(πk) ←Y(πk) + σi(πk)Z,
Z ∼N(0, 1)
14
if S (Y′) < S (Y) then Y ←Y′ and break.
15
Add Y to Yt+1
16
t ←t + 1
17 return {(Xbest,k, bk), k = 1, . . . , t}
multiplied by a constant w. That is,
σi = w |X(i) −X(I)| := w

|X(i),1 −X(I),1|
|X(i),2 −X(I),2|
...
|X(i),n −X(I),n|

.
Next, a uniform random permutation π of (1, . . . , n) is simulated (see Exercise 9). Lines
☞115
11–14 describe how, starting from a candidate child point Y, each coordinate of

 Y is re-
sampled, in the order determined by π, by adding a standard normal random variable to
that component, multiplied by the corresponding component of σi (Line 13). If the result-
ing Y′ has a function value that is less than that of Y, then the new candidate is accepted.
Otherwise, the same coordinate is tried again. If no improvement is found in MaxTry at-
tempts, the original component is retained. This process is performed for all elite samples,
to produce the first-generation population Y1. The procedure is then repeated for iterations
t = 1, 2, . . ., until some stopping criterion is met, e.g., when the best found function value
does not change for a number of consecutive iterations, or when the total number of func-
tion evaluations exceeds some threshold. The best found function value and corresponding
Monte Carlo Methods
105
argument (particle) are returned at the conclusion of the algorithm.
The input variable MaxTry governs how much computational time is dedicated to up-
dating a component. In most cases we have encountered, the choices w = 0.5 and MaxTry
= 5 work well. Empirically, relatively high value for ϱ work well, such as ϱ = 0.4, 0.8, or
even ϱ = 1. The latter case means that at each stage t all samples from Yt−1 carry over to
the elite set Xt.
Example 3.17 (Test Problem 112) Hock and Schittkowski [58] provide a rich source
of test problems for multiextremal optimization. A challenging one is Problem 112, where
the goal is to find x so as to minimize the function
S (x) =
10
X
j=1
x j
 
c j + ln
xj
x1 + · · · + x10
!
,
subject to the following set of constraints:
x1 + 2x2 + 2x3 + x6 + x10 −2
=
0,
x4 + 2x5 + x6 + x7 −1
=
0,
x3 + x7 + x8 + 2x9 + x10 −1
=
0,
x j
⩾
0.000001,
j = 1, . . . , 10,
where the constants {ci} are given in Table 3.1.
Table 3.1: Constants for Test Problem 112.
c1 = −6.089
c2 = −17.164
c3 = −34.054
c4 = −5.914
c5 = −24.721
c6 = −14.986
c7 = −24.100
c8 = −10.708
c9 = −26.662
c10 = −22.179
The best known minimal value in [58]

 was −47.707579. In [89] a better solution was
found, −47.760765, using a genetic algorithm. The corresponding solution vector was
completely different from the one in [58]. A further improvement,−47.76109081, was
found in [70], using the CE method, giving a similar solution vector to that in [89]:
0.04067247
0.14765159
0.78323637
0.00141368
0.48526222
0.00069291
0.02736897
0.01794290
0.03729653
0.09685870
To obtain a solution with SCO, we first converted this 10-dimensional problem into a
7-dimensional one by defining the objective function
S 7(y) = S (x),
where x2 = y1, x3 = y2, x5 = y3, x6 = y4, x7 = y5, x9 = y6, x10 = y7, and
x1
=
2 −(2y1 + 2y2 + y4 + x7),
x4
=
1 −(2y3 + y4 + y5),
x8
=
1 −(y2 + y5 + 2y6 + y7),
subject to x1, . . . , x10 ⩾0.000001, where the {xi} are taken as functions of the {yi}. We then
adopted a penalty approach (see Section B.4) by adding a penalty function to the original
☞415
106
Monte Carlo for Optimization
objective function:
eS 7(y) = S (x) + 1000
10
X
i=1
max{−(xi −0.000001), 0},
where, again, the {xi} are defined in terms of the {yi} as above.
Optimizing this last function with SCO, we found, in less time than the other al-
gorithms, a slightly smaller function value: −47.761090859365858, with solution vector
0.040668102417464
0.147730393049955
0.783153291185250
0.001414221643059
0.485246633088859
0.000693172682617
0.027399339496606
0.017947274343948
0.037314369272343
0.096871356429511
in line with the earlier solutions.
3.4.4
Noisy Optimization
In noisy optimization
noisy
optimization
, the objective function is unknown, but estimates of function val-
ues are available, e.g., via simulation. For example, to find an optimal prediction function
g in supervised learning, the exact risk ℓ(g) = E Loss(Y, g(x)) is usually unknown and
only estimates of the risk are available. Optimizing the risk is thus typically a noisy op-
☞20
timization problem. Noisy optimization features prominently in simulation studies where
the behavior of some syst

em (e.g., vehicles on a road network) is simulated under certain
parameters (e.g., the lengths of the traffic light intervals) and the aim is to choose those
parameters optimally (e.g., to maximize the traffic throughput). For each parameter setting
the exact value for the objective function is unknown but estimates can be obtained via the
simulation.
In general, suppose the goal is to minimize a function S , where S is unknown, but
an estimate of S (x) can be obtained for any choice of x ∈X. Because the gradient ∇S is
unknown, one cannot directly apply classical optimization methods. The stochastic approx-
imation method mimics the classical gradient descent method by replacing a deterministic
stochastic
approximation
gradient with an estimate c
∇S (x).
A simple estimator for the i-th component of ∇S (x) (that is, ∂S (u)/∂xi), is the central
difference estimator
central
difference
estimator
bS (x + ei δ/2) −bS (x −ei δ/2)
δ
,
(3.28)
where ei denotes the i-th unit vector, and bS (x+ei δ/2) and bS (x−ei δ/2) can be any estimators
of S (x + ei δ/2) and S (x −ei δ/2), respectively. The difference parameter δ > 0 should be
small enough to reduce the bias of the estimator, but large enough to keep the variance of
the estimator small.
To reduce the variance in the estimator (3.28) it is important to have bS (x + ei δ/2)
and bS (x −ei δ/2) positively correlated. This can for example be achieved by using
common random numbers
common random
numbers
in the simulation.
Monte Carlo Methods
107
In direct analogy to gradient descent methods, the stochastic approximation method
☞412
produces a sequence of iterates, starting with some x1 ∈X, via
xt+1 = xt −βt c
∇S (xt),
(3.29)
where β1, β2, . . . is a sequence of strictly positive step sizes. A generic stochastic approx-
imation algorithm for minimizing a function S is thus as follows.
Algorithm 3.4.5: Stochastic Approximation
input: A mechanism to estimate any gradient ∇S (x) and step sizes β1, β2, . . ..
output: Approximate optim

izer of S .
1 Initialize x1 ∈X. Set t ←1.
2 while a stopping criterion is not met do
3
Obtain an estimated gradient c
∇S (xt) of S at xt.
4
Determine a step size βt.
5
Set xt+1 ←xt −βt c
∇S (xt).
6
t ←t + 1
7 return xt
When c
∇S (xt) is an unbiased estimator of ∇S (xt) in (3.29) the stochastic approxima-
tion Algorithm 3.4.5 is referred to as the Robbins–Monro algorithm. When finite differ-
Robbins–Monro
ences are used to estimate c
∇S (xt), as in (3.28), the resulting algorithm is known as the
Kiefer–Wolfowitz algorithm. In Section 9.4.1 we will see how stochastic gradient descent
Kiefer–
Wolfowitz
is employed in deep learning to minimize the training loss, based on a “minibatch” of
training data.
☞335
It can be shown [72] that, under certain regularity conditions on S , the sequence
x1, x2, . . . converges to the true minimizer x∗when the step sizes decrease slowly enough
to 0; in particular, when
∞
X
t=1
βt = ∞
and
∞
X
t=1
β2
t < ∞.
(3.30)
In practice, one rarely uses step sizes that satisfy (3.30), as the convergence of the
sequence will be too slow to be of practical use.
An alternative approach to stochastic approximation is the stochastic counterpart
stochastic
counterpart
method, also called sample average approximation. It can be applied in situations where
the noisy objective function is of the form
S (x) = EeS (x, ξ),
x ∈X,
(3.31)
where ξ is a random vector that can be simulated and eS (x, ξ) can be evaluated exactly. The
idea is to replace the optimization of (3.31) with that of the sample average
bS (x) = 1
N
N
X
i=1
eS (x, ξi),
x ∈X,
(3.32)
where ξ1, . . . , ξN are iid copies of ξ. Note that bS is a deterministic function of x and so can
be optimized using any optimization algorithm. A solution to this sample average version
is taken to be an estimator of a solution x∗to the original problem (3.31).
108
Monte Carlo for Optimization
Example 3.18 (Determining Good Importance Sampling Parameters) The selection
of good importance sampling parameters can be

 viewed as a stochastic optimization prob-
lem. Consider, for instance, the importance sampling estimator in Example 3.14. Recall
☞94
that the nominal distribution is the uniform distribution on the square [−b, b]2, with pdf
fb(x) =
1
(2b)2,
x ∈[−b, b]2,
where b is large enough to ensure that µb is close to µ; in that example, we chose b = 1000.
The importance sampling pdf is
gλ(x) = fR,Θ(r, θ)1
r = λe−λr 1
2π
1
r = λe−λ√
x2
1+x2
2
2π
q
x2
1 + x2
2
,
x = (x1, x2) ∈R2 \ {0},
which depends on a free parameter λ. In the example we chose λ = 0.1. Is this the best
choice? Maybe λ = 0.05 or 0.2 would have resulted in a more accurate estimate. The im-
portant thing to realize is that the “effectiveness” of λ can be measured in terms of the
variance of the estimator bµ in (3.19), which is given by
☞93
1
N Vargλ
 
H(X) f(X)
gλ(X)
!
= 1
N Egλ
"
H2(X) f 2(X)
g2
λ(X)
#
−µ2
N = 1
N Ef
"
H2(X) f(X)
gλ(X)
#
−µ2
N .
Hence, the optimal parameter λ∗minimizes the function S (λ) = Ef[H2(X) f(X)/gλ(X)],
which is unknown, but can be estimated from simulation. To solve this stochastic minim-
ization problem, we first use stochastic approximation. Thus, at each step of the algorithm,
the gradient of S (λ) is estimated from realizations of bS (λ) = H2(X)f(X)/gλ(X), where
X ∼fb. As in the original problem (that is, the estimation of µ), the parameter b should
be large enough to avoid any bias in the estimator of λ∗, but also small enough to en-
sure a small variance. The following Python code implements a particular instance of Al-
gorithm 3.4.5. For sampling from fb here, we used b = 100 instead of b = 1000, as this will
improve the crude Monte Carlo estimation of λ∗, without noticeably affecting the bias. The
gradient of S (λ) is estimated in Lines 11–17, using the central difference estimator (3.28).
Notice how for the S (λ−δ/2) and S (λ+δ/2) the same random vector X = [X1, X2]⊤is used.
This significantly reduces the variance of the gradient estimator; see also Exercise 23. The
☞119
step 

size βt should be such that βt c
∇S (xt) ≈λt. Given the large gradient here, we choose
β0 = 10−7 and decrease it each step by a factor of 0.99. Figure 3.13 shows how the se-
quence λ0, λ1, . . . decreases towards approximately 0.125, which we take as an estimator
for the optimal importance sampling parameter λ∗.
stochapprox.py
import numpy as np
from numpy import pi
import matplotlib.pyplot as plt
b=100
# choose b large enough, but not too large
delta = 0.01
H = lambda x1, x2: (2*b)**2*np.exp(-np.sqrt(x1**2 + x2**2)/4)*(np.
Monte Carlo Methods
109
sin(2*np.sqrt(x1**2+x2**2)+1))*(x1**2+x2**2<b**2)
f = 1/(2*b)**2
g = lambda x1, x2, lam: lam*np.exp(-np.sqrt(x1**2+x2**2)*lam)/np.
sqrt(x1**2+x2**2)/(2*pi)
beta = 10**-7
#step size very small, as the gradient is large
lam=0.25
lams = np.array([lam])
N=10**4
for i in range(200):
x1 = -b + 2*b*np.random.rand(N,1)
x2 = -b + 2*b*np.random.rand(N,1)
lamL = lam - delta/2
lamR = lam + delta/2
estL = np.mean(H(x1,x2)**2*f/g(x1, x2, lamL))
estR = np.mean(H(x1,x2)**2*f/g(x1, x2, lamR))
#use SAME x1,x2
gr = (estR-estL)/delta
#gradient
lam = lam - gr*beta
#gradient descend
lams = np.hstack((lams, lam))
beta = beta*0.99
lamsize=range(0, (lams.size))
plt.plot(lamsize , lams)
plt.show()
0
25
50
75
100
125
150
175
200
steps
0.12
0.14
0.16
0.18
0.20
0.22
0.24
Figure 3.13: The stochastic optimization algorithm produces a sequence λt, t = 0, 1, 2, . . .
that tends to an approximate estimate of the optimal importance sampling parameter λ∗≈
0.125.
Next, we estimate λ∗using a stochastic counterpart approach. As the objective function
S (λ) is of the form (3.31) (with λ taking the role of x and X the role of ξ), we obtain the
sample average
bS (λ) = 1
N
N
X
i=1
H2(Xi) f(Xi)
gλ(Xi),
(3.33)
where X1, . . . , XN ∼iid fb. Once the X1, . . . , XN ∼iid fb have been simulated, bS (λ) is a de-
terministic function of λ, which can be optimized by any means. We take the most basic
110
Monte Carlo for Optimization
approach and simply evaluate the function 

for λ = 0.01, 0.02, . . . , 0.3 and select the min-
imizing λ on this grid. The code is given below and Figure 3.14 shows bS (λ) as a function
of λ. The minimum value found was 0.60·104 for minimizerbλ∗= 0.12, which is in accord-
ance with the value obtained via stochastic approximation. The sensitivity of this estimate
can be assessed from the graph: for a wide range of values (say from 0.04 to 0.15) bS stays
rather flat. So any of these values could be used in an importance sampling procedure to
estimate µ. However, very small values (less than 0.02) and large values (greater than 0.25)
should be avoided. Our original choice of λ = 0.1 was therefore justified and we could not
have done much better.
stochcounterpart.py
from stochapprox import *
lams = np.linspace(0.01, 0.31, 1000)
res=[]
res = np.array(res)
for i in range(lams.size):
lam = lams[i]
np.random.seed(1)
g = lambda x1, x2: lam*np.exp(-np.sqrt(x1**2+x2**2)*lam)/np.sqrt
(x1**2+x2**2)/(2*pi)
X=-b+2*b*np.random.rand(N,1)
Y=-b+2*b*np.random.rand(N,1)
Z=H(X,Y)**2*f/g(X,Y)
estCMC = np.mean(Z)
res = np.hstack((res, estCMC))
plt.plot(lams, res)
plt.xlabel(r'$\lambda$')
plt.ylabel(r'$\hat{S}(\lambda)$')
plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))
plt.show()
Monte Carlo Methods
111
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.5
1.0
1.5
2.0
2.5
3.0
S( )
1e4
Figure 3.14: The stochastic counterpart method replaces the unknown S (λ) (that is, the
scaled variance of the importance sampling estimator) with its sample average, bS (λ). The
minimum value of bS is attained around λ = 0.12.
A third method for stochastic optimization is the cross-entropy method. In particular,
Algorithm 3.4.3 can easily be modified to minimize noisy functions S (x) = EeS (x, ξ), as
☞101
defined in (3.31). The only change required in the algorithm is that every function value
S (x) be replaced by its estimate bS (x). Depending on the level of noise in the function, the
sample size N might have to be increased considerably.
Example 3.19

 (Cross-Entropy Method for Noisy Optimization) To explore the use
of the CE method for noisy optimization, take the following noisy discrete optimization
problem. Suppose there is a “black box” that contains an unknown binary sequence of n
bits. If one feeds the black box any input vector, it will first scramble the input by inde-
pendently flipping the bits (changing 0 to 1 and 1 to 0) with a probability θ and then return
the number of bits that do not match the true (unknown) binary sequence. This is illustrated
in Figure 3.15 for n = 10.
Figure 3.15: A noisy optimization function as a black box. The input to the black box is a
binary vector. Inside the black box the digits of the input vector are scrambled by flipping
bits with probability θ. The output is the number of bits of the scrambled vector that do not
match the true (unknown) binary vector.
112
Monte Carlo for Optimization
Denoting by S (x) the true number of matching digits for a binary input vector x, the
black box thus returns a noisy estimate bS (x). The objective is to estimate the binary se-
quence inside the black box, by feeding it with many input vectors and observing their
output. Or, to put it in a different way, to minimize S (x) using bS (x) as a proxy. Since there
are 2n possible input vectors, it is infeasible to try all possible vectors x even for moderate
n.
The following Python program implements the noisy function bS (x) for n = 100. Each
input bit is flipped with a rather high probability θ = 0.4, so that the output is a poor indic-
ator of how many bits actually match the true vector. This true vector has 1s at positions
1, . . . , 50 and 0s at 51, . . . , 100.
Snoisy.py
import numpy as np
def Snoisy(X):
#takes a matrix
n = X.shape[1]
N = X.shape[0]
# true binary vector
xorg = np.hstack((np.ones((1,n//2)), np.zeros((1,n//2))))
theta = 0.4 # probability to flip the input
# storing the number of bits unequal to the true vector
s = np.zeros(N)
for i in range(0,N):
# determine which bits

 to flip
flip = (np.random.uniform(size=(n)) < theta).astype(int)
ind = flip>0
X[i][ind] = 1-X[i][ind]
s[i] = (X[i] != xorg).sum()
return s
The CE code below to optimize S (x) is quite similar to the continuous optimization
code in Example 3.16. However, instead of sampling iid random variables X1, . . . , XN from
☞101
a normal distribution, we now sample iid binary vectors X1, . . . , XN from a Ber(p) distribu-
tion. More precisely, given a row vector of probabilities p = [p1, . . . , pn], we independently
simulate the components X1, . . . , Xn of each binary vector X according to Xi ∼Ber(pi),
i = 1, . . . , n. After each iteration, the vector p is updated as the (vector) mean of the elite
samples. The sample size is N = 1000 and the number of elite samples is 100. The compon-
ents of the initial sampling vector p are all equal to 1/2; that is, the X are initially uniformly
sampled from the set of all binary vectors of length n = 100. At each subsequent iteration
the parameter vector is updated via the mean of the elite samples and evolves towards a
degenerate vector p∗with only 1s and 0s. Sampling from such a Ber(p∗) distribution gives
an outcome x∗= p∗, which can be taken as an estimate for the minimizer of S ; that is, the
true binary vector hidden in the black box. The algorithm stops when p has degenerated
sufficiently.
Figure 3.16 shows the evolution of the vector of probabilities p. This figure may be
seen as the discrete analogue of Figure 3.12. We see that, despite the high noise, the CE
method is able to find the true state of the black box, and hence the minimum value of S .
Monte Carlo Methods
113
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
0
10
20
30
40
50
60
70
80
90
100
0
0.5
1
Figure 3.16: Evolution of the vector of probabilities p = [p1, . . . , pn] towards the degener-
ate solution.
CEnoisy.py
from Snoisy import Snoisy
import numpy as np
n = 100
rho = 0.1
N = 1000; Nel = int(N*rho); eps = 0.01
p = 0.5*np.ones(n)
i = 0
pstart = p
ps = np.zeros((1000,n)

)
ps[0] = pstart
pdist = np.zeros((1,1000))
while np.max(np.minimum(p,1-p)) > eps:
i += 1
X = (np.random.uniform(size=(N,n)) < p).astype(int)
X_tmp = np.array(X, copy=True)
SX = Snoisy(X_tmp)
114
Exercises
ids = np.argsort(SX,axis=0)
Elite = X[ids[0:Nel],:]
p = np.mean(Elite,axis=0)
ps[i] = p
print(p)
Further Reading
The article [68] explores why the Monte Carlo method is so important in today’s quantitat-
ive investigations. The Handbook of Monte Carlo Methods [71] provides a comprehensive
overview of Monte Carlo simulation that explores the latest topics, techniques, and real-
world applications. Popular books on simulation and the Monte Carlo method include [42],
[75], and [104]. A classic reference on random variable generation is [32]. Easy introduc-
tions to stochastic simulation are given in [49], [98], and [100]. More advanced theory
can be found in [5]. Markov chain Monte Carlo is detailed in [50] and [99]. The research
monograph on the cross-entropy method is [103] and a tutorial is provided in [30]. A range
of optimization applications of the CE method is given in [16]. Theoretical results on ad-
aptive tuning schemes for simulated annealing may be found, for example, in [111]. There
are several established ways for gradient estimation. These include the finite difference
method, infinitesimal perturbation analysis, the score function method, and the method of
weak derivatives; see, for example, [51, Chapter 7].
Exercises
1. We can modify the Box–Muller method in Example 3.1 to draw X and Y uniformly
☞69
on the unit disc, {(x, y) ∈R2 : x2 +y2 ⩽1}, in the following way: Independently draw
a radius R and an angle Θ ∼U(0, 2π), and return X = R cos(Θ), Y = R sin(Θ). The
question is how to draw R.
(a) Show that the cdf of R is given by FR(r) = r2 for 0 ⩽r ⩽1 (with FR(r) = 0 and
FR(r) = 1 for r < 0 and r > 1, respectively).
(b) Explain how to simulate R using the inverse-transform method.
(c) Simulate 100 independent draws of [X, Y]⊤according to the method desc

ribed
above.
2. A simple acceptance–rejection method to simulate a vector X in the unit d-ball {x ∈
Rd : ∥x∥⩽1} is to first generate X uniformly in the hyper cube [−1, 1]d and then to
accept the point only if ∥X∥⩽1. Determine an analytic expression for the probability
of acceptance as a function of d and plot this for d = 1, . . . , 50.
3. Let the random variable X have pdf
f(x) =

1
2 x ,
0 ⩽x < 1,
1
2 ,
1 ⩽x ⩽5
2 .
Monte Carlo Methods
115
Simulate a random variable from f(x), using
(a) the inverse-transform method;
(b) the acceptance–rejection method, using the proposal density
g(x) = 8
25 x ,
0 ⩽x ⩽5
2 .
4. Construct simulation algorithms for the following distributions:
(a) The Weib(α, λ) distribution, with cdf F(x) = 1−e−(λx)α, x ⩾0, where λ > 0 and
α > 0.
(b) The Pareto(α, λ) distribution, with pdf f(x) = αλ(1 + λx)−(α+1), x ⩾0, where
λ > 0 and α > 0.
5. We wish to sample from the pdf
f(x) = x e−x,
x ⩾0,
using acceptance–rejection with the proposal pdf g(x) = e−x/2/2, x ⩾0.
(a) Find the smallest C for which Cg(x) ⩾f(x) for all x.
(b) What is the efficiency of this acceptance–rejection method?
6. Let [X, Y]⊤be uniformly distributed on the triangle with corners (0, 0), (1, 2), and
(−1, 1). Give the distribution of [U, V]⊤defined by the linear transformation
"U
V
#
=
"1
2
3
4
# "X
Y
#
.
7. Explain how to generate a random variable from the extreme value distribution,
which has cdf
F(x) = 1 −e−exp( x−µ
σ ) ,
−∞< x < ∞,
(σ > 0),
via the inverse-transform method.
8. Write a program that generates and displays 100 random vectors that are uniformly
distributed within the ellipse
5 x2 + 21 x y + 25 y2 = 9.
[Hint: Consider generating uniformly distributed samples within the circle of radius
3 and use the fact that linear transformations preserve uniformity to transform the
circle to the given ellipse.]
9. Suppose that Xi ∼Exp(λi), independently, for all i = 1, . . . , n. Let Π = [Π1, . . . , Πn]⊤
be the random permutation induced by the ordering XΠ1 < XΠ2 < · · ·

 < XΠn, and
define Z1 := XΠ1 and Z j := XΠj −XΠj−1 for j = 2, . . . , n.
116
Exercises
(a) Determine an n × n matrix A such that Z = AX and show that det(A) = 1.
(b) Denote the joint pdf of X and Π as
fX,Π(x, π) =
n
Y
i=1
λπi exp  −λπixπi
 × 1{xπ1 < · · · < xπn},
x ⩾0, π ∈Pn,
where Pn is the set of all n! permutations of {1, . . . , n}. Use the multivariate
transformation formula (C.22) to show that
☞432
fZ,Π(z, π) = exp
−
n
X
i=1
zi
X
k⩾i
λπk

n
Y
i=1
λi,
z ⩾0, π ∈Pn.
Hence, conclude that the probability mass function of the random permutation
Π is:
P[Π = π] =
n
Y
i=1
λπi
P
k⩾i λπk
,
π ∈Pn.
(c) Write pseudo-code to simulate a uniform random permutation Π ∈Pn; that is,
such that P[Π = π] =
1
n!, and explain how this uniform random permutation
can be used to reshuffle a training set τn.
10. Consider the Markov chain with transition graph given in Figure 3.17, starting in
state 1.
Start
0.5
1
0.8
0.9
0.2
0.1
0.5
0.2
0.3
0.5
0.3
0.7
4
3
1
2
6
5
Figure 3.17: The transition graph for the Markov chain {Xt, t = 0, 1, 2, . . .}.
(a) Construct a computer program to simulate the Markov chain, and show a real-
ization for N = 100 steps.
(b) Compute the limiting probabilities that the Markov chain is in state 1,2,...,6,
by solving the global balance equations (C.42).
☞452
(c) Verify that the exact limiting probabilities correspond to the average fraction
of times that the Markov process visits states 1,2,...,6, for a large number of
steps N.
11. As a generalization of Example C.9, consider a random walk on an arbitrary undir-
☞453
ected connected graph with a finite vertex set V. For any vertex v ∈V, let d(v) be
Monte Carlo Methods
117
the number of neighbors of v — called the degree of v. The random walk can jump to
each one of the neighbors with probability 1/d(v) and can be described by a Markov
chain. Show that, if the chain is aperiodic, the limiting probability that the chain is
in state v is equal to d(v)/ P
v′∈V d(v′).
12. Let U, V ∼iid U(0, 1). The reaso

n why in Example 3.7 the sample mean and sample
☞
76
median behave very differently is that E[U/V] = ∞, while the median of U/V is
finite. Show this, and compute the median. [Hint: start by determining the cdf of
Z = U/V by writing it as an expectation of an indicator function.]
13. Consider the problem of generating samples from Y ∼Gamma(2, 10).
(a) Direct simulation: Let U1, U2 ∼iid U(0, 1). Show that −ln(U1)/10−ln(U2)/10 ∼
Gamma(2, 10). [Hint: derive the distribution of −ln(U1)/10 and use Ex-
ample C.1.]
☞427
(b) Simulation via MCMC: Implement an independence sampler to simulate from
the Gamma(2, 10) target pdf
f(x) = 100 x e−10x,
x ⩾0,
using proposal transition density q(y | x) = g(y), where g(y) is the pdf of an
Exp(5) random variable. Generate N = 500 samples, and compare the true cdf
with the empirical cdf of the data.
14. Let X = [X, Y]⊤be a random column vector with a bivariate normal distribution with
expectation vector µ = [1, 2]⊤and covariance matrix
Σ =
"1
a
a
4
#
.
(a) What are the conditional distributions of (Y | X = x) and (X | Y = y)? [Hint: use
Theorem C.8.]
☞436
(b) Implement a Gibbs sampler to draw 103 samples from the bivariate distribution
N(µ, Σ) for a = 0, 1, and 1.75, and plot the resulting samples.
15. Here the objective is to sample from the 2-dimensional pdf
f(x, y) = c e−(xy+x+y),
x ⩾0,
y ⩾0,
for some normalization constant c, using a Gibbs sampler. Let (X, Y) ∼f.
(a) Find the conditional pdf of X given Y = y, and the conditional pdf of Y given
X = x.
(b) Write working Python code that implements the Gibbs sampler and outputs
1000 points that are approximately distributed according to f.
(c) Describe how the normalization constant c could be estimated via Monte Carlo
simulation, using random variables X1, . . . , XN, Y1, . . . , YN
iid∼Exp(1).
118
Exercises
16. We wish to estimate µ =
R 2
−2 e−x2/2 dx =
R
H(x)f(x) dx via Monte Carlo simulation
using two different approaches: (1) defining H(x) = 4 e−x2/2 and f the pdf of the
U[−2, 2] dis

tribution and (2) defining H(x) =
√
2π 1{−2 ⩽x ⩽2} and f the pdf of
the N(0, 1) distribution.
(a) For both cases estimate µ via the estimator bµ
bµ = N−1
N
X
i=1
H(Xi).
(3.34)
Use a sample size of N = 1000.
(b) For both cases estimate the relative error κ of bµ using N = 100.
(c) Give a 95% confidence interval for µ for both cases using N = 100.
(d) From part (b), assess how large N should be such that the relative width of the
confidence interval is less than 0.01, and carry out the simulation with this N.
Compare the result with the true value of µ.
17. Consider estimation of the tail probability µ = P[X ⩾γ] of some random variable X,
where γ is large. The crude Monte Carlo estimator of µ is
bµ = 1
N
N
X
i=1
Zi,
(3.35)
where X1, . . . , XN are iid copies of X and Zi = 1{Xi ⩾γ}, i = 1, . . . , N.
(a) Show that bµ is unbiased; that is, E bµ = µ.
(b) Express the relative error of bµ, i.e.,
RE =
p
Varbµ
Ebµ
,
in terms of N and µ.
(c) Explain how to estimate the relative error of bµ from outcomes x1, . . . , xN of
X1, . . . , XN, and how to construct a 95% confidence interval for µ.
(d) An unbiased estimator Z of µ is said to be logarithmically efficient if
lim
γ→∞
ln EZ2
ln µ2 = 1.
(3.36)
Show that the CMC estimator (3.35) with N = 1 is not logarithmically efficient.
18. One of the test cases in [70] involves the minimization of the Hougen function. Im-
plement a cross-entropy and a simulated annealing algorithm to carry out this optim-
ization task.
Monte Carlo Methods
119
19. In the binary knapsack problem, the goal is to solve the optimization problem:
max
x∈{0,1}n p⊤x,
subject to the constraints
Ax ⩽c,
where p and w are n × 1 vectors of non-negative numbers, A = (ai j) is an m × n
matrix, and c is an m × 1 vector. The interpretation is that x j = 1 or 0 depending
on whether item j with value pj is packed into the knapsack or not , j = 1, . . . , n;
The variable aij represents the i-th attribute (e.g., volume, weight) of the j-th item.
Associated with each attribut

e is a maximal capacity, e.g., c1 could be the maximum
volume of the knapsack, c2 the maximum weight, etc.
Write a CE program to solve the Sento1.dat knapsack problem at http://peop
le.brunel.ac.uk/~mastjjb/jeb/orlib/files/mknap2.txt, as described in
[16].
20. Let (C1, R1), (C2, R2), . . . be a renewal reward process, with ER1 < ∞and
EC1 < ∞. Let At = PNt
i=1 Ri/t be the average reward at time t = 1, 2, . . ., where
Nt = max{n : Tn ⩽t} and we have defined Tn = Pn
i=1 Ci as the time of the n-th re-
newal.
(a) Show that Tn/n
a.s.
−→EC1 as n →∞.
(b) Show that Nt
a.s.
−→∞as t →∞.
(c) Show that Nt/t
a.s.
−→1/EC1 as t →∞. [Hint: Use the fact that TNt ⩽t ⩽TNt+1 for
all t = 1, 2, . . . .]
(d) Show that
At
a.s.
−→ER1
EC1
as t →∞.
21. Prove Theorem 3.3.
☞
92
22. Prove that if H(x) ⩾0 the importance sampling pdf g∗in (3.22) gives the zero-
☞
96
variance importance sampling estimator bµ = µ.
23. Let X and Y be random variables (not necessarily independent) and suppose we wish
to estimate the expected difference µ = E[X −Y] = EX −EY.
(a) Show that if X and Y are positively correlated, the variance of X −Y is smaller
than if X and Y are independent.
(b) Suppose now that X and Y have cdfs F and G, respectively, and are
simulated via the inverse-transform method: X = F−1(U), Y = G−1(V), with
U, V ∼U(0, 1), not necessarily independent. Intuitively, one might expect that
if U and V are positively correlated, the variance of X−Y would be smaller than
if U and V are independent. Show that this is not always the case by providing
a counter-example.
120
Exercises
(c) Continuing (b), assume now that F and G are continuous. Show that the vari-
ance of X −Y by taking common random numbers U = V is no larger than
when U and V are independent. [Hint: Use the following lemma of Hoeffding
[41]: If (X, Y) have joint cdf H with marginal cdfs of X and Y being F and G,
respectively, then
Cov(X, Y) =
Z ∞
−∞
Z ∞
−∞
(H(x, y) −F(x)G(y)) dx dy,
provided Cov(X, Y) exists.]
CHAPTER4
UNSUPERVISED LEARNING


When there is no distinction between response and explanatory variables, unsu-
pervised methods are required to learn the structure of the data. In this chapter we
look at various unsupervised learning techniques, such as density estimation, cluster-
ing, and principal component analysis. Important tools in unsupervised learning in-
clude the cross-entropy training loss, mixture models, the Expectation–Maximization
algorithm, and the Singular Value Decomposition.
4.1
Introduction
In contrast to supervised learning, where an “output” (response) variable y is explained by
an “input” (explanatory) vector x, in unsupervised learning there is no response variable
and the overall goal is to extract useful information and patterns from the data, e.g., in
the form τ = {x1, . . . , xn} or as a matrix X⊤= [x1, . . . , xn]. In essence, the objective of
unsupervised learning is to learn about the underlying probability distribution of the data.
We start in Section 4.2 by setting up a framework for unsupervised learning that is
similar to the framework used for supervised learning in Section 2.3. That is, we formulate
☞
23
unsupervised learning in terms of risk and loss minimization; but now involving the cross-
entropy risk, rather than the squared-error risk. In a natural way this leads to fundamental
learning concepts such as likelihood, Fisher information, and the Akaike information cri-
terion. Section 4.3 introduces the Expectation–Maximization (EM) algorithm as a useful
method for maximizing likelihood functions when their solution cannot be found easily in
closed form.
If the data forms an iid sample from some unknown distribution, the “empirical dis-
tribution” of the data provides valuable information about the unknown distribution. In
Section 4.4 we formalize the concept of the empirical distribution (a generalization of the
empirical cdf) and explain how we can produce an estimate of the underlying probability
☞
11
density function of the data using kernel density es

timators.
Most unsupervised learning techniques focus on identifying certain traits of the under-
lying distribution, such as its local maximizers. A related idea is to partition the data into
clusters of points that are in some sense “similar” to each other. In Section 4.5 we formu-
late the clustering problem in terms of a mixture model. In particular, the data are assumed
☞135
121
122
Risk and Loss in Unsupervised Learning
to come from a mixture of (usually Gaussian) distributions, and the objective is to recover
the parameters of the mixture distributions from the data. The principal tool for parameter
estimation in mixture models is the EM algorithm.
Section 4.6 discusses a more heuristic approach to clustering, where the data are
grouped according to certain “cluster centers”, whose positions are found by solving an
optimization problem. Section 4.7 describes how clusters can be constructed in a hierarch-
ical manner.
Finally, in Section 4.8 we discuss the unsupervised learning technique called Principal
Component Analysis (PCA), which is an important tool for reducing the dimensionality of
the data.
We will revisit various unsupervised learning techniques in subsequent chapters on su-
pervised learning. For example, cross-entropy training loss minimization will be important
in logistic regression (Section 5.7) and classification (Chapter 7), and PCA can be used
☞204
☞251
for variable selection and dimensionality reduction, to make models easier to train and
increase their predictive power; see e.g., Sections 6.8 and 7.4.
4.2
Risk and Loss in Unsupervised Learning
In unsupervised learning, the training data T := {X1, . . . , Xn} only consists of (what are
usually assumed to be) independent copies of a feature vector X; there is no response
data. Suppose our objective is to learn the unknown pdf f of X based on an outcome
τ = {x1, . . . , xn} of the training data T . Conveniently, we can follow the same line of reas-
oning as for supervised learning, discussed 

in Sections 2.3–2.5. Table 4.1 gives a summary
☞23
of definitions for the case of unsupervised learning. Compare this with Table 2.1 for the
supervised case.
☞25
Similar to supervised learning, we wish to find a function g, which is now a probability
density (continuous or discrete), that best approximates the pdf f in terms of minimizing a
risk
ℓ(g) := E Loss(f(X), g(X)),
(4.1)
where Loss is a loss function. In (2.27), we already encountered the Kullback–Leibler risk
ℓ(g) := E ln f(X)
g(X) = E ln f(X) −E ln g(X).
(4.2)
If G is a class of functions that contains f, then minimizing the Kullback–Leibler risk over
G will yield the (correct) minimizer f. Of course, the problem is that minimization of (4.2)
depends on f, which is generally not known. However, since the term E ln f(X) does not
depend on g, it plays no role in the minimization of the Kullback–Leibler risk. By removing
this term, we obtain the cross-entropy risk
cross-entropy
risk
(for discrete X replace the integral with a sum):
ℓ(g) := −E ln g(X) = −
Z
f(x) ln g(x) dx.
(4.3)
Thus, minimizing the cross-entropy risk (4.3) over all g ∈G, again gives the minimizer
f, provided that f ∈G. Unfortunately, solving (4.3) is also infeasible in general, as it still
Unsupervised Learning
123
Table 4.1: Summary of definitions for unsupervised learning.
x
Fixed feature vector.
X
Random feature vector.
f(x)
Pdf of X evaluated at the point x.
τ or τn
Fixed training data {xi, i = 1, . . . , n}.
T or Tn
Random training data {Xi, i = 1, . . . , n}.
g
Approximation of the pdf f.
Loss( f(x), g(x))
Loss incurred when approximating f(x) with g(x).
ℓ(g)
Risk for approximation function g; that is, E Loss( f(X), g(X)).
gG
Optimal approximation function in function class G; that is,
argming∈G ℓ(g).
ℓτ(g)
Training loss for approximation function (guess) g; that is,
the sample average estimate of ℓ(g) based on a fixed training
sample τ.
ℓT(g)
The same as ℓτ(g), but now for a random training sample T .
gG
τ or gτ
The learner: argming∈G

 ℓτ(g). That is, the optimal approxima-
tion function based on a fixed training set τ and function class
G. We suppress the superscript G if the function class is impli-
cit.
gG
T or gT
The learner for a random training set T .
depends on f. Instead, we seek to minimize the cross-entropy training loss
cross-entropy
training loss
:
ℓτ(g) := 1
n
n
X
i=1
Loss( f(xi), g(xi)) = −1
n
n
X
i=1
ln g(xi)
(4.4)
over the class of functions G, where τ = {x1, . . . , xn} is an iid sample from f. This optimiz-
ation is doable without knowing f and is equivalent to solving the maximization problem
max
g∈G
n
X
i=1
ln g(xi).
(4.5)
A key step in setting up the learning procedure is to select a suitable function class G over
which to optimize. The standard approach is to parameterize g with a parameter θ and let
G be the class of functions {g(· | θ), θ ∈Θ} for some p-dimensional parameter set Θ. For the
remainder of Section 4.2, we will be using this function class, as well as the cross-entropy
risk.
The function θ 7→g(x | θ) is called the likelihood function
likelihood
function
. It gives the likelihood of
the observed feature vector x under g(· | θ), as a function of the parameter θ. The natural
logarithm of the likelihood function is called the log-likelihood function and its gradient
with respect to θ is called the score function
score function
, denoted S(x | θ); that is,
S(x | θ) := ∂ln g(x | θ)
∂θ
=
∂g(x | θ)
∂θ
g(x | θ).
(4.6)
124
Risk and Loss in Unsupervised Learning
The random score S(X | θ), with X ∼g(· | θ), is of particular interest. In many cases, its
expectation is equal to the zero vector; namely,
EθS(X | θ) =
Z
∂g(x | θ)
∂θ
g(x | θ) g(x | θ) dx
=
Z ∂g(x | θ)
∂θ
dx =
∂
R
g(x | θ) dx
∂θ
= ∂1
∂θ = 0,
(4.7)
provided that the interchange of differentiation and integration is justified. This is true for
a large number of distributions, including the normal, exponential, and binomial distri-
butions. Notable exceptions are distributions whose support depends on the distribut

ional
parameter; for example the U(0, θ) distribution.
It is important to see whether expectations are taken with respect to X ∼g(· | θ) or
X ∼f. We use the expectation symbols Eθ and E to distinguish the two cases.
From now on we simply assume that the interchange of differentiation and integration
is permitted; see, e.g., [76] for sufficient conditions. The covariance matrix of the random
score S(X | θ) is called the Fisher information matrix
Fisher
information
matrix
, which we denote by F or F(θ) to
show its dependence on θ. Since the expected score is 0, we have
F(θ) = Eθ[S(X | θ) S(X | θ)⊤].
(4.8)
A related matrix is the expected Hessian matrix of −ln g(X | θ):
☞398
H(θ) := E
"
−∂S(X | θ)
∂θ
#
= −E

∂2 ln g(X | θ)
∂2θ1
∂2 ln g(X | θ)
∂θ1∂θ2
· · ·
∂2 ln g(X | θ)
∂θ1∂θp
∂2 ln g(X | θ)
∂θ2∂θ1
∂2 ln g(X | θ)
∂2θ2
· · ·
∂2 ln g(X | θ)
∂θ2∂θp
...
...
...
...
∂2 ln g(X | θ)
∂θp∂θ1
∂2 ln g(X | θ)
∂θp∂θ2
· · ·
∂2 ln g(X | θ)
∂2θp

.
(4.9)
Note that the expectation here is with respect to X ∼f. It turns out that if f = g(· | θ), the
two matrices are the same; that is,
F(θ) = H(θ),
(4.10)
provided that we may swap the order of differentiation and integration (expectation). This
result is called the information matrix equality
information
matrix equality
. We leave the proof as Exercise 1.
The matrices F(θ) and H(θ) play important roles in approximating the cross-entropy
risk for large n. To set the scene, let gG = g(· | θ∗) be the minimizer of the cross-entropy
risk
r(θ) := −E ln g(X | θ).
We assume that r, as a function of θ, is well-behaved; in particular, that in the neighborhood
of θ∗it is strictly convex and twice continuously differentiable (this holds true, for example,
if g is a Gaussian density). It follows that θ∗is a root of E S(X | θ), because
0 = ∂r(θ∗)
∂θ
= −∂E ln g(X | θ∗)
∂θ
= −E∂ln g(X | θ∗)
∂θ
= −E S(X | θ∗),
Unsupervised Learning
125
again provided that the order of differentiation and integration (expectat

ion) can be
swapped. In the same way, H(θ) is then the Hessian matrix of r. Let g(· |bθn) be the minim-
izer of the training loss
rTn(θ) := −1
n
n
X
i=1
ln g(Xi | θ),
where Tn = {X1, . . . , Xn} is a random training set. Let r∗be the smallest possible cross-
entropy risk, taken over all functions; clearly, r∗= −E ln f(X), where X ∼f. Similar to
the supervised learning case, we can decompose the generalization risk, ℓ(g(· |bθn)) = r(bθn),
into
r(bθn) = r∗+ r(θ∗) −r∗
|     {z     }
approx. error
+ r(bθn) −r(θ∗)
|         {z         }
statistical error
= r(θ∗) −E ln g(X | θ∗)
g(X |bθn)
.
The following theorem specifies the asymptotic behavior of the components of the gener-
alization risk. In the proof we assume thatbθn
P
−→θ∗as n →∞.
☞439
Theorem 4.1: Approximating the Cross-Entropy Risk
It holds asymptotically (n →∞) that
Er(bθn) −r(θ∗) ≃tr

F(θ∗) H−1(θ∗)

/(2n),
(4.11)
where
r(θ∗) ≃ErTn(bθn) + tr

F(θ∗) H−1(θ∗)

/(2n).
(4.12)
Proof: A Taylor expansion of r(bθn) around θ∗gives the statistical error
☞400
r(bθn) −r(θ∗) = (bθn −θ∗)⊤∂r(θ∗)
∂θ
| {z }
= 0
+1
2(bθn −θ∗)⊤H(θn)(bθn −θ∗),
(4.13)
where θn lies on the line segment between θ∗andbθn. For large n we may replace H(θn) with
H(θ∗) as, by assumption, bθn converges to θ∗. The matrix H(θ∗) is positive definite because
r(θ) is strictly convex at θ∗by assumption, and therefore invertible. It is important to realize
that bθn is in fact an M-estimator of θ∗. In particular, in the notation of Theorem C.19, we
☞449
have ψ = S, A = H(θ∗), and B = F(θ∗). Consequently, by that same theorem,
√n (bθn −θ∗)
d
−→N

0, H−1(θ∗) F(θ∗) H−⊤(θ∗)

.
(4.14)
Combining (4.13) with (4.14), it follows from Theorem C.2 that asymptotically the
☞430
expected estimation error is given by (4.11).
Next, we consider a Taylor expansion of rTn(θ∗) aroundbθn:
rTn(θ∗) = rTn(bθn) + (θ∗−bθn)⊤∂rTn(bθn)
∂θ
|   {z   }
= 0
+1
2(θ∗−bθn)⊤HTn(θn)(θ∗−bθn),
(4.15)
126
Risk and Loss in Unsupervised Learning
where HTn(θn) := −1
n
Pn
i=1
∂S(Xi |θn)
∂θ
is the Hessia

n of rTn(θ) at some θn between bθn and θ∗.
Taking expectations on both sides of (4.15), we obtain
r(θ∗) = ErTn(bθn) + 1
2E (θ∗−bθn)⊤HTn(θn)(θ∗−bθn).
Replacing HTn(θn) with H(θ∗) for large n and using (4.14), we have
n E (θ∗−bθn)⊤HTn(θn)(θ∗−bθn) −→tr

F(θ∗) H−1(θ∗)

,
n →∞.
Therefore, asymptotically as n →∞, we have (4.12).
□
Theorem 4.1 has a number of interesting consequences:
1. Similar to Section 2.5.1, the training loss ℓTn(gTn) = rTn(bθn) tends to underestimate the
☞35
risk ℓ(gG) = r(θ∗), because the training set Tn is used to both train g ∈G (that is, estimate
θ∗) and to estimate the risk. The relation (4.12) tells us that on average the training loss
underestimates the true risk by tr(F(θ∗) H−1(θ∗))/(2n).
2. Adding equations (4.11) and (4.12), yields the following asymptotic approximation to
the expected generalization risk:
E r(bθn) ≃E rTn(bθn) + 1
ntr

F(θ∗) H−1(θ∗)

(4.16)
The first term on the right-hand side of (4.16) can be estimated (without bias) via the
training loss rTn(bθn). As for the second term, we have already mentioned that when the
true model f ∈G, then F(θ∗) = H(θ∗). Therefore, when G is deemed to be a sufficiently
rich class of models parameterized by a p-dimensional vector θ, we may approximate the
second term as tr(F(θ∗)H−1(θ∗))/n ≈tr(Ip)/n = p/n. This suggests the following heuristic
approximation to the (expected) generalization risk:
E r(bθn) ≈rTn(bθn) + p
n.
(4.17)
3. Multiplying both sides of (4.16) by 2n and substituting tr

F(θ∗)H−1(θ∗)

≈p, we obtain
the approximation:
2n r(bθn) ≈−2
n
X
i=1
ln g(Xi |bθn) + 2p.
(4.18)
The right-hand side of (4.18) is called the Akaike information criterion
Akaike
information
criterion
(AIC). Just like
(4.17), the AIC approximation can be used to compare the difference in generalization risk
of two or more learners. We prefer the learner with the smallest (estimated) generalization
risk.
Suppose that, for a training set T , the training loss rT(θ) has a unique minimum point
bθ which lies in th

e interior of Θ. If rT(θ) is a differentiable function with respect to θ, then
we can find the optimal parameterbθ by solving
∂rT(θ)
∂θ
= 1
n
n
X
i=1
S(Xi | θ)
|           {z           }
ST (θ)
= 0.
Unsupervised Learning
127
In other words, the maximum likelihood estimatebθ for θ is obtained by solving the root of
the average score function, that is, by solving
ST(θ) = 0.
(4.19)
It is often not possible to find bθ in an explicit form. In that case one needs to solve the
equation (4.19) numerically. There exist many standard techniques for root-finding, e.g.,
via Newton’s method (see Section B.3.1), whereby, starting from an initial guess θ0, sub-
Newton’s
method
☞409
sequent iterates are obtained via the iterative scheme
θt+1 = θt + H−1
T (θt) ST(θt),
where
HT(θ) := −∂ST(θ)
∂θ
= 1
n
n
X
i=1
−∂S(Xi | θ)
∂θ
is the average Hessian matrix of {−ln g(Xi | θ)}n
i=1. Under f = g(· | θ), the expectation of
HT(θ) is equal to the information matrix F(θ), which does not depend on the data. This
suggests an alternative iterative scheme, called Fisher’s scoring method
Fisher’s
scoring method
:
θt+1 = θt + F−1(θt) ST(θt),
(4.20)
which is not only easier to implement (if the information matrix can be readily evaluated),
but also is more numerically stable.
Example 4.1 (Maximum Likelihood for the Gamma Distribution) We wish to ap-
proximate the density of the Gamma(α∗, λ∗) distribution for some true but unknown para-
meters α∗and λ∗, on the basis of a training set τ = {x1, . . . , xn} of iid samples from this
distribution. Choosing our approximating function g(· | α, λ) in the same class of gamma
densities,
g(x | α, λ) = λαxα−1e−λx
Γ(α)
,
x ⩾0,
(4.21)
with α > 0 and λ > 0, we seek to solve (4.19). Taking the logarithm in (4.21), the log-
likelihood function is given by
l(x | α, λ) := α ln λ −ln Γ(α) + (α −1) ln x −λx.
It follows that
S(α, λ) =

∂
∂αl(x | α, λ)
∂
∂λl(x | α, λ)
=
"ln λ −ψ(α) + ln x
α
λ −x
#
,
where ψ is the derivative of ln Γ: the so-called digamma functio

n
digamma
function
. Hence,
H(α, λ) = −E

∂2
∂α2l(X | α, λ)
∂2
∂α∂λl(X | α, λ)
∂2
∂α∂λl(X | α, λ)
∂2
∂λ2l(X | α, λ)
= −E
"−ψ′(α)
1
λ
1
λ
−α
λ2
#
=
"ψ′(α)
−1
λ
−1
λ
α
λ2
#
.
Fisher’s scoring method (4.20) can now be used to solve (4.19), with
Sτ(α, λ) =
"ln λ −ψ(α) + n−1 Pn
i=1 ln xi
α
λ −n−1 Pn
i=1 xi
#
and F(α, λ) = H(α, λ).
128
Expectation–Maximization (EM) Algorithm
4.3
Expectation–Maximization (EM) Algorithm
The Expectation–Maximization algorithm (EM) is a general algorithm for maximization of
complicated (log-)likelihood functions, through the introduction of auxiliary variables.
To simplify the notation in this section, we use a Bayesian notation system, where
the same symbol is used for different (conditional) probability densities.
As in the previous section, given independent observations τ = {x1, . . . , xn} from some
unknown pdf f, the objective is to find the best approximation to f in a function class
G = {g(· | θ), θ ∈Θ} by solving the maximum likelihood problem:
θ∗= argmax
θ∈Θ
g(τ | θ),
(4.22)
where g(τ | θ) := g(x1 | θ) · · · g(xn | θ). The key element of the EM algorithm is the aug-
mentation of the data τ with a suitable vector of latent variables
latent
variables
, z, such that
g(τ | θ) =
Z
g(τ, z | θ) dz.
The function θ 7→g(τ, z | θ) is usually referred to as the complete-data likelihood
complete-data
likelihood
function.
The choice of the latent variables is guided by the desire to make the maximization of
g(τ, z | θ) much easier than that of g(τ | θ).
Suppose p denotes an arbitrary density of the latent variables z. Then, we can write:
ln g(τ | θ) =
Z
p(z) ln g(τ | θ) dz
=
Z
p(z) ln
 g(τ, z | θ)/p(z)
g(z | τ, θ)/p(z)
!
dz
=
Z
p(z) ln
 g(τ, z | θ)
p(z)
!
dz −
Z
p(z) ln
 g(z | τ, θ)
p(z)
!
dz
=
Z
p(z) ln
 g(τ, z | θ)
p(z)
!
dz + D(p, g(· | τ, θ)),
(4.23)
where D(p, g(· | τ, θ)) is the Kullback–Leibler divergence from the density p to g(· | τ, θ).
☞42
Since D ⩾0, it follows that
ln g(τ | θ) ⩾
Z
p(z) ln
 g(τ, z | θ)
p(z)
!
dz =: L(p, 

θ)
for all θ and any density p of the latent variables. In other words, L(p, θ) is a lower bound
on the log-likelihood that involves the complete-data likelihood. The EM algorithm then
aims to increase this lower bound as much as possible by starting with an initial guess θ(0)
and then, for t = 1, 2, . . ., solving the following two steps:
1. p(t) = argmaxp L(p, θ(t−1)),
Unsupervised Learning
129
2. θ(t) = argmaxθ∈Θ L(p(t), θ).
The first optimization problem can be solved explicitly. Namely, by (4.23), we have
that
p(t) = argmin
p
D(p, g(· | τ, θ(t−1))) = g(· | τ, θ(t−1)).
That is, the optimal density is the conditional density of the latent variables given the data
τ and the parameter θ(t−1). The second optimization problem can be simplified by writing
L(p(t), θ) = Q(t)(θ) −Ep(t) ln p(t)(Z), where
Q(t)(θ) := Ep(t) ln g(τ, Z | θ)
is the expected complete-data log-likelihood under Z ∼p(t). Consequently, the maximiza-
tion of L(p(t), θ) with respect to θ is equivalent to finding
θ(t) = argmax
θ∈Θ
Q(t)(θ).
This leads to the following generic EM algorithm.
Algorithm 4.3.1: Generic EM Algorithm
input: Data τ, initial guess θ(0).
output: Approximation of the maximum likelihood estimate.
1 t ←1
2 while a stopping criterion is not met do
3
Expectation Step: Find p(t)(z) := g(z | τ, θ(t−1)) and compute the expectation
Q(t)(θ) := Ep(t) ln g(τ, Z | θ).
(4.24)
4
Maximization Step: Let θ(t) ←argmaxθ∈Θ Q(t)(θ).
5
t ←t + 1
6 return θ(t)
A possible stopping criterion is to stop when

ln g(τ | θ(t)) −ln g(τ | θ(t−1))
ln g(τ | θ(t))
 ⩽ε
for some small tolerance ε > 0.
Remark 4.1 (Properties of the EM Algorithm) The identity (4.23) can be used to
show that the likelihood g(τ | θ(t)) does not decrease with every iteration of the algorithm.
This property is one of the strengths of the algorithm. For example, it can be used to debug
computer implementations of the EM algorithm: if the likelihood is observed to decrease
at any iteration, then one has detected a bug in the prog

ram.
The convergence of the sequence {θ(t)} to a global maximum (if it exists) is highly
dependent on the initial value θ(0) and, in many cases, an appropriate choice of θ(0) may not
be clear. Typically, practitioners run the algorithm from different random starting points
over Θ, to ascertain empirically that a suitable optimum is achieved.
130
Expectation–Maximization (EM) Algorithm
Example 4.2 (Censored Data) Suppose the lifetime (in years) of a certain type of
machine is modeled via a N(µ, σ2) distribution. To estimate µ and σ2, the lifetimes of
n (independent) machines are recorded up to c years. Denote these censored lifetimes
by x1, . . . , xn. The {xi} are thus realizations of iid random variables {Xi}, distributed as
min{Y, c}, where Y ∼N(µ, σ2).
By the law of total probability (see (C.9)), the marginal pdf of each X can be written
☞428
as:
g(x | µ, σ2) = Φ((c −µ)/σ)
|          {z          }
P[Y<c]
φσ2(x −µ)
Φ((c −µ)/σ)1{x < c} + Φ((c −µ)/σ)
|          {z          }
P[Y⩾c]
1{x = c},
where φσ2(·) is the pdf of the N(0, σ2) distribution, Φ is the cdf of the standard normal
distribution, and Φ := 1 −Φ. It follows that the likelihood of the data τ = {x1, . . . , xn} as a
function of the parameter θ := [µ, σ2]⊤is:
g(τ | θ) =
Y
i:xi<c
exp

−(xi−µ)2
2σ2

√
2πσ2
×
Y
i:xi=c
Φ((c −µ)/σ).
Let nc be the total number of xi such that xi = c. Using nc latent variables z = [z1, . . . , znc]⊤,
we can write the joint pdf:
g(τ, z | θ) =
1
(2πσ2)n/2 exp
 
−
P
i:xi<c(xi −µ)2
2σ2
−
Pnc
i=1(zi −µ)2
2σ2
!
1

min
i
zi ⩾c

,
so that
R
g(τ, z | θ) dz = g(τ | θ). We can thus apply the EM algorithm to maximize the like-
lihood, as follows.
For the E(xpectation)-step, we have for a fixed θ:
g(z | τ, θ) =
nc
Y
i=1
g(zi | τ, θ),
where g(z | τ, θ) = 1{z ⩾c} φσ2(z −µ)/Φ((c −µ)/σ) is simply the pdf of the N(µ, σ2)
distribution, truncated to [c, ∞).
For the M(aximization)-step, we compute the expectation of the complete log-
likelihood with respect to a fixed g(z | τ, θ) and use the fact 

that Z1, . . . , Znc are iid:
E ln g(τ, Z | θ) = −
P
i:xi<c(xi −µ)2
2σ2
−ncE(Z −µ)2
2σ2
−n
2 ln σ2 −n
2 ln(2π),
where Z has a N(µ, σ2) distribution, truncated to [c, ∞). To maximize the last expression
with respect to µ we set the derivative with respect to µ to zero, and obtain:
µ = ncEZ + P
i:xi<c xi
n
.
Similarly, setting the derivative with respect to σ2 to zero gives:
σ2 = ncE(Z −µ)2 + P
i:xi<c(xi −µ)2
n
.
In summary, the EM iterates for t = 1, 2, . . . are as follows.
Unsupervised Learning
131
E-step. Given the current estimate θt := [µt, σ2
t ]⊤, compute the expectations νt := EZ and
ζ2
t := E(Z −µt)2, where Z ∼N(µt, σ2
t ), conditional on Z ⩾c; that is,
νt := µt + σ2
t
φσ2
t (c −µt)
Φ((c −µt)/σt)
ζ2
t := σ2
t
1 + (c −µt)
φσ2
t (c −µt)
Φ((c −µt)/σt)
.
M-step. Update the estimate to θt+1 := [µt+1, σ2
t+1]⊤via the formulas:
µt+1 = ncνt + P
i:xi<c xi
n
σ2
t+1 = ncζ2
t + P
i:xi<c(xi −µt+1)2
n
.
4.4
Empirical Distribution and Density Estimation
In Section 1.5.2.3 we saw how the empirical cdf bFn, obtained from an iid training set
☞
11
τ = {x1, . . . , xn} from an unknown distribution on R, gives an estimate of the unknown cdf
F of this sampling distribution. The function bFn is a genuine cdf, as it is right-continuous,
increasing, and lies between 0 and 1. The corresponding discrete probability distribution
is called the empirical distribution
empirical
distribution
of the data. A random variable X distributed according
to this empirical distribution takes the values x1, . . . , xn with equal probability 1/n. The
concept of empirical distribution naturally generalizes to higher dimensions: a random
vector X that is distributed according to the empirical distribution of x1, . . . , xn has discrete
pdf P[X = xi] = 1/n, i = 1, . . . , n. Sampling from such a distribution — in other words
resampling the original data — was discussed in Section 3.2.4. The preeminent usage of
☞
76
such sampling is the bootstrap method, discussed in Section 3.3.2.
☞
88
In a w

ay, the empirical distribution is the natural answer to the unsupervised learning
question: what is the underlying probability distribution of the data? However, the empir-
ical distribution is, by definition, a discrete distribution, whereas the true sampling distri-
bution might be continuous. For continuous data it makes sense to also consider estimation
of the pdf of the data. A common approach is to estimate the density via a kernel density
estimate (KDE), the most prevalent learner to carry this out is given next.
Definition 4.1: Gaussian KDE
Let x1, . . . , xn ∈Rd be the outcomes of an iid sample from a continuous pdf f. A
Gaussian kernel density estimate
Gaussian
kernel density
estimate
of f is a mixture of normal pdfs, of the form
gτn(x | σ) = 1
n
n
X
i=1
1
(2π)d/2σd e−∥x−xi∥2
2σ2 ,
x ∈Rd,
(4.25)
where σ > 0 is called the bandwidth.
132
Empirical Distribution and Density Estimation
We see that gτn in (4.25) is the average of a collection of n normal pdfs, where each
normal distribution is centered at the data point xi and has covariance matrix σ2Id. A major
question is how to choose the bandwidth σ so as to best approximate the unknown pdf f.
Choosing very small σ will result in a “spiky” estimate, whereas a large σ will produce
an over-smoothed estimate that may not identify important peaks that are present in the
unknown pdf. Figure 4.1 illustrates this phenomenon. In this case the data are comprised
of 20 points uniformly drawn from the unit square. The true pdf is thus 1 on [0, 1]2 and 0
elsewhere.
Figure 4.1: Two two-dimensional Gaussian KDEs, with σ = 0.01 (left) and σ = 0.1 (right).
Let us write the Gaussian KDE in (4.25) as
gτn(x | σ) = 1
n
n
X
i=1
1
σd ϕ
 x −xi
σ

,
(4.26)
where
ϕ(z) =
1
(2π)d/2 e−∥z∥2
2 ,
z ∈Rd
(4.27)
is the pdf of the d-dimensional standard normal distribution. By choosing a different prob-
ability density ϕ in (4.26), satisfying ϕ(x) = ϕ(−x) for all x, we can obtain a wide variety
of kernel density estimates. A simple pdf ϕ is

, for example, the uniform pdf on [−1, 1]d:
ϕ(z) =

2−d,
if z ∈[−1, 1]d,
0,
otherwise.
Figure 4.2 shows the graph of the corresponding KDE, using the same data as in Figure 4.1
and with bandwidth σ = 0.1. We observe qualitatively similar behavior for the Gaussian
and uniform KDEs. As a rule, the choice of the function ϕ is less important than the choice
of the bandwidth in determining the quality of the estimate.
The important issue of bandwidth selection has been extensively studied for one-
dimensional data. To explain the ideas, we use our usual setup and let τ = {x1, . . . , xn}
be the observed (one-dimensional) data from the unknown pdf f. First, we define the loss
function as
Loss(f(x), g(x)) = (f(x) −g(x))2
f(x)
.
(4.28)
Unsupervised Learning
133
Figure 4.2: A two-dimensional uniform KDE, with bandwidth σ = 0.1.
The risk to minimize is thus ℓ(g) := EfLoss( f(X), g(X)) =
R
( f(x) −g(x))2 dx. We bypass
the selection of a class of approximation functions by choosing the learner to be specified
by (4.25) for a fixed σ. The objective is now to find a σ that minimizes the generalization
risk ℓ(gτ(· | σ)) or the expected generalization risk Eℓ(gT(· | σ)). The generalization risk is
in this case
Z
(f(x) −gτ(x | σ))2 dx =
Z
f 2(x) dx −2
Z
f(x)gτ(x | σ) dx +
Z
g2
τ(x | σ) dx.
Minimizing this expression with respect to σ is equivalent to minimizing the last two terms,
which can be written as
−2 Efgτ(X | σ) +
Z 
1
n
n
X
i=1
1
σϕ
 x −xi
σ

2
dx.
This expression in turn can be estimated by using a test sample {x′
1 . . . , x′
n′} from f, yielding
the following minimization problem:
min
σ −2
n′
n′
X
i=1
gτ(x′
i | σ) + 1
n2
n
X
i=1
n
X
j=1
Z
1
σ2ϕ
 x −xi
σ

ϕ
 x −x j
σ

dx,
where
R
1
σ2ϕ
 x−xi
σ

ϕ
 x−x j
σ

dx =
1
√
2σϕ
 xi−xj
√
2σ

in the case of the Gaussian kernel (4.27) with
d = 1. To estimate σ in this way clearly requires a test sample, or at least an application of
☞
38
cross-validation. Another approach is to minimize the expected ge

neralization risk, (that
is, averaged over all training sets):
E
Z
(f(x) −gT(x | σ))2 dx.
This is called the mean integrated squared error
mean integrated
squared error
(MISE). It can be decomposed into an
integrated squared bias and integrated variance component:
Z
( f(x) −EgT(x | σ))2 dx +
Z
Var(gT(x | σ)) dx.
134
Empirical Distribution and Density Estimation
A typical analysis now proceeds by investigating how the MISE behaves for large n, under
various assumptions on f. For example, it is shown in [114] that, for σ →0 and nσ →∞,
the asymptotic approximation to the MISE of the Gaussian kernel density estimator (4.25)
(for d = 1) is given by
1
4 σ4 ∥f ′′∥2 +
1
2n
√
πσ2,
(4.29)
where ∥f ′′∥2 :=
R
(f ′′(x))2 dx. The asymptotically optimal value of σ is the minimizer
σ∗:=
 
1
2n √π ∥f ′′∥2
!1/5
.
(4.30)
To compute the optimal σ∗in (4.30), one needs to estimate the functional ∥f ′′∥2. The
Gaussian rule of thumb
Gaussian rule
of thumb
is to assume that f is the density of the N(x, s2) distribution, where
x and s2 are the sample mean and variance of the data, respectively [113]. In this case
∥f ′′∥2 = s−5π−1/23/8 and the Gaussian rule of thumb becomes:
σrot =
 4 s5
3 n
!1/5
≈1.06 s n−1/5.
We recommend, however, the fast and reliable theta KDE
theta KDE
of [14], which chooses the
bandwidth in an optimal way via a fixed-point procedure. Figures 4.1 and 4.2 illustrate a
common problem with traditional KDEs: for distributions on a bounded domain, such as
the uniform distribution on [0, 1]2, the KDE assigns positive probability mass outside this
domain. An additional advantage of the theta KDE is that it largely avoids this boundary
effect. We illustrate the theta KDE with the following example.
Example 4.3 (Comparison of Gaussian and theta KDEs) The following Python pro-
gram draws an iid sample from the Exp(1) distribution and constructs a Gaussian kernel
density estimate. We see in Figure 4.3 that with an appropriate choice of the bandwidth
a good fit to the true pdf can 

be achieved, except at the boundary x = 0. The theta KDE
does not exhibit this boundary effect. Moreover, it chooses the bandwidth automatically,
to achieve a superior fit. The theta KDE source code is available as kde.py on the book’s
GitHub site.
0
1
2
3
4
5
6
0.0
0.2
0.4
0.6
0.8
1.0
Gaussian KDE
Theta KDE
True density
Figure 4.3: Kernel density estimates for Exp(1)-distributed data.
Unsupervised Learning
135
gausthetakde.py
import matplotlib.pyplot as plt
import numpy as np
from kde import *
sig = 0.1; sig2 = sig**2; c = 1/np.sqrt(2*np.pi)/sig #Constants
phi = lambda x,x0: np.exp(-(x-x0)**2/(2*sig2)) #Unscaled Kernel
f = lambda x: np.exp(-x)*(x >= 0) # True PDF
n = 10**4 # Sample Size
x = -np.log(np.random.uniform(size=n))# Generate Data via IT method
xx = np.arange(-0.5,6,0.01, dtype = "d")# Plot Range
phis = np.zeros(len(xx))
for i in range(0,n):
phis = phis + phi(xx,x[i])
phis = c*phis/n
plt.plot(xx,phis,'r')# Plot Gaussian KDE
[bandwidth ,density ,xmesh,cdf] = kde(x,2**12,0,max(x))
idx = (xmesh <= 6)
plt.plot(xmesh[idx],density[idx])# Plot Theta KDE
plt.plot(xx,f(xx))# Plot True PDF
4.5
Clustering via Mixture Models
Clustering is concerned with the grouping of unlabeled feature vectors into clusters, such
that samples within a cluster are more similar to each other than samples belonging to
different clusters. Usually, it is assumed that the number of clusters is known in advance,
but otherwise no prior information is given about the data. Applications of clustering can
be found in the areas of communication, data compression and storage, database searching,
pattern matching, and object recognition.
A common approach to clustering analysis is to assume that the data comes from a mix-
ture of (usually Gaussian) distributions, and thus the objective is to estimate the parameters
of the mixture model by maximizing the likelihood function for the data. Direct optimiza-
tion of the likelihood function in this case is not a simple task, due to necessary constraints


on the parameters (more about this later) and the complicated nature of the likelihood func-
tion, which in general has a great number of local maxima and saddle-points. A popular
method to estimate the parameters of the mixture model is the EM algorithm, which was
discussed in a more general setting in Section 4.3. In this section we explain the basics of
☞128
mixture modeling and explain the workings of the EM method in this context. In addition,
we show how direct optimization methods can be used to maximize the likelihood.
4.5.1
Mixture Models
Let T := {X1, . . . , Xn} be iid random vectors taking values in some set X ⊆Rd, each Xi
being distributed according to the mixture density
mixture density
g(x | θ) = w1ϕ1(x) + · · · + wKϕK(x),
x ∈X,
(4.31)
136
Clustering via Mixture Models
where ϕ1, . . . , ϕK are probability densities (discrete or continuous) on X, and the positive
weights w1, . . . , wK sum up to 1. This mixture pdf can be interpreted in the following way.
weights
Let Z be a discrete random variable taking values 1, 2, . . . , K with probabilities w1, . . . , wK,
and let X be a random vector whose conditional pdf, given Z = z, is ϕz. By the product rule
(C.17), the joint pdf of Z and X is given by
☞431
ϕZ,X(z, x) = ϕZ(z) ϕX | Z(x | z) = wz ϕz(x)
and the marginal pdf of X is found by summing the joint pdf over the values of z, which
gives (4.31). A random vector X ∼g can thus be simulated in two steps:
1. First, draw Z according to the probabilities P[Z = z] = wz, z = 1, . . . , K.
2. Then draw X according to the pdf ϕZ.
As T only contain the {Xi} variables, the {Zi} are viewed as latent variables. We can inter-
pret Zi as the hidden label of the cluster to which Xi belongs.
Typically, each ϕk in (4.31) is assumed to be known up to some parameter vector ηk. It
is customary1 in clustering analysis to work with Gaussian mixtures; that is, each density
ϕk is Gaussian with some unknown expectation vector µk and covariance matrix Σk. We
gather all unknown pa

rameters, including the weights {wk}, into a parameter vector θ. As
usual, τ = {x1, . . . , xn} denotes the outcome of T . As the components of T are iid, their
(joint) pdf is given by
g(τ | θ) :=
n
Y
i=1
g(xi | θ) =
n
Y
i=1
K
X
k=1
wk ϕk(xi | µk, Σk).
(4.32)
Following the same reasoning as for (4.5), we can estimate θ from an outcome τ by max-
imizing the log-likelihood function
l(θ | τ) :=
n
X
i=1
ln g(xi | θ) =
n
X
i=1
ln

K
X
k=1
wk ϕk(xi | µk, Σk)
.
(4.33)
However, finding the maximizer of l(θ | τ) is not easy in general, since the function is typ-
ically multiextremal.
Example 4.4 (Clustering via Mixture Models) The data depicted in Figure 4.4 con-
sists of 300 data points that were independently generated from three bivariate normal
distributions, whose parameters are given in that same figure. For each of these three dis-
tributions, exactly 100 points were generated. Ideally, we would like to cluster the data into
three clusters that correspond to the three cases.
To cluster the data into three groups, a possible model for the data is to assume that
the points are iid draws from an (unknown) mixture of three 2-dimensional Gaussian dis-
tributions. This is a sensible approach, although in reality the data were not simulated
in this way. It is instructive to understand the difference between the two models. In the
mixture model, each cluster label Z takes the value {1, 2, 3} with equal probability, and
hence, drawing the labels independently, the total number of points in each cluster would
1Other common mixture distributions include Student t and Beta distributions.
Unsupervised Learning
137
-8
-6
-4
-2
0
2
4
-6
-4
-2
0
2
4
cluster
mean vector
covariance matrix
1
"−4
0
#
" 2
1.4
1.4
1.5
#
2
"0.5
−1
#
"
2
−0.95
−0.95
1
#
3
"−1.5
−3
#
" 2
0.1
0.1
0.1
#
Figure 4.4: Cluster the 300 data points (left) into three clusters, without making any as-
sumptions about the probability distribution of the data. In fact, the data were generated
from three biva

riate normal distributions, whose parameters are listed on the right.
be Bin(300, 1/3) distributed. However, in the actual simulation, the number of points in
each cluster is exactly 100. Nevertheless, the mixture model would be an accurate (al-
though not exact) model for these data. Figure 4.5 displays the “target” Gaussian mixture
density for the data in Figure 4.4; that is, the mixture with equal weights and with the exact
parameters as specified in Figure 4.4.
Figure 4.5: The target mixture density for the data in Figure 4.4.
In the next section we will carry out the clustering by using the EM algorithm.
4.5.2
EM Algorithm for Mixture Models
As we saw in Section 4.3, instead of maximizing the log-likelihood function (4.33) directly
from the data τ = {x1, . . . , xn}, the EM algorithm first augments the data
data
augmentation
with the vector of
latent variables — in this case the hidden cluster labels z = {z1, . . . , zn}. The idea is that τ is
138
Clustering via Mixture Models
only the observed part of the complete random data (T , Z), which were generated via the
two-step procedure described above. That is, for each data point X, first draw the cluster
label Z ∈{1, . . . , K} according to probabilities {w1, . . . , wK} and then, given Z = z, draw X
from ϕz. The joint pdf of T and Z is
g(τ, z | θ) =
n
Y
i=1
wzi ϕzi(xi),
which is of a much simpler form than (4.32). It follows that the complete-data log-
likelihood
complete-data
log-likelihood
function
el(θ | τ, z) =
n
X
i=1
ln[wzi ϕzi(xi)]
(4.34)
is often easier to maximize than the original log-likelihood (4.33), for any given (τ, z). But,
of course the latent variables z are not observed and soel(θ | τ, z) cannot be evaluated. In the
E-step of the EM algorithm, the complete-data log-likelihood is replaced with the expect-
ation Epel(θ | τ, Z), where the subscript p in the expectation indicates that Z is distributed
according to the conditional pdf of Z given T = τ; that is, with pdf
p(z) = g(z | τ, θ) ∝g(τ, z 

| θ).
(4.35)
Note that p(z) is of the form p1(z1) · · · pn(zn) so that, given T = τ, the components of Z are
independent of each other. The EM algorithm for mixture models can now be formulated
as follows.
Algorithm 4.5.1: EM Algorithm for Mixture Models
input: Data τ, initial guess θ(0).
output: Approximation of the maximum likelihood estimate.
1 t ←1
2 while a stopping criterion is not met do
3
Expectation Step: Find p(t)(z) := g(z | τ, θ(t−1)) and Q(t)(θ) := Ep(t)el(θ | τ, Z).
4
Maximization Step: Let θ(t) ←argmaxθ Q(t)(θ).
5
t ←t + 1
6 return θ(t)
A possible termination condition is to stop when
l(θ(t) | τ) −l(θ(t−1) | τ)
 /
l(θ(t) | τ)
 < ε
for some small tolerance ε > 0. As was mentioned in Section 4.3, the sequence of log-
likelihood values does not decrease with each iteration. Under certain continuity con-
ditions, the sequence {θ(t)} is guaranteed to converge to a local maximizer of the log-
likelihood l. Convergence to a global maximizer (if it exists) depends on the appropriate
choice for the starting value. Typically, the algorithm is run from different random starting
points.
For the case of Gaussian mixtures, each ϕk = ϕ(· | µk, Σk), k = 1, . . . , K is the density
of a d-dimensional Gaussian distribution. Let θ(t−1) be the current guess for the optimal
parameter vector, consisting of the weights {w(t−1)
k
}, mean vectors {µ(t−1)
k
}, and covariance
matrices {Σ(t−1)
k
}. We first determine p(t) — the pdf of Z conditional on T = τ — for the
given guess θ(t−1). As mentioned before, the components of Z given T = τ are independent,
Unsupervised Learning
139
so it suffices to specify the discrete pdf, p(t)
i say, of each Zi given the observed point Xi = xi.
The latter can be found from Bayes’ formula:
p(t)
i (k) ∝w(t−1)
k
ϕk(xi | µ(t−1)
k
, Σ(t−1)
k
),
k = 1, . . . , K.
(4.36)
Next, in view of (4.34), the function Q(t)(θ) can be written as
Q(t)(θ) = Ep(t)
n
X
i=1

ln wZi + ln ϕZi(xi | µZi, ΣZi)

=
n
X
i=1
Ep(t)
i
h
ln wZi + ln ϕZi(xi | µZi, Σ

Zi)
i
,
where the {Zi} are independent and Zi is distributed according to p(t)
i in (4.36). This com-
pletes the E-step. In the M-step we maximize Q(t) with respect to the parameter θ; that is,
with respect to the {wk}, {µk}, and {Σk}. In particular, we maximize
n
X
i=1
K
X
k=1
p(t)
i (k) ln wk + ln ϕk(xi | µk, Σk) ,
under the condition P
k wk = 1. Using Lagrange multipliers and the fact that PK
k=1 p(t)
i (k) = 1
gives the solution for the {wk}:
wk = 1
n
n
X
i=1
p(t)
i (k),
k = 1, . . . , K.
(4.37)
The solutions for µk and Σk now follow from maximizing Pn
i=1 p(t)
i (k) ln ϕk(xi | µk, Σk), lead-
ing to
µk =
Pn
i=1 p(t)
i (k) xi
Pn
i=1 p(t)
i (k)
,
k = 1, . . . , K
(4.38)
and
Σk =
Pn
i=1 p(t)
i (k) (xi −µk)(xi −µk)⊤
Pn
i=1 p(t)
i (k)
,
k = 1, . . . , K,
(4.39)
which are very similar to the well-known formulas for the MLEs of the parameters of a
Gaussian distribution. After assigning the solution parameters to θ(t) and increasing the
iteration counter t by 1, the steps (4.36), (4.37), (4.38), and (4.39) are repeated until con-
vergence is reached. Convergence of the EM algorithm is very sensitive to the choice of
initial parameters. It is therefore recommended to try various different starting conditions.
For a further discussion of the theoretical and practical aspects of the EM algorithm we
refer to [85].
Example 4.5 (Clustering via EM) We return to the data in Example 4.4, depicted in
Figure 4.4, and adopt the model that the data is coming from a mixture of three bivariate
Gaussian distributions.
The Python code below implements the EM procedure described in Algorithm 4.5.1.
The initial mean vectors {µk} of the bivariate Gaussian distributions are chosen (from visual
inspection) to lie roughly in the middle of each cluster, in this case [−2, −3]⊤, [−4, 1]⊤, and
[0, −1]⊤. The corresponding covariance matrices are initially chosen as identity matrices,
which is appropriate given the observed spread of the data in Figure 4.4. Finally, the initial
weights are 1/3, 1

/3, 1/3. For simplicity, the algorithm stops after 100 iterations, which in
this case is more than enough to guarantee convergence. The code and data are available
from the book’s website in the GitHub folder Chapter4.
140
Clustering via Mixture Models
EMclust.py
import numpy as np
from scipy.stats import multivariate_normal
Xmat = np.genfromtxt('clusterdata.csv', delimiter=',')
K = 3
n, D = Xmat.shape
W = np.array([[1/3,1/3,1/3]])
M
= np.array([[-2.0,-4,0],[-3,1,-1]], dtype=np.float32)
# Note that if above *all* entries were written as integers , M would
# be defined to be of integer type, which will give the wrong answer
C = np.zeros((3,2,2))
C[:,0,0] = 1
C[:,1,1] = 1
p = np.zeros((3,300))
for i in range(0,100):
#E-step
for k in range(0,K):
mvn = multivariate_normal( M[:,k].T, C[k,:,:] )
p[k,:] = W[0,k]*mvn.pdf(Xmat)
# M-Step
p = (p/sum(p,0))
#normalize
W = np.mean(p,1).reshape(1,3)
for k in range(0,K):
M[:,k] = (Xmat.T @ p[k,:].T)/sum(p[k,:])
xm = Xmat.T - M[:,k].reshape(2,1)
C[k,:,:] = xm @ (xm*p[k,:]).T/sum(p[k,:])
The estimated parameters of the mixture distribution are given on the right-hand side
of Figure 4.6. After relabeling of the clusters, we can observe a close match with the
parameters in Figure 4.4.
The ellipses on the left-hand side of Figure 4.6 show a close match between the 95%
probability ellipses2 of the original Gaussian distributions (in gray) and the estimated ones.
A natural way to cluster each point xi is to assign it to the cluster k for which the conditional
probability pi(k) is maximal (with ties resolved arbitrarily). This gives the clustering of the
points into red, green, and blue clusters in the figure.
2For each mixture component, the contour of the corresponding bivariate normal pdf is shown that en-
closes 95% of the probability mass.
Unsupervised Learning
141
-6
-4
-2
0
2
4
-4
-3
-2
-1
0
1
2
3
weight
mean vector
covariance matrix
0.33
"−1.51
−3.01
#
"1.75
0.03
0.03
0.095
#
0.32
" −4.08
−0.033
#
"1.37
0.92
0.92
1.03
#
0.35
" 0.36


−0.88
#
" 1.93
−1.20
−1.20
1.44
#
Figure 4.6: The results of the EM clustering algorithm applied to the data depicted in
Figure 4.4.
As an alternative to the EM algorithm, one can of course use continuous multiextremal
optimization algorithms to directly optimize the log-likelihood function l(θ | τ) = ln g(τ | θ)
in (4.33) over the set Θ of all possible θ. This is done for example in [15], demonstrating
superior results to EM when there are few data points. Closer investigation of the likelihood
function reveals that there is a hidden problem with any maximum likelihood approach for
clustering if Θ is chosen as large as possible — i.e., any mixture distribution is possible. To
demonstrate this problem, consider Figure 4.7, depicting the probability density function,
g(· | θ) of a mixture of two Gaussian distributions, where θ = [w, µ1, σ2
1, µ2, σ2
2]⊤is the
vector of parameters for the mixture distribution. The log-likelihood function is given by
l(θ | τ) = P4
i=1 ln g(xi | θ), where x1, . . . , x4 are the data (indicated by dots in the figure).
-4
-2
0
2
4
6
8
0
0.1
0.2
0.3
0.4
0.5
Figure 4.7: Mixture of two Gaussian distributions.
It is clear that by fixing the mixing constant w at 0.25 (say) and centering the first
cluster at x1, one can obtain an arbitrarily large likelihood value by taking the variance of
the first cluster to be arbitrarily small. Similarly, for higher dimensional data, by choosing
“point” or “line” clusters, or in general “degenerate” clusters, one can make the value of
the likelihood infinite. This is a manifestation of the familiar overfitting problem for the
142
Clustering via Vector Quantization
training loss that we already encountered in Chapter 2. Thus, the unconstrained maximiza-
tion of the log-likelihood function is an ill-posed problem, irrespective of the choice of the
optimization algorithm!
Two possible solutions to this “overfitting” problem are:
1.
Restrict the parameter set Θ in such a way that degenerate clusters (sometimes

 called
spurious clusters) are not allowed.
2.
Run the given algorithm and if the solution is degenerate, discard it and run the
algorithm afresh. Keep restarting the algorithm until a non-degenerate solution is
obtained.
The first approach is usually applied to multiextremal optimization algorithms and the
second is used for the EM algorithm.
4.6
Clustering via Vector Quantization
In the previous section we introduced clustering via mixture models, as a form of paramet-
ric density estimation (as opposed to the nonparametric density estimation in Section 4.4).
The clusters were modeled in a natural way via the latent variables and the EM algorithm
provided a convenient way to assign the cluster members. In this section we consider a
more heuristic approach to clustering by ignoring the distributional properties of the data.
The resulting algorithms tend to scale better with the number of samples n and the dimen-
sionality d.
In mathematical terms, we consider the following clustering (also called data segment-
ation) problem. Given a collection τ = {x1, . . . , xn} of data points in some d-dimensional
space X, divide this data set into K clusters (groups) such that some loss function is min-
imized. A convenient way to determine these clusters is to first divide up the entire space
X, using some distance function dist(·, ·) on this space. A standard choice is the Euclidean
(or L2) distance:
dist(x, x′) = ∥x −x′∥=
v
t d
X
i=1
(xi −x′
i)2.
Other commonly used distance measures on Rd include the Manhattan distance
Manhattan
distance
:
d
X
i=1
|xi −x′
i|
and the maximum distance
maximum
distance
:
max
i=1,...,d |xi −x′
i|.
On the set of strings of length d, an often-used distance measure is the Hamming distance
Hamming
distance
:
d
X
i=1
1{xi , x′
i},
that is, the number of mismatched characters. For example, the Hamming distance between
010101 and 011010 is 4.
Unsupervised Learning
143
We can partition the space X into regions as follows: First, we choose K points
c1,

 . . . , cK called cluster centers or source vectors
source vectors
. For each k = 1, . . . , K, let
Rk = {x ∈X : dist(x, ck) ⩽dist(x, ci) for all i , k}
be the set of points in X that lie closer to ck than any other center. The regions or cells
{Rk} divide the space X into what is called a Voronoi diagram or a Voronoi tessellation
Voronoi
tessellation
.
Figure 4.8 shows a Voronoi tessellation of the plane into ten regions, using the Euclidean
distance. Note that here the boundaries between the Voronoi cells are straight line seg-
ments. In particular, if cell Ri and Rj share a border, then a point on this border must satisfy
∥x −ci∥= ∥x −c j∥; that is, it must lie on the line that passes through the point (c j + ci)/2
(that is, the midway point of the line segment between ci and c j) and be perpendicular to
cj −ci.
-2
0
2
4
-2
0
2
Figure 4.8: A Voronoi tessellation of the plane into ten cells, determined by the (red) cen-
ters.
Once the centers (and thus the cells {Rk}) are chosen, the points in τ can be clustered
according to their nearest center. Points on the boundary have to be treated separately. This
is a moot point for continuous data, as generally no data points will lie exactly on the
boundary.
The main remaining issue is how to choose the centers so as to cluster the data in some
optimal way. In terms of our (unsupervised) learning framework, we wish to approximate
a vector x via one of c1, . . . , cK, using a piecewise constant vector-valued function
g(x | C) :=
K
X
k=1
ck 1{x ∈Rk},
where C is the d × K matrix [c1, . . . , cK]. Thus, g(x | C) = ck when x falls in region Rk (we
ignore ties). Within this class G of functions, parameterized by C, our aim is to minimize
the training loss. In particular, for the squared-error loss, Loss(x, x′) = ∥x−x′∥2, the training
loss is
ℓτn(g(· | C)) = 1
n
n
X
i=1
∥xi −g(xi | C)∥2 = 1
n
K
X
k=1
X
x∈Rk∩τn
||x −ck||2.
(4.40)
Thus, the training loss minimizes the average squared distance between the centers. This
framework 

also combines both the encoding and decoding steps in vector quantization
vector
quantization
144
Clustering via Vector Quantization
[125]. Namely, we wish to “quantize” or “encode” the vectors in τ in such a way that each
vector is represented by one of K source vectors c1, . . . , cK, such that the loss (4.40) of this
representation is minimized.
Most well-known clustering and vector quantization methods update the vector of cen-
ters, starting from some initial choice and using iterative (typically gradient-based) proced-
ures. It is important to realize that in this case (4.40) is seen as a function of the centers,
where each point x is assigned to the nearest center, thus determining the clusters. It is well
known that this type of problem — optimization with respect to the centers — is highly
multiextremal and, depending on the initial clusters, gradient-based procedures tend to
converge to a local minimum rather than a global minimum.
4.6.1
K-Means
One of the simplest methods for clustering is the K-means method. It is an iterative method
where, starting from an initial guess for the centers, new centers are formed by taking
sample means of the current points in each cluster. The new centers are thus the centroids
centroids
of the points in each cell. Although there exist many different varieties of the K-means
algorithm, they are all essentially of the following form:
Algorithm 4.6.1: K-Means
input: Collection of points τ = {x1, . . . , xn}, number of clusters K, initial centers
c1, . . . , cK.
output: Cluster centers and cells (regions).
1 while a stopping criterion is not met do
2
R1, . . . , RK ←∅(empty sets).
3
for i = 1 to n do
4
d ←[dist(xi, c1), . . . , dist(xi, cK)]
// distances to centers
5
k ←argminj dj
6
Rk ←Rk ∪{xi}
// assign xi to cluster k
7
for k = 1 to K do
8
ck ←
P
x∈Rk x
|Rk|
// compute the new center as a centroid of points
9 return {ck}, {Rk}
Thus, at each iteration, for a given choice of centers, each point in τ is assigned to
its neares

t center. After all points have been assigned, the centers are recomputed as the
centroids of all the points in the current cluster (Line 8). A typical stopping criterion is to
stop when the centers no longer change very much. As the algorithm is quite sensitive to
the choice of the initial centers, it is prudent to try multiple starting values, e.g., chosen
randomly from the bounding box of the data points.
We can see the K-means method as a deterministic (or “hard”) version of the probab-
ilistic (or “soft”) EM algorithm as follows. Suppose in the EM algorithm we have Gaus-
sian mixtures with a fixed covariance matrix Σk = σ2Id, k = 1, . . . , K, where σ2 should be
thought of as being infinitesimally small. Consider iteration t of the EM algorithm. Having
obtained the expectation vectors µ(t−1)
k
and weights w(t−1)
k
, k = 1, . . . , K, each point xi is as-
signed a cluster label Zi according to the probabilities p(t)
i (k), k = 1, . . . , K given in (4.36).
Unsupervised Learning
145
But for σ2 →0 the probability distribution {p(t)
i (k)} becomes degenerate, putting all its
probability mass on argmink ∥xi −µk∥2. This corresponds to the K-means rule of assigning
xi to its nearest cluster center. Moreover, in the M-step (4.38) each cluster center µ(t)
k is now
updated according to the average of the {xi} that have been assigned to cluster k. We thus
obtain the same deterministic updating rule as in K-means.
Example 4.6 (K-means Clustering) We cluster the data from Figure 4.4 via K-means,
using the Python implementation below. Note that the data points are stored as a 300 × 2
matrix Xmat. We take the same starting centers as in the EM example: c1 = [−2, −3]⊤, c2 =
[−4, 1]⊤, and c3 = [0, −1]⊤. Note also that squared Euclidean distances are used in the
computations, as these are slightly faster to compute than Euclidean distances (as no square
root computations are required) while yielding exactly the same cluster center evaluations.
Kmeans.py
import numpy as np
Xmat =

 np.genfromtxt('clusterdata.csv', delimiter=',')
K = 3
n, D = Xmat.shape
c
= np.array([[-2.0,-4,0],[-3,1,-1]])
#initialize centers
cold = np.zeros(c.shape)
dist2 = np.zeros((K,n))
while np.abs(c - cold).sum() > 0.001:
cold = c.copy()
for i in range(0,K): #compute the squared distances
dist2[i,:] = np.sum((Xmat - c[:,i].T)**2, 1)
label = np.argmin(dist2 ,0) #assign the points to nearest centroid
minvals = np.amin(dist2 ,0)
for i in range(0,K): # recompute the centroids
c[:,i] = np.mean(Xmat[np.where(label == i),:],1).reshape(1,2)
print('Loss = {:3.3f}'.format(minvals.mean()))
Loss = 2.288
146
Clustering via Vector Quantization
-6
-4
-2
0
2
4
-5
-4
-3
-2
-1
0
1
2
3
Figure 4.9: Results of the K-means algorithm applied to the data in Figure 4.4. The thick
black circles are the centroids and the dotted lines define the cell boundaries.
We found the cluster centers c1 = [−1.9286, −3.0416]⊤, c2 = [−3.9237, 0.0131]⊤, and
c3 = [0.5611, −1.2980]⊤, giving the clustering depicted in Figure 4.9. The corresponding
loss (4.40) was found to be 2.288.
4.6.2
Clustering via Continuous Multiextremal Optimization
As already mentioned, the exact minimization of the loss function (4.40) is difficult to
accomplish via standard local search methods such as gradient descent, as the function
is highly multimodal. However, nothing is preventing us from using global optimization
methods such as the CE or SCO methods discussed in Sections 3.4.2 and 3.4.3.
☞100
Example 4.7 (Clustering via CE) We take the same data set as in Example 4.6 and
cluster the points via minimization of the loss (4.40) using the CE method. The Python
code below is very similar to the code in Example 3.16, except that now we are dealing
☞101
with a six-dimensional optimization problem. The loss function is implemented in the func-
tion Scluster, which essentially reuses the squared distance computation of the K-means
code in Example 4.6. The CE program typically converges to a loss of 2.287, correspond-
ing to the (global)

 minimizers c1 = [−1.9286, −3.0416]⊤, c2 = [−3.8681, 0.0456]⊤, and
c3 = [0.5880, −1.3526]⊤, which slightly differs from the local minimizers for the K-means
algorithm.
clustCE.py
import numpy as np
np.set_printoptions(precision=4)
Xmat = np.genfromtxt('clusterdata.csv', delimiter=',')
K = 3
n, D = Xmat.shape
def Scluster(c):
n, D = Xmat.shape
dist2 = np.zeros((K,n))
Unsupervised Learning
147
cc = c.reshape(D,K)
for i in range(0,K):
dist2[i,:] = np.sum((Xmat - cc[:,i].T)**2, 1)
minvals = np.amin(dist2 ,0)
return minvals.mean()
numvar = K*D
mu = np.zeros(numvar)
#initialize centers
sigma = np.ones(numvar)*2
rho = 0.1
N = 500; Nel = int(N*rho); eps = 0.001
func = Scluster
best_trj = np.array(numvar)
best_perf = np.Inf
trj = np.zeros(shape=(N,numvar))
while(np.max(sigma)>eps):
for i in range(0,numvar):
trj[:,i] = (np.random.randn(N,1)*sigma[i]+ mu[i]).reshape(N,)
S = np.zeros(N)
for i in range(0,N):
S[i] = func(trj[i])
sortedids = np.argsort(S) # from smallest to largest
S_sorted = S[sortedids]
best_trj = np.array(n)
best_perf = np.Inf
eliteids = sortedids[range(0,Nel)]
eliteTrj = trj[eliteids ,:]
mu = np.mean(eliteTrj ,axis=0)
sigma = np.std(eliteTrj ,axis=0)
if(best_perf >S_sorted[0]):
best_perf = S_sorted[0]
best_trj = trj[sortedids[0]]
print(best_perf)
print(best_trj.reshape(2,3))
2.2874901831572947
[[-3.9238 -1.8477
0.5895]
[ 0.0134 -3.0292 -1.2442]]
4.7
Hierarchical Clustering
It is sometimes useful to determine data clusters in a hierarchical manner; an example
is the construction of evolutionary relationships between animal species. Establishing a
hierarchy of clusters can be done in a bottom-up or a top-down manner. In the bottom-up
approach, also called agglomerative clustering
agglomerative
clustering
, the data points are merged in larger and
larger clusters until all the points have been merged into a single cluster. In the top-down
148
Hierarchical Clustering
or divisive clustering
divisive
clustering
approach, the data set is divided up into smaller and s

maller clusters.
The left panel of Figure 4.10 depicts a hierarchy of clusters.
7
8
6
1
2
3
4
5
Labels
10
20
30
40
Distance
Figure 4.10: Left: a cluster hierarchy of 15 clusters. Right: the corresponding dendrogram.
In Figure 4.10, each cluster is given a cluster identifier. At the lowest level are clusters
comprised of the original data points (identifiers 1, . . . , 8). The union of clusters 1 and 2
form a cluster with identifier 9, and the union of 3 and 4 form a cluster with identifier 10.
In turn the union of clusters 9 and 10 constitutes cluster 12, and so on.
The right panel of Figure 4.10 shows a convenient way to visualize cluster hierarchies
using a dendrogram
dendrogram
(from the Greek dendro for tree). A dendrogram not only summarizes
how clusters are merged or split, but also shows the distance between clusters, here on the
vertical axis. The horizontal axis shows which cluster each data point (label) belongs to.
Many different types of hierarchical clustering can be performed, depending on how
the distance is defined between two data points and between two clusters. Denote the data
set by X = {xi, i = 1, . . . , n}. As in Section 4.6, let dist(xi, xj) be the distance between data
points xi and xj. The default choice is the Euclidean distance dist(xi, xj) = ∥xi −xj∥.
Let I and J be two disjoint subsets of {1, . . . , n}. These sets correspond to two disjoint
subsets (that is, clusters) of X: {xi, i = I} and {xj, j = J}. We denote the distance between
these two clusters by d(I, J). By specifying the function d, we indicate how the clusters
are linked. For this reason it is also referred to as the linkage
linkage
criterion. We give a number
of examples:
• Single linkage. The closest distance between the clusters.
dmin(I, J) := min
i∈I,j∈J dist(xi, x j).
• Complete linkage. The furthest distance between the clusters.
dmax(I, J) := max
i∈I,j∈J dist(xi, xj).
• Group average. The mean distance between the clusters. Note that this depends on
the cluster sizes.


davg(I, J) :=
1
|I| |J|
X
i∈I
X
j∈J
dist(xi, xj).
Unsupervised Learning
149
For these linkage criteria, X is usually assumed to be Rd with the Euclidean distance.
Another notable measure for the distance between clusters is Ward’s minimum vari-
ance linkage criterion.
Ward’s linkage
Here, the distance between clusters is expressed as the additional
amount of “variance” (expressed in terms of the sum of squares) that would be intro-
duced if the two clusters were merged. More precisely, for any set K of indices (labels) let
xK = P
k∈K xk/|K| denote its corresponding cluster mean. Then
dWard(I, J) :=
X
k∈I∪J
∥xk −xI∪J∥2 −

X
i∈I
∥xi −xI∥2 +
X
j∈J
∥xj −xJ∥2
.
(4.41)
It can be shown (see Exercise 8) that the Ward linkage depends only on the cluster means
and the cluster sizes for I and J:
dWard(I, J) =
|I| |J|
|I| + |J|∥xI −xJ∥2.
In software implementations, the Ward linkage function is often rescaled by mul-
tiplying it by a factor of 2. In this way, the distance between one-point clusters {xi}
and {xj} is the squared Euclidean distance ∥xi −xj∥2.
Having chosen a distance on X and a linkage criterion, a general agglomerative clus-
tering algorithm proceeds in the following “greedy” manner.
Algorithm 4.7.1: Greedy Agglomerative Clustering
input: Distance function dist, linkage function d, number of clusters K.
output: The label sets for the tree.
1 Initialize the set of cluster identifiers: I = {1, . . . , n}.
2 Initialize the corresponding label sets: Li = {i}, i ∈I.
3 Initialize a distance matrix D = [di j] with di j = d({i}, { j}).
4 for k = n + 1 to 2n −K do
5
Find i and j > i in I such that di j is minimal.
6
Create a new label set Lk := Li ∪L j.
7
Add the new identifier k to I and remove the old identifiers i and j from I.
8
Update the distance matrix D with respect to the identifiers i, j, and k.
9 return Li, i = 1, . . . , 2n −K
Initially, the distance matrix D contains the (linkage) distances between the one-point
clusters containing one of the

 data points x1, . . . , xn, and hence with identifiers 1, . . . , n.
Finding the shortest distance amounts to a table lookup in D. When the closest clusters
are found, they are merged into a new cluster, and a new identifier k (the smallest positive
integer that has not yet been used as an identifier) is assigned to this cluster. The old iden-
tifiers i and j are removed from the cluster identifier set I. The matrix D is then updated
by adding a k-th column and row that contain the distances between k and any m ∈I. This
updating step could be computationally quite costly if the cluster sizes are large and the
linkage distance between the clusters depends on all points within the clusters. Fortunately,
for many linkage functions, the matrix D can be updated in an efficient manner.
150
Hierarchical Clustering
Suppose that at some stage in the algorithm, clusters I and J, with identifiers i and j,
respectively, are merged into a cluster K = I ∪J with identifier k. Let M, with identifier
m, be a previously assigned cluster. An update rule of the linkage distance dkm between K
and M is called a Lance–Williams
Lance–
Williams
update if it can be written in the form
dkm = α dim + β djm + γ di j + δ |dim −djm|,
where α, . . . , δ depend only on simple characteristics of the clusters involved, such as the
number of elements within the clusters. Table 4.2 shows the update constants for a number
of common linkage functions. For example, for single linkage, dim is the minimal distance
between I and M, and djm is the minimal distance between J and M. The smallest of
these is the minimal distance between K and M. That is, dkm = min{dim, djm} = dim/2 +
djm/2 −|dim −djm|/2.
Table 4.2: Constants for the Lance–Williams update rule for various linkage functions,
with ni, nj, nm denoting the number of elements in the corresponding clusters.
Linkage
α
β
γ
δ
Single
1/2
1/2
0
−1/2
Complete
1/2
1/2
0
1/2
Group avg.
ni
ni + nj
nj
ni + nj
0
0
Ward
ni + nm
ni + nj + nm
n j + nm
ni + nj + nm


−nm
ni + nj + nm
0
In practice, Algorithm 4.7.1 is run until a single cluster is obtained. Instead of returning
the label sets of all 2n −1 clusters, a linkage matrix
linkage matrix
is returned that contains the same
information. At the end of each iteration (Line 8) the linkage matrix stores the merged
labels i and j, as well as the (minimal) distance di j. Other information such as the number
of elements in the merged cluster can also be stored. Dendrograms and cluster labels can be
directly constructed from the linkage matrix. In the following example, the linkage matrix
is returned by the method agg_cluster.
Example 4.8 (Agglomerative Hierarchical Clustering) The Python code below gives
a basic implementation of Algorithm 4.7.1 using the Ward linkage function. The methods
fcluster and dendrogram from the scipy module can be used to identify the labels in
a cluster and to draw the corresponding dendrogram.
AggCluster.py
import numpy as np
from scipy.spatial.distance import cdist
def update_distances(D,i,j, sizes): # distances for merged cluster
n = D.shape[0]
d = np.inf * np.ones(n+1)
for k in range(n): # Update distances
d[k] = ((sizes[i]+sizes[k])*D[i,k] +
(sizes[j]+sizes[k])*D[j,k] -
Unsupervised Learning
151
sizes[k]*D[i,j])/(sizes[i] + sizes[j] + sizes[k])
infs =
np.inf * np.ones(n) # array of infinity
D[i,:],D[:,i],D[j,:],D[:,j] =
infs,infs,infs,infs # deactivate
new_D = np.inf * np.ones((n+1,n+1))
new_D[0:n,0:n] = D # copy old matrix into new_D
new_D[-1,:], new_D[:,-1] = d,d # add new row and column
return new_D
def agg_cluster(X):
n = X.shape[0]
sizes = np.ones(n)
D = cdist(X, X,metric = 'sqeuclidean') # initialize dist. matrix
.
np.fill_diagonal(D, np.inf * np.ones(D.shape[0]))
Z = np.zeros((n-1,4))
#linkage matrix encodes hierarchy tree
for t in range(n-1):
i,j = np.unravel_index(D.argmin(), D.shape) # minimizer pair
sizes = np.append(sizes, sizes[i] + sizes[j])
Z[t,:]=np.array([i, j, np.sqrt(D[i,j]), sizes[-1]])
D = update_distances(D, i,j, sizes)
# up

date distance matr.
return Z
import scipy.cluster.hierarchy as h
X = np.genfromtxt('clusterdata.csv',delimiter=',') # read the data
Z = agg_cluster(X)
# form the linkage matrix
h.dendrogram(Z) # SciPy can produce a dendrogram from Z
# fcluster function assigns cluster ids to all points based on Z
cl = h.fcluster(Z, criterion = 'maxclust', t=3)
import matplotlib.pyplot as plt
plt.figure(2), plt.clf()
cols = ['red','green','blue']
colors = [cols[i-1] for i in cl]
plt.scatter(X[:,0], X[:,1],c=colors)
plt.show()
Note that the distance matrix is initialized with the squared Euclidean distance, so that
the Ward linkage is rescaled by a factor of 2. Also, note that the linkage matrix stores
the square root of the minimal cluster distances rather than the distances themselves. We
leave it as an exercise to check that by using these modifications the results agree with the
linkage method from scipy; see Exercise 9.
In contrast to the bottom-up (agglomerative) approach to hierarchical clustering, the
divisive approach starts with one cluster, which is divided into two clusters that are as
“dissimilar” as possible, which can then be further divided, and so on. We can use the same
linkage criteria as for agglomerative clustering to divide a parent cluster into two child
clusters by maximizing the distance between the child clusters. Although it is a natural to try
to group together data by separating dissimilar ones as far as possible, the implementation
of this idea tends to scale poorly with n. The problem is related to the well-known max-cut
problem
max-cut
problem
: given an n × n matrix of positive costs ci j, i, j ∈{1, . . . , n}, partition the index set
152
Hierarchical Clustering
I = {1, . . . , n} into two subsets J and K such that the total cost across the sets, that is,
X
j∈J
X
k∈K
djk,
is maximal. If instead we maximize according to the average distance, we obtain the group
average linkage criterion.
Example 4.9 (Divisive Clustering via CE) The following Python code

 is used to di-
vide a small data set (of size 300) into two parts according to maximal group average link-
age. It uses a short cross-entropy algorithm similar to the one presented in Example 3.19.
Given a vector of probabilities {pi, i = 1, . . . , n}, the algorithm generates an n × n matrix
☞111
of Bernoulli random variables with success probability pi for column i. For each row, the
0s and 1s divide the index set into two clusters, and the corresponding average linkage
distance is computed. The matrix is then sorted row-wise according to these distances. Fi-
nally, the probabilities {pi} are updated according to the mean values of the best 10% rows.
The process is repeated until the {pi} degenerate to a binary vector. This then presents the
(approximate) solution.
clustCE2.py
import numpy as np
from numpy import genfromtxt
from scipy.spatial.distance import squareform
from scipy.spatial.distance import pdist
import matplotlib.pyplot as plt
def S(x,D):
V1 = np.where(x==0)[0] # {V1,V2} is the partition
V2 = np.where(x==1)[0]
tmp = D[V1]
tmp = tmp[:,V2]
return np.mean(tmp) # the size of the cut
def maxcut(D,N,eps,rho,alpha):
n = D.shape[1]
Ne = int(rho*N)
p = 1/2*np.ones(n)
p[0] = 1.0
while (np.max(np.minimum(p,np.subtract(1,p))) > eps):
x = np.array(np.random.uniform(0,1,(N,n))<=p, dtype=np.int64)
sx = np.zeros(N)
for i in range(N):
sx[i] = S(x[i],D)
sortSX = np.flip(np.argsort(sx))
#print("gamma = ",sx[sortSX[Ne-1]], " best=",sx[sortSX[0]])
elIds = sortSX[0:Ne]
elites = x[elIds]
pnew = np.mean(elites, axis=0)
p = alpha*pnew + (1.0-alpha)*p
Unsupervised Learning
153
return np.round(p)
Xmat = genfromtxt('clusterdata.csv', delimiter=',')
n = Xmat.shape[0]
D = squareform(pdist(Xmat))
N = 1000
eps = 10**-2
rho = 0.1
alpha = 0.9
# CE
pout = maxcut(D,N,eps,rho, alpha);
cutval = S(pout,D)
print("cutvalue ",cutval)
#plot
V1 = np.where(pout==0)[0]
xblue = Xmat[V1]
V2 = np.where(pout==1)[0]
xred = Xmat[V2]
plt.scatter(xblue[:,0],xblue[:,1], c="blue")
plt.scatter(xred[:,0],x

red[:,1], c="red")
cutvalue
4.625207676517948
6
4
2
0
2
4
4
3
2
1
0
1
2
3
Figure 4.11: Division of the data in Figure 4.4 into two clusters, via the cross-entropy
method.
4.8
Principal Component Analysis (PCA)
The main idea of principal component analysis
principal
component
analysis
(PCA) is to reduce the dimensionality of
a data set consisting of many variables. PCA is a feature reduction (or feature extraction)
154
Principal Component Analysis (PCA)
mechanism, that helps us to handle high-dimensional data with more features than is con-
venient to interpret.
4.8.1
Motivation: Principal Axes of an Ellipsoid
Consider a d-dimensional normal distribution with mean vector 0 and covariance matrix
Σ. The corresponding pdf (see (2.33)) is
☞46
f(x) =
1
√(2π)n |Σ|
e−1
2 x⊤Σ−1x,
x ∈Rd.
If we were to draw many iid samples from this pdf, the points would roughly have an
ellipsoid pattern, as illustrated in Figure 3.1, and correspond to the contours of f: sets of
☞71
points x such that x⊤Σ−1x = c, for some c ⩾0. In particular, consider the ellipsoid
x⊤Σ−1x = 1,
x ∈Rd.
(4.42)
Let Σ = BB⊤, where B is for example the (lower) Cholesky matrix. Then, as explained
☞373
in Example A.5, the ellipsoid (4.42) can also be viewed as the linear transformation of
☞366
d-dimensional unit sphere via matrix B. Moreover, the principal axes of the ellipsoid can
principal axes
be found via a singular value decomposition (SVD) of B (or Σ); see Section A.6.5 and
singular value
decomposition
Example A.8. In particular, suppose that an SVD of B is
☞378
B = UDV⊤
(note that an SVD of Σ is then UD2U⊤).
The columns of the matrix UD correspond to the principal axes of the ellipsoid, and the
relative magnitudes of the axes are given by the elements of the diagonal matrix D. If some
of these magnitudes are small compared to the others, a reduction in the dimension of the
space may be achieved by projecting each point x ∈Rd onto the subspace spanned by the
main (say k ≪d) columns of U — the so-called principa

l components
principal
components
. Suppose without
loss of generality that the first k principal components are given by the first k columns of
U, and let Uk be the corresponding d × k matrix.
With respect to the standard basis {ei}, the vector x = x1e1 +· · ·+ xded is represented by
the d-dimensional vector [x1, . . . , xd]⊤. With respect to the orthonormal basis {ui} formed
by the columns of matrix U, the representation of x is U⊤x. Similarly, the projection of
any point x onto the subspace spanned by the first k principal vectors is represented by the
k-dimensional vector U⊤
k x, with respect to the orthonormal basis formed by the columns of
Uk. So, the idea is that if a point x lies close to its projection UkU⊤
k x, we may represent it via
k numbers instead of d, using the combined features given by the k principal components.
See Section A.4 for a review of projections and orthonormal bases.
☞362
Example 4.10 (Principal Components) Consider the matrix
Σ =

14
8
3
8
5
2
3
2
1
,
which can be written as Σ = BB⊤, with
B =

1
2
3
0
1
2
0
0
1
.
Unsupervised Learning
155
Figure 4.12 depicts the ellipsoid x⊤Σ−1x = 1, which can be obtained by linearly transform-
ing the points on the unit sphere by means of the matrix B. The principal axes and sizes of
the ellipsoid are found through a singular value decomposition B = UDV⊤, where U and
D are
U =

0.8460
0.4828
0.2261
0.4973
−0.5618
−0.6611
0.1922
−0.6718
0.7154

and
D =

4.4027
0
0
0
0.7187
0
0
0
0.3160
.
The columns of U show the directions of the principal axes of the ellipsoid, and the di-
agonal elements of D indicate the relative magnitudes of the principal axes. We see that
the first principal component is given by the first column of U, and the second principal
component by the second column of U.
The projection of the point x = [1.052, 0.6648, 0.2271]⊤onto the 1-dimensional space
spanned by the first principal component u1 = [0.8460

, 0.4972, 0.1922]⊤is z = u1u⊤
1 x =
[1.0696, 0.6287, 0.2429]⊤. With respect to the basis vector u1, z is represented by the num-
ber u⊤
1 z = 1.2643. That is, z = 1.2643u1.
Figure 4.12: A “surfboard” ellipsoid where one principal axis is significantly larger than
the other two.
4.8.2
PCA and Singular Value Decomposition (SVD)
In the setting above, we did not consider any data set drawn from a multivariate pdf f. The
whole analysis rested on linear algebra. In principal component analysis
principal
component
analysis
(PCA) we start
with data x1, . . . , xn, where each x is d-dimensional. PCA does not require assumptions
how the data were obtained, but to make the link with the previous section, we can think
of the data as iid draws from a multivariate normal pdf.
Let us collect the data in a matrix X in the usual way; that is,
☞
44
X =

x11
x12
. . .
x1d
x21
x22
. . .
x2d
...
...
...
...
xn1
xn2
. . .
xnd

=

x⊤
1
x⊤
2...
x⊤
n

.
156
Principal Component Analysis (PCA)
The matrix X will be the PCA’s input. Under this setting, the data consists of points in d-
dimensional space, and our goal is to present the data using n feature vectors of dimension
k < d.
In accordance with the previous section, we assume that underlying distribution of the
data has expectation vector 0. In practice, this means that before PCA is applied, the data
needs to be centered by subtracting the column mean in every column:
x′
i j = xi j −x j,
where x j = 1
n
Pn
i=1 xi j.
We assume from now on that the data comes from a general d-dimensional distribution
with mean vector 0 and some covariance matrix Σ. The covariance matrix Σ is by definition
equal to the expectation of the random matrix XX⊤, and can be estimated from the data
x1, . . . , xn via the sample average
bΣ = 1
n
n
X
i=1
xix⊤
i = 1
nX⊤X.
As bΣ is a covariance matrix, we may conduct the same analysis for bΣ as we did for Σ in the
previous section. Specifically, suppo

se bΣ = UD2U⊤is an SVD of bΣ and let Uk be the matrix
whose columns are the k principal components; that is, the k columns of U corresponding to
the largest diagonal elements in D2. Note that we have used D2 instead of D to be compat-
ible with the previous section. The transformation zi = UkU⊤
k xi maps each vector xi ∈Rd
(thus, with d features) to a vector zi ∈Rd lying in the subspace spanned by the columns of
Uk. With respect to this basis, the point zi has representation zi = U⊤
k (UkU⊤
k xi) = U⊤
k xi ∈Rk
(thus with k features). The corresponding covariance matrix of the zi, i = 1, . . . , n is diag-
onal. The diagonal elements {dℓℓ} of D can be interpreted as standard deviations of the data
in the directions of the principal components. The quantity v = P
ℓ=1 d2
ℓℓ(that is, the trace of
D2) is thus a measure for the amount of variance in the data. The proportion d2
ℓℓ/v indicates
how much of the variance in the data is explained by the ℓ-th principal component.
Another way to look at PCA is by considering the question: How can we best project the
data onto a k-dimensional subspace in such a way that the total squared distance between
the projected points and the original points is minimal? From Section A.4, we know that
☞362
any orthogonal projection to a k-dimensional subspace Vk can be represented by a matrix
UkU⊤
k , where Uk = [u1, . . . , uk] and the {uℓ, ℓ= 1, . . . , k} are orthogonal vectors of length 1
that span Vk. The above question can thus be formulated as the minimization program:
min
u1,...,uk
n
X
i=1
∥xi −UkU⊤
k xi∥2.
(4.43)
Unsupervised Learning
157
Now observe that
1
n
n
X
i=1
∥xi −UkU⊤
k xi∥2 = 1
n
n
X
i=1
(x⊤
i −x⊤
i UkU⊤
k )(xi −UkU⊤
k xi)
= 1
n
n
X
i=1
∥xi∥2
|       {z       }
c
−1
n
n
X
i=1
x⊤
i UkU⊤
k xi = c −1
n
n
X
i=1
k
X
ℓ=1
tr(x⊤
i uℓu⊤
ℓxi)
= c −1
n
k
X
ℓ=1
n
X
i=1
u⊤
ℓxix⊤
i uℓ= c −
k
X
ℓ=1
u⊤
ℓbΣ uℓ,
where we have used the cyclic property of a trace (Theorem A.1) and the fact that UkU⊤
k
☞357
can be written as Pk
ℓ=1 uℓu⊤
ℓ. It 

follows that the minimization problem(4.43) is equivalent
to the maximization problem
max
u1,...,uk
k
X
ℓ=1
u⊤
ℓbΣ uℓ.
(4.44)
This maximum can be at most Pk
ℓ=1 d2
ℓℓand is attained precisely when u1, . . . , uk are the
first k principal components of bΣ.
Example 4.11 (Singular Value Decomposition) The following data set consists of in-
dependent samples from the three-dimensional Gaussian distribution with mean vector 0
and covariance matrix Σ given in Example 4.10:
X =

3.1209
1.7438
0.5479
−2.6628
−1.5310
−0.2763
3.7284
3.0648
1.8451
0.4203
0.3553
0.4268
−0.7155
−0.6871
−0.1414
5.8728
4.0180
1.4541
4.8163
2.4799
0.5637
2.6948
1.2384
0.1533
−1.1376
−0.4677
−0.2219
−1.2452
−0.9942
−0.4449

.
After replacing X with its centered version, an SVD UD2U⊤of bΣ = X⊤X/n yields the
principal component matrix U and diagonal matrix D:
U =

−0.8277
0.4613
0.3195
−0.5300
−0.4556
−0.7152
−0.1843
−0.7613
0.6216

and
D =

3.3424
0
0
0
0.4778
0
0
0
0.1038
.
We also observe that, apart from the sign of the first column, the principal component
matrix U is similar to that in Example 4.10. Likewise for the matrix D. We see that 97.90%
of the total variance is explained by the first principal component. Figure 4.13 shows the
projection of the centered data onto the subspace spanned by this principal component.
158
Principal Component Analysis (PCA)
x
4
2
0
2
4
y
2
1
0
1
2
3
z
1.0
0.5
0.0
0.5
1.0
1.5
Figure 4.13: Data from the “surfboard” pdf is projected onto the subspace spanned by the
largest principal component.
The following Python code was used.
PCAdat.py
import numpy as np
X = np.genfromtxt('pcadat.csv', delimiter=',')
n = X.shape[0]
X = X - X.mean(axis=0)
G = X.T @ X
U, _ , _ = np.linalg.svd(G/n)
# projected points
Y = X @ np.outer(U[:,0],U[:,0])
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure()
ax = fig.add_s

ubplot(111, projection='3d')
ax.w_xaxis.set_pane_color((0, 0, 0, 0))
ax.plot(Y[:,0], Y[:,1], Y[:,2], c='k', linewidth=1)
ax.scatter(X[:,0], X[:,1], X[:,2], c='b')
ax.scatter(Y[:,0], Y[:,1], Y[:,2], c='r')
for i in range(n):
ax.plot([X[i,0], Y[i,0]], [X[i,1],Y[i,1]], [X[i,2],Y[i,2]], 'b')
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('z')
plt.show()
Next is an application of PCA to Fisher’s famous iris data set, already mentioned in
Section 1.1, and Exercise 1.5.
☞2
Unsupervised Learning
159
Example 4.12 (PCA for the Iris Data Set) The iris data set contains measurements
on four features of the iris plant: sepal length and width, and petal length and width, for a
total of 150 specimens. The full data set also contains the species name, but for the purpose
of this example we ignore it.
Figure 1.9 shows that there is a significant correlation between the different features.
☞
17
Can we perhaps describe the data using fewer features by taking certain linear combin-
ations of the original features? To investigate this, let us perform a PCA, first centering
the data. The following Python code implements the PCA. It is assumed that a CSV file
irisX.csv has been made that contains the iris data set (without the species information).
PCAiris.py
import seaborn as sns, numpy as np
np.set_printoptions(precision=4)
X = np.genfromtxt('IrisX.csv',delimiter=',')
n = X.shape[0]
X = X - np.mean(X, axis=0)
[U,D2,UT]= np.linalg.svd((X.T @ X)/n)
print('U = \n', U); print('\n diag(D^2) = ', D2)
z =
U[:,0].T @ X.T
sns.kdeplot(z, bw=0.15)
U =
[[-0.3614 -0.6566
0.582
0.3155]
[ 0.0845 -0.7302 -0.5979 -0.3197]
[-0.8567
0.1734 -0.0762 -0.4798]
[-0.3583
0.0755 -0.5458
0.7537]]
diag(D^2) =
[4.2001 0.2411 0.0777 0.0237]
The output above shows the principal component matrix (which we called U) as well as
the diagonal of matrix D2. We see that a large proportion of the variance, 4.2001/(4.2001+
0.2411+0.0777+0.0237) = 92.46%, is explained by the first principal component. Thus, it
makes sense

 to transform each data point x ∈R4 to u⊤
1 x ∈R. Figure 4.14 shows the kernel
density estimate of the transformed data. Interestingly, we see two modes, indicating at
least two clusters in the data.
160
Exercises
-4
-3
-2
-1
0
1
2
3
4
PCA-combined data
0
0.2
0.4
0.6
kernel density estimate
Figure 4.14: Kernel density estimate of the PCA-combined iris data.
Further Reading
Various information-theoretic measures to quantify uncertainty, including the Shannon en-
tropy and Kullback–Leibler divergence, may be found in [28]. The Fisher information, the
prominent information measure in statistics, is discussed in detail in [78]. Akaike’s inform-
ation criterion appeared in [2]. The EM algorithm was introduced in [31] and [85] gives an
in-depth treatment. Convergence proofs for the EM algorithm may be found in [19, 128].
A classical reference on kernel density estimation is [113], and [14] is the main reference
for the theta kernel density estimator. Theory and applications on finite mixture models
may be found in [86]. For more details on clustering applications and algorithms as well
as references on data compression, vector quantization, and pattern recognition, we refer
to [1, 35, 107, 125]. A useful modification of the K-means algorithm is the fuzzy K-means
algorithm; see, e.g., [9]. A popular way to choose the starting positions in K-means is given
by the K-means++ heuristic, introduced in [4].
Exercises
1. This exercise is to show that the Fisher information matrix F(θ) in (4.8) is equal to the
matrix H(θ) in (4.9), in the special case where f = g(· | θ), and under the assumption that
integration and differentiation orders can be interchanged.
(a) Let h be a vector-valued function and k a real-valued function. Prove the following
quotient rule for differentiation
quotient rule
for
differentiation
:
∂[h(θ)/k(θ)]
∂θ
=
1
k(θ)
∂h(θ)
∂θ
−
1
k2(θ)
∂k(θ)
∂θ h(θ)⊤.
(4.45)
(b) Now take h(θ) =
∂g(X | θ)
∂θ
and k(θ) = g(X | θ) in (4.45) and take expectations with
respect to E

θ on both sides to show that
−H(θ) = Eθ

1
g(X | θ)
∂∂g(X | θ)
∂θ
∂θ

|                    {z                    }
A
−F(θ).
Unsupervised Learning
161
(c) Finally show that A is the zero matrix.
2. Plot the mixture of N(0, 1), U(0, 1), and Exp(1) distributions, with weights w1 = w2 =
w3 = 1/3.
3. Denote the pdfs in Exercise 2 by f1, f2, f3, respectively. Suppose that X is simulated via
the two-step procedure: First, draw Z from {1, 2, 3}, then draw X from fZ. How likely is it
that the outcome x = 0.5 of X has come from the uniform pdf f2?
4. Simulate an iid training set of size 100 from the Gamma(2.3, 0.5) distribution, and
implement the Fisher scoring method in Example 4.1 to find the maximum likelihood es-
timate. Plot the true and approximate pdfs.
5. Let T = {X1, . . . , Xn} be iid data from a pdf g(x | θ) with Fisher matrix F(θ). Explain
why, under the conditions where (4.7) holds,
ST(θ) := 1
n
n
X
i=1
S(Xi | θ)
for large n has approximately a multivariate normal distribution with expectation vector 0
and covariance matrix F(θ)/n.
6. Figure 4.15 shows a Gaussian KDE with bandwidth σ = 0.2 on the points −0.5, 0,
0.2, 0.9, and 1.5. Reproduce the plot in Python. Using the same bandwidth, plot also the
KDE for the same data, but now with ϕ(z) = 1/2, z ∈[−1, 1].
-1
0
1
2
0
0.2
0.4
0.6
0.8
Figure 4.15: The Gaussian KDE (solid line) is the equally weighted mixture of normal pdfs
centered around the data and with standard deviation σ = 0.2 (dashed).
7. For fixed x′, the Gaussian kernel function
f(x | t) :=
1
√
2πt
e−1
2
(x−x′)2
t
is the solution to Fourier’s heat equation
∂
∂t f(x | t) = 1
2
∂2
∂x2 f(x | t),
x ∈R, t > 0,
with initial condition f(x | 0) = δ(x −x′) (the Dirac function at x′). Show this. As a con-
sequence, the Gaussian KDE is the solution to the same heat equation, but now with initial
condition f(x | 0) = n−1 Pn
i=1 δ(x −xi). This was the motivation for the theta KDE [14],
which is a solution to the same heat equation but now on a bounded i

nterval.
162
Exercises
8. Show that the Ward linkage given in (4.41) is equal to
dWard(I, J) =
|I| |J|
|I| + |J|∥xI −xJ∥2.
9. Carry out the agglomerative hierarchical clustering of Example 4.8 via the linkage
method from scipy.cluster.hierarchy. Show that the linkage matrices are the same.
Give a scatterplot of the data, color coded into K = 3 clusters.
10. Suppose that we have the data τn = {x1, . . . , xn} in R and decide to train the two-
component Gaussian mixture model
g(x | θ) = w1
1
q
2πσ2
1
exp
 
−(x −µ1)2
2σ2
1
!
+ w2
1
q
2πσ2
2
exp
 
−(x −µ2)2
2σ2
2
!
,
where the parameter vector θ = [µ1, µ2, σ1, σ2, w1, w2]⊤belongs to the set
Θ = {θ : w1 + w2 = 1, w1 ∈[0, 1], µi ∈R, σi > 0, ∀i}.
Suppose that the training is via the maximum likelihood in (2.28). Show that
sup
θ∈Θ
1
n
n
X
i=1
ln g(xi | θ) = ∞.
In other words, find a sequence of values for θ ∈Θ such that the likelihood grows without
bound. How can we restrict the set Θ to ensure that the likelihood remains bounded?
11. A d-dimensional normal random vector X ∼N(µ, Σ) can be defined via an affine
transformation, X = µ + Σ1/2Z, of a standard normal random vector Z ∼N(0, Id), where
Σ1/2(Σ1/2)⊤= Σ. In a similar way, we can define a d-dimensional Student random vector
X ∼tα(µ, Σ) via a transformation
X = µ +
1
√
S
Σ1/2Z,
(4.46)
where, Z ∼N(0, Id) and S ∼Gamma( α
2, α
2) are independent, α > 0, and Σ1/2(Σ1/2)⊤= Σ.
Note that we obtain the multivariate normal distribution as a limiting case for α →∞.
(a) Show that the density of the tα(0, Id) distribution is given by
tα(x) := Γ((α + d)/2)
(πα)d/2Γ(α/2)
 
1 + 1
α∥x∥2
!−α+d
2
.
By the transformation rule (C.23), it follows that the density of X ∼tα(µ, Σ) is given
☞433
by tα,Σ(x −µ), where
tα,Σ(x) :=
1
|Σ1/2|tα(Σ−1/2x).
[Hint: conditional on S = s, X has a N(0, Id/s) distribution.]
Unsupervised Learning
163
(b) We wish to fit a tν(µ, Σ) distribution to given data τ = {x1, . . . , xn} in Rd via the EM
method. We use the representation (4.46) and augment the data with the

 vector S =
[S 1, . . . , S n]⊤of hidden variables. Show that the complete-data likelihood is given by
g(τ, s | θ) =
Y
i
(α/2)α/2s(α+d)/2−1
i
exp(−si
2 α −si
2 ∥Σ−1/2(xi −µ)∥2)
Γ(α/2)(2π)d/2|Σ1/2|
.
(4.47)
(c) Show that, as a consequence, conditional on the data τ and parameter θ, the hidden
data are mutually independent, and
(S i | τ, θ) ∼Gamma
 α + d
2
, α + ∥Σ−1/2(xi −µ)∥2
2
!
,
i = 1, . . . , n.
(d) At iteration t of the EM algorithm, let g(t)(s) = g(s | τ, θ(t−1)) be the density of the
missing data, given the observed data τ and the current parameter guess θ(t−1). Verify
that the expected complete-data log-likelihood is given by:
Eg(t) ln g(τ, S | θ) = nα
2 ln α
2 −nd
2 ln(2π) −n ln Γ
α
2

−n
2 ln |Σ|
+ α + d −2
2
n
X
i=1
Eg(t) ln S i −
n
X
i=1
α + ∥Σ−1/2(xi −µ)∥2
2
Eg(t)S i.
Show that
Eg(t)S i =
α(t−1) + d
α(t−1) + ∥(Σ(t−1))−1/2(xi −µ(t−1))∥2 =: w(t−1)
i
Eg(t) ln S i = ψ
 α(t−1) + d
2
!
−ln
 α(t−1) + d
2
!
+ ln w(t−1)
i
,
where ψ := (ln Γ)′ is digamma function.
(e) Finally, show that in the M-step of the EM algorithm θ(t) is updated from θ(t−1) as
follows:
µ(t) =
Pn
i=1 w(t−1)
i
xi
Pn
i=1 w(t−1)
i
Σ(t) = 1
n
n
X
i=1
w(t−1)
i
(xi −µ(t))(xi −µ(t))⊤,
and α(t) is defined implicitly through the solution of the nonlinear equation:
ln
α
2

−ψ
α
2

+ ψ
 α(t) + d
2
!
−ln
 α(t) + d
2
!
+ 1 +
Pn
i=1

ln(w(t−1)
i
) −w(t−1)
i

n
= 0.
12. A generalization of both the gamma and inverse-gamma distribution is the generalized
inverse-gamma distribution
generalized
inverse-gamma
distribution
, which has density
f(s) = (a/b)p/2
2Kp(
√
ab)
sp−1e−1
2(as+b/s),
a, b, s > 0, p ∈R,
(4.48)
164
Exercises
where Kp is the modified Bessel function of the second kind
modified Bessel
function of the
second kind
, which can be defined as the
integral
Kp(x) =
Z ∞
0
e−x cosh(t) cosh(pt) dt,
x > 0, p ∈R.
(4.49)
We write S ∼GIG(a, b, p) to denote that S has a pdf of the form (4.48). The function Kp
has many interesting properties. Special cases include
K1/2(x) =
r
x π
2 e−x 1
x
K3/2(x) =
r


x π
2 e−x
 1
x + 1
x2
!
K5/2(x) =
r
x π
2 e−x
 1
x + 3
x2 + 3
x3
!
.
More generally, Kp satisfies the recursion
Kp+1(x) = Kp−1(x) + 2p
x Kp(x).
(4.50)
(a) Using the change of variables ez = s √a/b, show that
Z ∞
0
sp−1e−1
2(as+b/s) ds = 2Kp(
√
ab)(b/a)p/2.
(b) Let S ∼GIG(a, b, p). Show that
ES =
√
b Kp+1(
√
ab)
√a Kp(
√
ab)
(4.51)
and
ES −1 =
√a Kp+1(
√
ab)
√
b Kp(
√
ab)
−2p
b .
(4.52)
13. In Exercise 11 we viewed the multivariate Student tα distribution as a scale-mixture
scale-mixture
of the N(0, Id) distribution. In this exercise, we consider a similar transformation, but now
Σ1/2Z ∼N(0, Σ) is not divided but is multiplied by
√
S , with S ∼Gamma(α/2, α/2):
X = µ +
√
S Σ1/2 Z,
(4.53)
where S and Z are independent and α > 0.
(a) Show, using Exercise 12, that for Σ1/2 = Id and µ = 0, the random vector X has a
d-dimensional Bessel distribution
Bessel
distribution
, with density:
κα(x) := 21−(α+d)/2α(α+d)/4 ∥x∥(α−d)/2
πd/2Γ(α/2)
K(α−d)/2

∥x∥√α

,
x ∈Rd,
where Kp is the modified Bessel function of the second kind given in (4.49). We write
X ∼Besselα(0, Id). A random vector X is said to have a Besselα(µ, Σ) distribution if
Unsupervised Learning
165
it can be written in the form (4.53). By the transformation rule (C.23), its density is
given by
1
√|Σ|κα(Σ−1/2(x −µ)). Special instances of the Bessel pdf include:
κ2(x) = exp(−
√
2 |x|)
√
2
κ4(x) = 1 + 2 |x|
2
exp(−2 |x|)
κ4(x1, x2, x3) = 1
π exp

−2
q
x2
1 + x2
2 + x2
3

κd+1(x) =
((d + 1)/2)d/2 √π
(2π)d/2Γ((d + 1)/2) exp

−
√
d + 1 ∥x∥

,
x ∈Rd.
Note that k2 is the (scaled) pdf of the double-exponential or Laplace distribution.
(b) Given the data τ = {x1, . . . , xn} in Rd, we wish to fit a Bessel pdf to the data by
employing the EM algorithm, augmenting the data with the vector S = [S 1, . . . , S n]⊤
of missing data. We assume that α is known and α > d. Show that conditional on
τ (and given θ), the missing data vector S has independent components, with S i ∼
GIG(α, bi, (α −d)/2), with bi := ∥Σ−1/2(xi −µ)∥2, i = 

1, . . . , n.
(c) At iteration t of the EM algorithm, let g(t)(s) = g(s | τ, θ(t−1)) be the density of the
missing data, given the observed data τ and the current parameter guess θ(t−1). Show
that the expected complete-data log-likelihood is given by:
Q(t)(θ) := Eg(t) ln g(τ, S | θ) = −1
2
n
X
i=1
bi(θ) w(t−1)
i
+ constant,
(4.54)
where bi(θ) = ∥Σ−1/2(xi −µ)∥2 and
w(t−1)
i
:=
√α K(α−d+2)/2
p
α bi(θ(t−1))

p
bi(θ(t−1)) K(α−d)/2
p
α bi(θ(t−1))
 −
α −d
bi(θ(t−1)),
i = 1, . . . , n.
(d) From (4.54) derive the M-step of the EM algorithm. That is, show how θ(t) is updated
from θ(t−1).
14. Consider the ellipsoid E = {x ∈Rd : xΣ−1x = 1} in (4.42). Let UD2U⊤be an SVD of
Σ. Show that the linear transformation x 7→U⊤D−1x maps the points on E onto the unit
sphere {z ∈Rd : ∥z∥= 1}.
15. Figure 4.13 shows how the centered “surfboard” data are projected onto the first
column of the principal component matrix U. Suppose we project the data instead onto
the plane spanned by the first two columns of U. What are a and b in the representation
ax1 + bx2 = x3 of this plane?
16. Figure 4.14 suggests that we can assign each feature vector x in the iris data set to
one of two clusters, based on the value of u⊤
1 x, where u1 is the first principal component.
Plot the sepal lengths against petal lengths and color the points for which u⊤
1 x < 1.5 differ-
ently to points for which u⊤
1 x ⩾1.5. To which species of iris do these clusters correspond?
166
CHAPTER5
REGRESSION
Many supervised learning techniques can be gathered under the name “regression”.
The purpose of this chapter is to explain the mathematical ideas behind regression
models and their practical aspects. We analyze the fundamental linear model in detail,
and also discuss nonlinear and generalized linear models.
5.1
Introduction
Francis Galton observed in an article in 1889 that the heights of adult offspring are, on the
whole, more “average” than the heights of their parents. Galton interpreted this as a degen-
erative phenomen

on, using the term “regression” to indicate this “return to mediocrity”.
Nowadays, regression
regression
refers to a broad class of supervised learning techniques where the
aim is to predict a quantitative response (output) variable y via a function g(x) of an ex-
planatory (input) vector x = [x1, . . . , xp]⊤, consisting of p features, each of which can be
continuous or discrete. For instance, regression could be used to predict the birth weight of
a baby (the response variable) from the weight of the mother, her socio-economic status,
and her smoking habits (the explanatory variables).
Let us recapitulate the framework of supervised learning established in Chapter 2. The
☞
19
aim is to find a prediction function g that best guesses1 what the random output Y will be
for a random input vector X. The joint pdf f(x, y) of X and Y is unknown, but a training
set τ = {(x1, y1), . . . , (xn, yn)} is available, which is thought of as the outcome of a random
training set T = {(X1, Y1), . . . , (Xn, Yn)} of iid copies of (X, Y). Once we have selected a
loss function Loss(y,by), such as the squared-error loss
squared-error
loss
Loss(y,by) = (y −by)2,
(5.1)
then the “best” prediction function g is defined as the one that minimizes the risk
risk
ℓ(g) =
E Loss(Y, g(X)). We saw in Section 2.2 that for the squared-error loss this optimal predic-
tion function is the conditional expectation
g∗(x) = E[Y | X = x].
1Recall the mnemonic use of “g” for “guess”
167
168
Introduction
As the squared-error loss is the most widely-used loss function for regression, we will
adopt this loss function in most of this chapter.
The optimal prediction function g∗has to be learned from the training set τ by minim-
izing the training loss
ℓτ(g) = 1
n
n
X
i=1
(yi −g(xi))2
(5.2)
over a suitable class of functions G. Note that in the above definition, the training set τ is
assumed to be fixed. For a random training set T , we will write the training loss as ℓT(g).
The function gG
τ that minimizes the tra

ining loss is the function we use for prediction —
the so-called learner
learner
. When the function class G is clear from the context, we drop the
superscript in the notation.
As we already saw in (2.2), conditional on X = x, the response Y can be written as
☞21
Y = g∗(x) + ε(x),
where E ε(x) = 0. This motivates a standard modeling assumption in supervised learn-
ing, in which the responses Y1, . . . , Yn, conditional on the explanatory variables X1 =
x1, . . . , Xn = xn, are assumed to be of the form
Yi = g(xi) + εi,
i = 1, . . . , n,
where the {εi} are independent with E εi = 0 and Var εi = σ2 for some function g ∈G and
variance σ2. The above model is usually further specified by assuming that g is completely
known up to an unknown parameter vector; that is,
Yi = g(xi | β) + εi,
i = 1, . . . , n.
(5.3)
While the model (5.3) is described conditional on the explanatory variables, it will be
convenient to make one further model simplification, and view (5.3) as if the {xi} were
fixed, while the {Yi} are random.
For the remainder of this chapter, we assume that the training feature vectors {xi} are
fixed and only the responses are random; that is, T = {(x1, Y1), . . . , (xn, Yn)}.
The advantage of the model (5.3) is that the problem of estimating the function g from
the training data is reduced to the (much simpler) problem of estimating the parameter
vector β. An obvious disadvantage is that functions of the form g(· | β) may not accurately
approximate the true unknown g∗. The remainder of this chapter deals with the analysis
of models of the form (5.3). In the important case where the function g(· | β) is linear, the
analysis proceeds through the class of linear models. If, in addition, the error terms {εi} are
assumed to be Gaussian, this analysis can be carried out using the rich theory of normal
linear models.
Regression
169
5.2
Linear Regression
The most basic regression model involves a linear relationship between the response and a
single explanatory variable

. In particular, we have measurements (x1, y1), . . . , (xn, yn) that
lie approximately on a straight line, as in Figure 5.1.
-3
-2
-1
0
1
2
3
-5
0
5
10
15
Figure 5.1: Data from a simple linear regression model.
Following the general scheme captured in (5.3), a simple model for these data is that
the {xi} are fixed and variables {Yi} are random such that
Yi = β0 + β1 xi + εi,
i = 1, . . . , n,
(5.4)
for certain unknown parameters β0 and β1. The {εi} are assumed to be independent with
expectation 0 and unknown variance σ2. The unknown line
y = β0 + β1 x
|    {z    }
g(x | β)
(5.5)
is called the regression line
regression line
. Thus, we view the responses as random variables that would
lie exactly on the regression line, were it not for some “disturbance” or “error” term repres-
ented by the {εi}. The extent of the disturbance is modeled by the parameter σ2. The model
in (5.4) is called simple linear regression
simple linear
regression
model
. This model can easily be extended to incorporate
more than one explanatory variable, as follows.
Definition 5.1: Multiple Linear Regression Model
In a multiple linear regression model
multiple linear
regression
model
the response Y depends on a d-dimensional
explanatory vector x = [x1, . . . , xd]⊤, via the linear relationship
Y = β0 + β1 x1 + · · · + βd xd + ε,
(5.6)
where E ε = 0 and Var ε = σ2.
170
Linear Regression
Thus, the data lie approximately on a d-dimensional affine hyperplane
y = β0 + β1x1 + · · · + βdxd
|                      {z                      }
g(x | β)
,
where we define β = [β0, β1, . . . , βd]⊤. The function g(x | β) is linear in β, but not linear in
the feature vector x, due to the constant β0. However, augmenting the feature space with
the constant 1, the mapping [1, x⊤]⊤7→g(x | β) := [1, x⊤] β becomes linear in the feature
space and so (5.6) becomes a linear model (see Section 2.1). Most software packages for
☞44
regression include 1 as a feature by default.
Note that in (5.6) we only specified the mode

l for a single pair (x, Y). The model for the
training set T = {(x1, Y1), . . . , (xn, Yn)} is simply that each Yi satisfies (5.6) (with x = xi)
and that the {Yi} are independent. Setting Y = [Y1, . . . , Yn]⊤, we can write the multiple
linear regression model for the training data compactly as
Y = Xβ + ε,
(5.7)
where ε = [ε1, . . . , εn]⊤is a vector of iid copies of ε and X is the model matrix
model matrix
given by
X =

1
x11
x12
· · ·
x1d
1
x21
x22
· · ·
x2d
...
...
...
...
...
1
xn1
xn2
· · ·
xnd

=

1
x⊤
1
1
x⊤
2
...
...
1
x⊤
n

.
Example 5.1 (Multiple Linear Regression Model) Figure 5.2 depicts a realization of
the multiple linear regression model
Yi = xi1 + xi2 + εi,
i = 1, . . . , 100,
where ε1, . . . , ε100 ∼iid N(0, 1/16). The fixed feature vectors (vectors of explanatory vari-
ables) xi = [xi1, xi2]⊤, i = 1, . . . , 100 lie in the unit square.
1
0
0
1
2
1 0
Figure 5.2: Data from a multiple linear regression model.
Regression
171
5.3
Analysis via Linear Models
Analysis of data from a linear regression model is greatly simplified through the linear
model representation (5.7). In this section we present the main ideas for parameter estima-
tion and model selection for a general linear model of the form
Y = Xβ + ε,
(5.8)
where X is an n×p matrix, β = [β1, . . . , βp]⊤a vector of p parameters, and ε = [ε1, . . . , εn]⊤
an n-dimensional vector of independent error terms, with E εi = 0 and Var εi = σ2, i =
1, . . . , n. Note that the model matrix X is assumed to be fixed, and Y and ε are random. A
specific outcome of Y is denoted by y (in accordance with the notation in Section 2.8).
☞
47
Note that the multiple linear regression model in (5.7) was defined using a different
parameterization; in particular, there we used β = [β0, β1, . . . , βd]⊤. So, when apply-
ing the results in the present section to such models, be aware that p = d + 1. Also,
in this section a feature vector x includes the con

stant 1, so that X⊤= [x1, . . . , xn].
5.3.1
Parameter Estimation
The linear model Y = Xβ + ε contains two unknown parameters, β and σ2, which have
to be estimated from the training data τ. To estimate β, we can repeat exactly the same
reasoning used in our recurring polynomial regression Example 2.1 as follows. For a linear
☞
26
prediction function g(x) = x⊤β, the (squared-error) training loss can be written as
ℓτ(g) = 1
n ∥y −Xβ∥2,
and the optimal learner gτ minimizes this quantity, leading to the least-squares estimate bβ,
which satisfies the normal equations
X⊤X β = X⊤y.
(5.9)
The corresponding training loss can be taken as an estimate of σ2; that is,
c
σ2 = 1
n ∥y −Xbβ ∥2.
(5.10)
To justify the latter, note that σ2 is the second moment of the model errors εi, i = 1, . . . , n,
in (5.8) and could be estimated via the method of moments (see Section C.12.1) using the
☞455
sample average n−1 P
i ε2
i = ∥ε∥2/n = ∥Y −Xβ∥2/n, if β were known. By replacing β with
its estimator, we arrive at (5.10). Note that no distributional properties of the {εi} were used
other than E εi = 0 and Var εi = σ2, i = 1, . . . , n. The vector e := y −Xbβ is called the
vector of residuals
residuals
and approximates the (unknown) vector of model errors ε. The quantity
∥e∥2 = Pn
i=1 e2
i is called the residual sum of squares (RSS). Dividing the RSS by n−p gives
residual sum of
squares
an unbiased estimate of σ2, which we call the estimated residual squared error (RSE); see
residual
squared error
Exercise 12.
172
Analysis via Linear Models
In terms of the notation given in the summary Table 2.1 for supervised learning, we
thus have:
☞25
1. The (observed) training data is τ = {X, y}.
2. The function class G is the class of linear functions of x; that is G = {g(· | β) : x 7→
x⊤β, β ∈Rp}.
3. The (squared-error) training loss is ℓτ(g(· | β)) = ∥y −Xβ∥2/n.
4. The learner gτ is given by gτ(x) = x⊤bβ, where bβ = argminβ∈Rp ∥y −Xβ∥2.
5. The minimal training loss is ℓτ(gτ) = ∥y −Xbβ∥2/n = c
σ2.
5.3.2


Model Selection and Prediction
Even if we restrict the learner to be a linear function, there is still the issue of which explan-
atory variables (features) to include. While including too few features may result in large
approximation error (underfitting), including too many may result in large statistical error
(overfitting). As discussed in Section 2.4, we need to select the features which provide the
☞31
best tradeoff between the approximation and statistical errors, so that the (expected) gener-
alization risk of the learner is minimized. Depending on how the (expected) generalization
risk is estimated, there are a number of strategies for feature selection:
1. Use test data τ′ = (X′, y′) that are obtained independently from the training data τ,
to estimate the generalization risk E ∥Y −gτ(X)∥2 via the test loss (2.7). Then choose
☞24
the collection of features that minimizes the test loss. When there is an abundance of
data, part of the data can be reserved as test data, while the remaining data is used as
training data.
2. When there is a limited amount of data, we can use cross-validation to estimate the
expected generalization risk E ∥Y −gT(X)∥2 (where T is a random training set), as
explained in Section 2.5.2. This is then minimized over the set of possible choices
☞38
for the explanatory variables.
3. When one has to choose between many potential explanatory variables, techniques
such as regularized least-squares and lasso regression become important. Such
methods offer another approach to model selection, via the regularization (or ho-
motopy) paths. This will be the topic of Section 6.2 in the next chapter.
☞216
4. Rather than using computer-intensive techniques, such as the ones above, one can
use theoretical estimates of the expected generalization risk, such as the in-sample
risk, AIC, and BIC, as in Section 2.5, and minimize this to determine a good set of
☞35
explanatory variables.
5. All of the above approaches do not assume any distributional pro

perties of the error
terms {εi} in the linear model, other than that they are independent with expectation
0 and variance σ2. If, however, they are assumed to have a normal (Gaussian) distri-
bution, (that is, {εi} ∼iid N(0, σ2)), then the inclusion and exclusion of variables can
Regression
173
be decided by means of hypotheses tests. This is the classical approach to model
selection, and will be discussed in Section 5.4. As a consequence of the central limit
theorem, one can use the same approach when the error terms are not necessarily
normal, provided their variance is finite and the sample size n is large.
6. Finally, when using a Bayesian approach, comparison of two models can be achieved
by computing their so-called Bayes factor (see Section 2.9).
All of the above strategies can be thought of as specifications of a simple rule formu-
lated by William of Occam, which can be interpreted as:
When presented with competing models, choose the simplest one that explains
the data.
This age-old principle, known as Occam’s razor
Occam’s razor
, is mirrored in a famous quote of Einstein:
Everything should be made as simple as possible, but not simpler.
In linear regression, the number of parameters or predictors is usually a reasonable measure
of the simplicity of the model.
5.3.3
Cross-Validation and Predictive Residual Sum of Squares
We start by considering the n-fold cross-validation, also called leave-one-out cross-
validation
leave-one-out
cross-validation
, for the linear model (5.8). We partition the data into n data sets, leaving out
precisely one observation per data set, which we then predict based on the n −1 remaining
observations; see Section 2.5.2 for the general case. Let by−i denote the prediction for the
☞
38
i-th observation using all the data except yi. The error in the prediction, yi −by−i, is called a
predicted residual
predicted
residual
— in contrast to an ordinary residual, ei = yi −byi, which is the difference
between an observation and its fitte

d valuebyi = gτ(xi) obtained using the whole sample. In
this way, we obtain the collection of predicted residuals {yi −by−i}n
i=1 and summarize them
through the predicted residual sum of squares (PRESS
PRESS
):
PRESS =
n
X
i=1
(yi −by−i)2.
Dividing the PRESS by n gives an estimate of the expected generalization risk.
In general, computing the PRESS is computationally intensive as it involves training
and predicting n separate times. For linear models, however, the predicted residuals can be
☞171
calculated quickly using only the ordinary residuals and the projection matrix P = XX+
onto the linear space spanned by the columns of the model matrix X (see (2.13)). The i-th
☞
28
diagonal element Pii of the projection matrix is called the i-th leverage
leverage
, and it can be shown
that 0 ⩽Pii ⩽1 (see Exercise 10).
174
Analysis via Linear Models
Theorem 5.1: PRESS for Linear Models
Consider the linear model (5.8), where the n×p model matrix X is of full rank. Given
an outcome y = [y1, . . . , yn]⊤of Y, the fitted values can be obtained asby = Py, where
P = XX+ = X(X⊤X)−1X⊤is the projection matrix. If the leverage value pi := Pii , 1
for all i = 1, . . . , n, then the predicted residual sum of squares can be written as
PRESS =
n
X
i=1
 
ei
1 −pi
!2
,
where ei = yi −byi = yi −(Xbβ)i is the i-th residual.
Proof: It suffices to show that the i-th predicted residual can be written as yi −by−i =
ei/(1 −pi). Let X−i denote the model matrix X with the i-th row, x⊤
i , removed, and define
y−i similarly. Then, the least-squares estimate for β using all but the i-th observation is
bβ−i = (X⊤
−iX−i)−1X⊤
−iy−i. Writing X⊤X = X⊤
−iX−i + xix⊤
i , we have by the Sherman–Morrison
formula
☞371
(X⊤
−iX−i)−1 = (X⊤X)−1 + (X⊤X)−1xix⊤
i (X⊤X)−1
1 −x⊤
i (X⊤X)−1xi
,
where x⊤
i (X⊤X)−1xi = pi < 1. Also, X⊤
−iy−i = X⊤y −xiyi. Combining all these identities,
we have
bβ−i = (X⊤
−iX−i)−1X⊤
−iy−i
=
 
(X⊤X)−1 + (X⊤X)−1xix⊤
i (X⊤X)−1
1 −pi
!
(X⊤y −xiyi)
= bβ + (X⊤X)−1xix⊤
i bβ
1 −pi
−(X⊤X)−1xiyi −(X⊤X)−

1xipiyi
1 −pi
= bβ + (X⊤X)−1xix⊤
i bβ
1 −pi
−(X⊤X)−1xiyi
1 −pi
= bβ −(X⊤X)−1xi(yi −x⊤
i bβ)
1 −pi
= bβ −(X⊤X)−1xiei
1 −pi
.
It follows that the predicted value for the i-th observation is given by
by−i = x⊤
i bβ−i = x⊤
i bβ −x⊤
i (X⊤X)−1xiei
1 −pi
= byi −
piei
1 −pi
.
Hence, yi −by−i = ei + piei/(1 −pi) = ei/(1 −pi).
□
Example 5.2 (Polynomial Regression (cont.)) We return to Example 2.1, where we
☞26
estimated the generalization risk for various polynomial prediction functions using inde-
pendent validation data. Instead, let us estimate the expected generalization risk via cross-
validation (thus using only the training set) and apply Theorem 5.1 to compute the PRESS.
☞174
Regression
175
polyregpress.py
import numpy as np
import matplotlib.pyplot as plt
def generate_data(beta , sig, n):
u = np.random.rand(n, 1)
y = u ** np.arange(0, 4) @ beta.reshape(4,1) + (
sig * np.random.randn(n, 1))
return u, y
np.random.seed(12)
beta = np.array([[10.0, -140, 400, -250]]).T;
sig=5; n = 10**2;
u,y = generate_data(beta,sig,n)
X = np.ones((n, 1))
K = 12 #maximum number of parameters
press = np.zeros(K+1)
for k in range(1,K):
if k > 1:
X = np.hstack((X, u**(k-1))) # add column to matrix
P = X @ np.linalg.pinv(X) # projection matrix
e = y - P @ y
press[k] = np.sum((e/(1-np.diag(P).reshape(n,1)))**2)
plt.plot(press[1:K]/n)
The PRESS values divided by n = 100 for the constant, linear, quadratic, cubic, and
quartic order polynomial regression models are, respectively, 152.487, 56.249, 51.606,
30.999, and 31.634. Hence, the cubic polynomial regression model has the lowest PRESS,
indicating that it has the best predictive performance.
5.3.4
In-Sample Risk and Akaike Information Criterion
In Section 2.5.1 we introduced the in-sample risk as a measure for the accuracy of the
☞
35
prediction function. To recapitulate, given a fixed data set τ with associated response vector
y and n × p matrix of explanatory variables X, the in-sample risk of a prediction function
g is defined as
ℓin(g) := 

EX Loss(Y, g(X)),
(5.11)
where EX signifies that the expectation is taken under a different probability model, in
which X takes the values x1, . . . , xn with equal probability, and given X = xi the random
variable Y is drawn from the conditional pdf f(y | xi). The difference between the in-sample
risk and the training loss is called the optimism. For the squared-error loss, Theorem 2.2 ex-
☞
36
presses the expected optimism of a learner gT as two times the average covariance between
the predicted values and the responses.
If the conditional variance of the error Y −g∗(X) given X = x does not depend on x,
then the expected in-sample risk of a learner gτ, averaged over all training sets, has a simple
expression:
176
Analysis via Linear Models
Theorem 5.2: Expected In-Sample Risk for Linear Models
Let X be the model matrix for a linear model, of dimension n × p. If Var[Y −
g∗(X) | X = x] =: v2 does not depend on x, then the expected in-sample risk (with
respect to the squared-error loss) for a random learner gT is given by
EX ℓin(gT) = EX ℓT(gT) + 2ℓ∗p
n ,
(5.12)
where ℓ∗is the irreducible risk.
Proof:
The expected optimism is, by definition, EX[ℓin(gT) −ℓT(gT)] which, for the
squared-error loss, is equal to 2ℓ∗p/n, using exactly the same reasoning as in Example 2.3.
Note that here ℓ∗= v2.
□
Equation (5.12) is the basis of the following model comparison heuristic: Estimate the
irreducible risk ℓ∗= v2 via bv2, using a model with relatively high complexity. Then choose
the linear model with the lowest value of
∥y −Xbβ∥2 + 2 bv2p.
(5.13)
We can also use the Akaike information criterion (AIC) as a heuristic for model com-
parison. We discussed the AIC in the unsupervised learning setting in Section 4.2, but the
☞122
arguments used there can also be applied to the supervised case, under the in-sample model
for the data. In particular, let Z = (X, Y). We wish to predict the joint density
f(z) = f(x, y) := 1
n
n
X
i=1
1{x=xi} f(y | xi),
using a prediction function g(z | θ) f

rom a family G := {g(z | θ), θ ∈Rq}, where
g(z | θ) = g(x, y | θ) := 1
n
n
X
i=1
1{x=xi} gi(y | θ).
Note that q is the number of parameters (typically larger than p for a linear model with a
n × p design matrix).
Following Section 4.2, the in-sample cross-entropy risk in this case is
r(θ) := −EX ln g(Z | θ),
and to approximate the optimal parameter θ∗we minimize the corresponding training loss
rτn(θ) := −1
n
n
X
j=1
ln g(zj | θ).
The optimal parameterbθn for the training loss is thus found by minimizing
−1
n
n
X
j=1

−ln n + ln gj(y j | θ)

.
Regression
177
That is, it is the maximum likelihood estimate of θ:
bθn = argmax
θ
n
X
i=1
ln gi(yi | θ).
Under the assumption that f = g(· | θ∗) for some parameter θ∗, we have from Theorem 4.1
that the estimated in-sample generalization risk can be approximated as
☞125
EX r(bθn) ≈rTn(bθn) + q
n = ln n −1
n
n
X
j=1
ln gj(yj |bθn) + q
n.
This leads to the heuristic of selecting the learner g(· |bθn) with the smallest value of the
AIC:
−2
n
X
i=1
ln gi(yi |bθn) + 2q.
(5.14)
Example 5.3 (Normal Linear Model) For the normal linear model Y ∼N(x⊤β, σ2)
(see (2.34)), with a p-dimensional vector β, we have
☞
47
gi(yi | β, σ2
|{z}
= θ
) =
1
√
2πσ2 exp
 
−1
2
(yi −x⊤
i β)2
σ2
!
,
i = 1, . . . , n,
so that the AIC is
n ln(2π) + n ln bσ2 + ∥y −Xbβ∥2
bσ2
+ 2q,
(5.15)
where (bβ, bσ2) is the maximum likelihood estimate and q = p+1 is the number of parameters
(including σ2). For model comparison we may remove the n ln(2π) term if all the models
are normal linear models.
Certain software packages report the AIC without the n ln bσ2 term in (5.15). This
may lead to sub-optimal model selection if normal models are compared with non-
normal ones.
5.3.5
Categorical Features
Suppose that, as described in Chapter 1, the data is given in the form of a spreadsheet or
data frame with n rows and p + 1 columns, where the first element of row i is the response
variable yi, and the remaining p elements form the vector of explanatory variables x⊤
i .
When 

all the explanatory variables (features, predictors) are quantitative, then the model
matrix X can be directly read off from the data frame as the n × p matrix with rows x⊤
i , i =
1, . . . , n.
However, when some explanatory variables are qualitative (categorical), such a one-to-
one correspondence between data frame and model matrix no longer holds. The solution is
to include indicator or dummy variables.
Linear models with continuous responses and categorical explanatory variables often
arise in factorial experiments. These are controlled statistical experiments in which the
factorial
experiments
178
Analysis via Linear Models
aim is to assess how a response variable is affected by one or more factors tested at several
factors
levels. A typical example is an agricultural experiment where one wishes to investigate
levels
how the yield of a food crop depends on factors such as location, pesticide, and fertilizer.
Example 5.4 (Crop Yield) The data in Table 5.1 lists the yield of a food crop for four
different crop treatments (e.g., strengths of fertilizer) on four different blocks (plots).
Table 5.1: Crop yield for different treatments and blocks.
Treatment
Block
1
2
3
4
1
9.2988
9.4978
9.7604
10.1025
2
8.2111
8.3387
8.5018
8.1942
3
9.0688
9.1284
9.3484
9.5086
4
8.2552
7.8999
8.4859
8.9485
The corresponding data frame, given in Table 5.2, has 16 rows and 3 columns: one
column for the crop yield (the response variable), one column for the Treatment, with
levels 1, 2, 3, 4, and one column for the Block, also with levels 1, 2, 3, 4. The values 1,
2, 3, and 4 have no quantitative meaning (it does not make sense to take their average, for
example) — they merely identify the category of the treatment or block.
Table 5.2: Crop yield data organized as a data frame in standard format.
Yield
Treatment
Block
9.2988
1
1
8.2111
1
2
9.0688
1
3
8.2552
1
4
9.4978
2
1
8.3387
2
2
...
...
...
9.5086
4
3
8.9485
4
4
In general, suppose there are r factor (categorical) variables u1, . . 

. , ur, where the j-
th factor has pj mutually exclusive levels, denoted by 1, . . . , pj. In order to include these
categorical variables in a linear model, a common approach is to introduce an indicator
feature
indicator
feature
xjk = 1{uj = k} for each factor j at level k. Thus, x jk = 1 if the value of factor j
is k and 0 otherwise. Since P
k 1{uj = k} = 1, it suffices to consider only pj −1 of these
indicator features for each factor j (this prevents the model matrix from being rank defi-
cient). For a single response Y, the feature vector x⊤is thus a row vector of binary variables
Regression
179
that indicates which levels were observed for each factor. The model assumption is that Y
depends in a linear way on the indicator features, apart from an error term. That is,
Y = β0 +
rX
j=1
p j
X
k=2
βjk 1{uj = k}
|     {z     }
xjk
+ ε,
where we have omitted one indicator feature (corresponding to level 1) for each factor
j. For independent responses Y1, . . . , Yn, where each Yi corresponds to the factor values
ui1, . . . , uir, let xijk = 1{uij = k}. Then, the linear model for the data becomes
Yi = β0 +
rX
j=1
p j
X
k=2
β jkxi jk + εi,
(5.16)
where the {εi} are independent with expectation 0 and some variance σ2. By gathering the
β0 and {β jk} into a vector β, and the {xi jk} into a matrix X, we have again a linear model of
the form (5.8). The model matrix X has n rows and 1 + Pr
j=1(pj −1) columns. Using the
above convention that the β j1 parameters are subsumed in the parameter β0 (correspond-
ing to the “constant” feature), we can interpret β0 as a baseline response when using the
explanatory vector x⊤for which x j1 = 1 for all factors j = 1, . . . , r. The other parameters
{β jk} can be viewed as incremental effects
incremental
effects
relative to this baseline effect. For example, β12
describes by how much the response is expected to change if level 2 is used instead of level
1 for factor 1.
Example 5.5 (Crop Yield (cont.)) In Example 5.4, the linear model (5

.16) has eight
parameters: β0, β12, β13, β14, β22, β23, β24, and σ2. The model matrix X depends on how
the crop yields are organized in a vector y and on the ordering of the factors. Let
us order y column-wise from Table 5.1, as in y = [9.2988, 8.2111, 9.0688, 8.2552,
9.4978, . . . , 8.9485]⊤, and let Treatment be Factor 1 and Block be Factor 2. Then we can
write (5.16) as
Y =

1
0
0
0
C
1
1
0
0
C
1
0
1
0
C
1
0
0
1
C

|               {z               }
X

β0
β12
β13
β14
β22
β23
β24

|{z}
β
+ ε,
where
C =

0
0
0
1
0
0
0
1
0
0
0
1

,
and with 1 = [1, 1, 1, 1]⊤and 0 = [0, 0, 0, 0]⊤. Estimation of β and σ2, model selection,
and prediction can now be carried out in the usual manner for linear models.
In the context of factorial experiments, the model matrix is often called the design
matrix
design matrix
, as it specifies the design of the experiment; e.g., how many replications are taken
for each combination of factor levels. The model (5.16) can be extended by adding products
of indicator variables as new features. Such features are called interaction
interaction
terms.
180
Analysis via Linear Models
5.3.6
Nested Models
Let X be a n × p model matrix of the form X = [X1, X2], where X1 and X2 are model
matrices of dimension n × k and n × (p −k), respectively. The linear models Y = X1β1 + ε
and Y = X2β2 + ε are said to be
nested models
nested within the linear model Y = Xβ + ε. This simply
means that certain features in X are ignored in each of the first two models. Note that β, β1,
and β2 are parameter vectors of dimension p, k, and p −k, respectively. In what follows,
we assume that n ⩾p and that all model matrices are full-rank.
Suppose we wish to assess whether to use the full model matrix X or the reduced model
matrix X1. Let bβ be the estimate of β under the full model (that is, obtained via (5.9)), and
let b
β1 denote the estimate of β1 for the 

reduced model. Let Y(2) = Xbβ be the projection of Y
onto the space Span(X) spanned by the columns of X; and let Y(1) = X1 b
β1 be the projection
of Y onto the space Span(X1) spanned by the columns of X1 only; see Figure 5.3. In order
to decide whether the features in X2 are needed, we may compare the estimated error terms
of the two models, as calculated by (5.10); that is, by the residual sum of squares divided
by the number of observations n. If the outcome of this comparison is that there is little
difference between the model error for the full and reduced model, then it is appropriate to
adopt the reduced model, as it has fewer parameters than the full model, while explaining
the data just as well. The comparison is thus between the squared norms ∥Y −Y(2)∥2 and
∥Y −Y(1)∥2. Because of the nested nature of the linear models, Span(X1) is a subspace of
Span(X) and, consequently, the orthogonal projection of Y(2) onto Span(X1) is the same
as the orthogonal projection of Y onto Span(X1); that is, Y(1). By Pythagoras’ theorem, we
thus have the decomposition ∥Y(2)−Y(1)∥2+∥Y −Y(2)∥2 = ∥Y −Y(1)∥2. This is also illustrated
in Figure 5.3.
Y
Y −Y(1)
Y −Y(2)
Y(2)
O
Span(X)
Span(X1)
Y(2) −Y(1)
Y(1)
Figure 5.3: The residual sum of squares for the full model corresponds to ∥Y−Y(2)∥2 and for
the reduced model it is ∥Y−Y(1)∥2. By Pythagoras’s theorem, the difference is ∥Y(2)−Y(1)∥2.
The above decomposition can be generalized to more than two model matrices. Sup-
pose that the model matrix can be decomposed into d submatrices: X = [X1, X2, . . . , Xd],
where the matrix Xi has pi columns and n rows, i = 1, . . . , d. Thus, the number of columns2
2As always, we assume the columns are linearly independent.
Regression
181
in the full model matrix is p = p1+· · ·+pd. This creates an increasing sequence of “nested”
model matrices: X1, [X1, X2], . . . , [X1, X2, . . . , Xd], from (say) the baseline normal model
matrix X1 = 1 to the full model matrix X. Think of each model matrix corresp

onding to
specific variables in the model.
We follow a similar projection procedure as in Figure 5.3: First project Y onto Span(X)
to yield the vector Y(d), then project Y(d) onto Span([X1, . . . , Xd−1]) to obtain Y(d−1), and so
on, until Y(2) is projected onto Span(X1) to yield Y(1) = Y1 (in the case that X1 = 1).
By applying Pythagoras’ theorem, the total sum of squares can be decomposed as
∥Y −Y(1)∥2
|       {z       }
df=n−p1
= ∥Y −Y(d)∥2
|       {z       }
df=n−p
+ ∥Y(d) −Y(d−1)∥2
|            {z            }
df=pd
+ · · · + ∥Y(2) −Y(1)∥2
|          {z          }
df=p2
.
(5.17)
Software packages typically report the sums of squares as well as the corresponding de-
grees of freedom (df): n −p, pd, . . . , p2.
degrees of
freedom
5.3.7
Coefficient of Determination
To assess how a linear model Y = Xβ + ε compares to the default model Y = β01 + ε, we
can compare the variance of the original data, estimated via P
i(Yi −Y)2/n = ∥Y −Y1∥2/n,
with the variance of the fitted data; estimated via P
i(bYi −Y)2/n = ∥bY −Y1∥2/n, where
bY = Xbβ. The sum P
i(Yi −Y)2/n = ∥Y −Y1∥2 is sometimes called the total sum of squares
total sum of
squares
(TSS), and the quantity
R2 = ∥bY −Y1∥2
∥Y −Y1∥2
(5.18)
is called the coefficient of determination
coefficient of
determination
of the linear model. In the notation of Figure 5.3,
bY = Y(2) and Y1 = Y(1), so that
R2 = ∥Y(2) −Y(1)∥2
∥Y −Y(1)∥2
= ∥Y −Y(1)∥2 −∥Y −Y(2)∥2
∥Y −Y(1)∥2
= TSS −RSS
TSS
.
Note that R2 lies between 0 and 1. An R2 value close to 1 indicates that a large propor-
tion of the variance in the data has been explained by the model.
Many software packages also give the adjusted coefficient of determination
adjusted
coefficient of
determination
, or simply
the adjusted R2, defined by
R2
adjusted = 1 −(1 −R2)n −1
n −p.
The regular R2 is always non-decreasing in the number of parameters (see Exercise 15),
but this may not indicate better predictive power. The adjusted R2 compensates for this
increase by decreasing the regular R2

 as the number of variables increases. This heuristic
adjustment can make it easier to compare the quality of two competing models.
182
Inference for Normal Linear Models
5.4
Inference for Normal Linear Models
So far we have not assumed any distribution for the random vector of errors ε =
[ε1, . . . , εn]⊤in a linear model Y = Xβ + ε. When the error terms {εi} are assumed to be
normally distributed (that is, {εi} ∼iid N(0, σ2)), whole new avenues open up for inference
on linear models. In Section 2.8 we already saw that for such normal linear models, estim-
☞47
ation of β and σ2 can be carried out via maximum likelihood methods, yielding the same
estimators from (5.9) and (5.10).
The following theorem lists the properties of these estimators. In particular, it shows
that bβ and c
σ2n/(n −p) are independent and unbiased estimators of β and σ2, respectively.
Theorem 5.3: Properties of the Estimators for a Normal Linear Model
Consider the linear model Y = Xβ + ε, with ε ∼N(0, σ2In), where β is a p-
dimensional vector of parameters and σ2 a dispersion parameter. The following res-
ults hold.
1. The maximum likelihood estimators bβ and c
σ2 are independent.
2. bβ ∼N(β, σ2(X⊤X)+).
3. n c
σ2/σ2 ∼χ2
n−p, where p = rank(X).
Proof: Using the pseudo-inverse (Definition A.2), we can write the random vector bβ as
☞360
X+Y, which is a linear transformation of a normal random vector. Consequently, bβ has a
multivariate normal distribution; see Theorem C.6. The mean vector and covariance matrix
☞435
follow from the same theorem:
Ebβ = X+ EY = X+X β = β
and
Cov(bβ) = X+σ2In(X+)⊤= σ2(X⊤X)+.
To show that bβ and c
σ2 are independent, define Y(2) = Xbβ. Note that Y/σ has a N(µ, In)
distribution, with expectation vector µ = Xβ/σ. A direct application of Theorem C.10
now shows that (Y −Y(2))/σ is independent of Y(2)/σ. Since bβ = X+Xbβ = X+Y(2) and
☞438
c
σ2 = ∥Y −Y(2)∥2/n, it follows that c
σ2 is independent of bβ. Finally, by the same theorem,
the random variable ∥Y −Y(2)∥2/σ2 has a χ2


n−p distribution, as Y(2) has the same expectation
vector as Y.
□
As a corollary, we see that each estimatorbβi of βi has a normal distribution with expect-
ation βi and variance σ2u⊤
i X+(X+)⊤ui = σ2∥u⊤
i X+∥2, where ui = [0, . . . , 0, 1, 0, . . . , 0]⊤is
the i-th unit vector; in other words, the variance is σ2[(X⊤X)+]ii.
It is of interest to test whether certain regression parameters βi are 0 or not, since if
βi = 0, the i-th explanatory variable has no direct effect on the expected response and so
could be removed from the model. A standard procedure is to conduct a hypothesis test
(see Section C.14 for a review of hypothesis testing) to test the null hypothesis H0 : βi = 0
☞458
Regression
183
against the alternative H1 : βi , 0, using the test statistic
T =
bβi/∥u⊤
i X+∥
√
RSE
,
(5.19)
where RSE is the residual squared error; that is RSE = RSS/(n −p). This test statistic has
a tn−p distribution under H0. To see this, write T = Z/
p
V/(n −p), with
Z =
bβi
σ∥u⊤
i X+∥
and
V = n c
σ2/σ2.
Then, by Theorem 5.3, Z ∼N(0, 1) under H0, V ∼χ2
n−p, and Z and V are independent. The
result now follows directly from Corollary C.1.
☞439
5.4.1
Comparing Two Normal Linear Models
Suppose we have the following normal linear model for data Y = [Y1, . . . , Yn]⊤:
Y = X1β1 + X2β2
|          {z          }
Xβ
+ε,
ε ∼N(0, σ2In),
(5.20)
where β1 and β2 are unknown vectors of dimension k and p −k, respectively; and X1
and X2 are full-rank model matrices of dimensions n × k and n × (p −k), respectively.
Above we implicitly defined X = [X1, X2] and β⊤= [β⊤
1 , β⊤
2 ]. Suppose we wish to test the
hypothesis H0 : β2 = 0 against H1 : β2 , 0. Following Section 5.3.6, the idea is to compare
the residual sum of squares for both models, expressed as ∥Y −Y(2)∥2 and ∥Y −Y(1)∥2. Using
Pythagoras’ theorem we saw that ∥Y −Y(2)∥2 −∥Y −Y(1)∥2 = ∥Y(2) −Y(1)∥2, and so it makes
sense to base the decision whether to retain or reject H0 on the basis of the quotient of
∥Y(2) −Y(1)∥2 and ∥Y −Y(2)∥2. This leads 

to the following test statistics.
Theorem 5.4: Test Statistic for Comparing Two Normal Linear Models
For the model (5.20), let Y(2) and Y(1) be the projections of Y onto the space spanned
by the p columns of X and the k columns of X1, respectively. Then under H0 : β2 = 0
the test statistic
T = ∥Y(2) −Y(1)∥2/(p −k)
∥Y −Y(2)∥2/(n −p)
(5.21)
has an F(p −k, n −p) distribution.
Proof: Define X := Y/σ with expectation µ := Xβ/σ, and X j := Y(j)/σ with expectation
µj, j = k, p. Note that µp = µ and, under H0, µk = µp. We can directly apply Theorem C.10
to find that ∥Y −Y(2)∥2/σ2 = ∥X −Xp∥2 ∼χ2
n−p and, under H0, ∥Y(2) −Y(1)∥2/σ2 = ∥Xp −
☞438
Xk∥2 ∼χ2
p−k. Moreover, these random variables are independent of each other. The proof
is completed by applying Theorem C.11.
□
184
Inference for Normal Linear Models
Note that H0 is rejected for large values of T. The testing procedure thus proceeds as
follows:
1. Compute the outcome, t say, of the test statistic T in (5.21).
2. Evaluate the P-value P(T ⩾t), with T ∼F(p −k, n −p).
3. Reject H0 if this P-value is too small, say less than 0.05.
For nested models [X1, X2, . . . , Xi], i = 1, 2, . . . , d, as in Section 5.3.6, the F test statistic
in Theorem 5.4 can now be used to test whether certain Xi are needed or not. In particular,
☞183
software packages will report the outcomes of
Fi = ∥Y(i) −Y(i−1)∥2/pi
∥Y −Y(d)∥2/(n −p)
,
(5.22)
in the order i = 2, 3, . . . , d. Under the null hypothesis that Y(i) and Y(i−1) have the same ex-
pectation (that is, adding Xi to Xi−1 has no additional effect on reducing the approximation
error), the test statistic Fi has an F(pi, n −p) distribution, and the corresponding P-values
quantify the strength of the decision to include an additional variable in the model or not.
This procedure is called analysis of variance (ANOVA).
analysis of
variance
Note that the output of an ANOVA table depends on the order in which the variables
are considered.
Example 5.6 (Crop Yield (cont.)) We continue Examples 5.4

 and 5.5. Decompose the
linear model as
Y =

1
1
1
1

|{z}
X1
β0
|{z}
β1
+

0
0
0
1
0
0
0
1
0
0
0
1

|     {z     }
X2

β12
β13
β14

|{z}
β2
+

C
C
C
C

|{z}
X3

β22
β23
β24

|{z}
β3
+ ε.
Is the crop yield dependent on treatment levels as well as blocks? We first test whether we
can remove Block as a factor in the model against it playing a significant role in explain-
ing the crop yields. Specifically, we test β3 = 0 versus β3 , 0 using Theorem 5.4. Now
the vector Y(2) is the projection of Y onto the (p = 7)-dimensional space spanned by the
columns of X = [X1, X2, X3]; and Y(1) is the projection of Y onto the (k = 4)-dimensional
space spanned by the columns of X12 := [X1, X2]. The test statistic, T12 say, under H0 has
an F(3, 9) distribution.
The Python code below calculates the outcome of the test statistic T12 and the corres-
ponding P-value. We find t12 = 34.9998, which gives a P-value 2.73 × 10−5. This shows
that the block effects are extremely important for explaining the data.
Using the extended model (including the block effects), we can test whether β2 = 0 or
not; that is, whether the treatments have a significant effect on the crop yield in the presence
of the Block factor. This is done in the last six lines of the code below. The outcome of
Regression
185
the test statistic is 4.4878, with a P-value of 0.0346. By including the block effects, we
effectively reduce the uncertainty in the model and are able to more accurately assess the
effects of the treatments, to conclude that the treatment seems to have an effect on the crop
yield. A closer look at the data shows that within each block (row) the crop yield roughly
increases with the treatment level.
crop.py
import numpy as np
from scipy.stats import f
from numpy.linalg import lstsq, norm
yy = np.array([9.2988, 9.4978, 9.7604, 10.1025,
8.2111, 8.3387, 8.5018,
8.1942,
9.0688, 9.

1284, 9.3484,
9.5086,
8.2552, 7.8999, 8.4859,
8.9485]).reshape(4,4).T
nrow, ncol = yy.shape[0], yy.shape[1]
n = nrow * ncol
y = yy.reshape(16,)
X_1 = np.ones((n,1))
KM = np.kron(np.eye(ncol),np.ones((nrow ,1)))
KM[:,0]
X_2 = KM[:,1:ncol]
IM = np.eye(nrow)
C = IM[:,1:nrow]
X_3 = np.vstack((C, C))
X_3 = np.vstack((X_3, C))
X_3 = np.vstack((X_3, C))
X = np.hstack((X_1,X_2))
X = np.hstack((X,X_3))
p = X.shape[1] #number of parameters in full model
betahat = lstsq(X, y,rcond=None)[0]
#estimate under the full model
ym = X @ betahat
X_12 = np.hstack((X_1, X_2)) #omitting the block effect
k = X_12.shape[1] #number of parameters in reduced model
betahat_12 = lstsq(X_12, y,rcond=None)[0]
y_12 = X_12 @ betahat_12
T_12=(n-p)/(p-k)*(norm(y-y_12)**2 -
norm(y-ym)**2)/norm(y-ym)**2
pval_12 = 1 - f.cdf(T_12,p-k,n-p)
X_13 = np.hstack((X_1, X_3)) #omitting the treatment effect
k = X_13.shape[1] #number of parameters in reduced model
betahat_13 = lstsq(X_13, y,rcond=None)[0]
y_13 = X_13 @ betahat_13
T_13=(n-p)/(p-k)*(norm(y-y_13)**2 - norm(y-ym)**2)/norm(y-ym)**2
pval_13 = 1 - f.cdf(T_13,p-k,n-p)
186
Inference for Normal Linear Models
5.4.2
Confidence and Prediction Intervals
As in all supervised learning settings, linear regression is most useful when we wish to
predict how a new response variable will behave on the basis of a new explanatory vector
x. For example, it may be difficult to measure the response variable, but by knowing the
estimated regression line and the value for x, we will have a reasonably good idea what Y
or the expected value of Y is going to be.
Thus, consider a new x and let Y ∼N(x⊤β, σ2), with β and σ2 unknown. First we
are going to look at the expected value of Y, that is EY = x⊤β. Since β is unknown, we
do not know EY either. However, we can estimate it via the estimator bY = x⊤bβ, where
bβ ∼N(β, σ2(X⊤X)+), by Theorem 5.3. Being linear in the components of β, bY therefore
has a normal distribution with expectation x⊤β and variance σ2∥x⊤X+∥2. Let Z ∼N(0, 1)
be

 the standardized version of bY and V = ∥Y −Xbβ∥2/σ2 ∼χ2
n−p. Then the random variable
T := (x⊤bβ −x⊤β) / ∥x⊤X+∥
∥Y −Xbβ∥/
p
(n −p)
=
Z
p
V/(n −p)
(5.23)
has, by Corollary C.1, a tn−p distribution. After rearranging the identity P(|T| ⩽tn−p;1−α/2) =
☞439
1 −α, where tn−p;1−α/2 is the (1 −α/2) quantile of the tn−p distribution, we arrive at the
stochastic confidence interval
confidence
interval
x⊤bβ ± tn−p;1−α/2
√
RSE ∥x⊤X+∥,
(5.24)
where we have identified ∥Y −Xbβ∥2/(n −p) with RSE. This confidence interval quantifies
the uncertainty in the learner (regression surface).
A prediction interval
prediction
interval
for a new response Y is different from a confidence interval for
EY. Here the idea is to construct an interval such that Y lies in this interval with a certain
guaranteed probability. Note that now we have two sources of variation:
1. Y ∼N(x⊤β, σ2) itself is a random variable.
2. Estimating x⊤β via bY brings another source of variation.
We can construct a (1 −α) prediction interval, by finding two random bounds such that
the random variable Y lies between these bounds with probability 1 −α. We can reason as
follows. Firstly, note that Y ∼N(x⊤β, σ2) and bY ∼N(x⊤β, σ2∥x⊤X+∥2) are independent. It
follows that Y −bY has a normal distribution with expectation 0 and variance
σ2(1 + ∥x⊤X+∥2).
(5.25)
Secondly, letting Z ∼N(0, 1) be the standardized version of Y −bY, and repeating the
steps used for the construction of the confidence interval (5.24), we arrive at the prediction
interval
x⊤bβ ± tn−p;1−α/2
√
RSE
p
1 + ∥x⊤X+∥2.
(5.26)
This prediction interval captures the uncertainty from an as-yet-unobserved response as
well as the uncertainty in the parameters of the regression model itself.
Regression
187
Example 5.7 (Confidence Limits in Simple Linear Regression) The following pro-
gram draws n = 100 samples from a simple linear regression model with parameters
β = [6, 13]⊤and σ = 2, where the x-coordinates are evenly spaced on the interval [0, 1].
The parameters are

 estimated in the third block of the code. Estimates for β and σ are
[6.03, 13.09]⊤and bσ = 1.60, respectively. The program then proceeds by calculating the
95% numeric confidence and prediction intervals for various values of the explanatory
variable. Figure 5.4 shows the results.
confpred.py
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import t
from
numpy.linalg import inv, lstsq, norm
np.random.seed(123)
n = 100
x = np.linspace(0.01,1,100).reshape(n,1)
# parameters
beta = np.array([6,13])
sigma = 2
Xmat = np.hstack((np.ones((n,1)), x)) #design matrix
y = Xmat @ beta + sigma*np.random.randn(n)
# solve the normal equations
betahat = lstsq(Xmat, y,rcond=None)[0]
# estimate for sigma
sqMSE = norm(y - Xmat @ betahat)/np.sqrt(n-2)
tquant = t.ppf(0.975,n-2) # 0.975 quantile
ucl = np.zeros(n) #upper conf. limits
lcl = np.zeros(n) #lower conf. limits
upl = np.zeros(n)
lpl = np.zeros(n)
rl = np.zeros(n)
# (true) regression line
u = 0
for i in range(n):
u = u + 1/n;
xvec = np.array([1,u])
sqc = np.sqrt(xvec.T @ inv(Xmat.T @ Xmat) @ xvec)
sqp = np.sqrt(1 + xvec.T @ inv(Xmat.T @ Xmat) @ xvec)
rl[i] = xvec.T @ beta;
ucl[i] = xvec.T @ betahat + tquant*sqMSE*sqc;
lcl[i] = xvec.T @ betahat - tquant*sqMSE*sqc;
upl[i] = xvec.T @ betahat + tquant*sqMSE*sqp;
lpl[i] = xvec.T @ betahat - tquant*sqMSE*sqp;
plt.plot(x,y, '.')
plt.plot(x,rl,'b')
plt.plot(x,ucl,'k:')
plt.plot(x,lcl,'k:')
plt.plot(x,upl,'r--')
plt.plot(x,lpl,'r--')
188
Nonlinear Regression Models
0.0
0.2
0.4
0.6
0.8
1.0
5
10
15
20
Figure 5.4: The true regression line (blue, solid) and the upper and lower 95% prediction
curves (red, dashed) and confidence curves (dotted).
5.5
Nonlinear Regression Models
So far we have been mostly dealing with linear regression models, in which the predic-
tion function is of the form g(x | β) = x⊤β. In this section we discuss some strategies for
handling general prediction functions g(x | β), where the functional form is known up to an
unknown parameter vector β. So th

e regression model becomes
Yi = g(xi | β) + εi,
i = 1, . . . , n,
(5.27)
where ε1, . . . , εn are independent with expectation 0 and unknown variance σ2. The model
can be further specified by assuming that the error terms have a normal distribution.
Table 5.3 gives some common examples of nonlinear prediction functions for data tak-
ing values in R.
Table 5.3: Common nonlinear prediction functions for one-dimensional data.
Name
g(x | β)
β
Exponential
a ebx
a, b
Power law
a xb
a, b
Logistic
(1 + ea+bx)−1
a, b
Weibull
1 −exp(−xb/a)
a, b
Polynomial
Pp−1
k=0 βkxk
p, {βk}p−1
k=0
The logistic and polynomial prediction functions in Table 5.3 can be readily gener-
alized to higher dimensions. For example, for x ∈R2 a general second-order polynomial
prediction function is of the form
g(x | β) = β0 + β1 x1 + β2 x2 + β11 x2
1 + β22 x2
2 + β12 x1 x2.
(5.28)
Regression
189
This function can be viewed as a second-order approximation to a general smooth predic-
tion function g(x1, x2); see also Exercise 4. Polynomial regression models are also called
response surface models.
response
surface model
The generalization of the above logistic prediction to Rd is
g(x | β) = (1 + e−x⊤β)−1.
(5.29)
This function will make its appearance in Section 5.7 and later on in Chapters 7 and 9.
The first strategy for performing regression with nonlinear prediction functions is to
extend the feature space to obtain a simpler (ideally linear) prediction function in the ex-
tended feature space. We already saw an application of this strategy in Example 2.1 for
☞
26
the polynomial regression model, where the original feature u was extended to the feature
vector x = [1, u, u2, . . . , up−1]⊤, yielding a linear prediction function. In a similar way, the
right-hand side of the polynomial prediction function in (5.28) can be viewed as a linear
function of the extended feature vector ϕ(x) = [1, x1, x2, x2
1, x2
2, x1x2]⊤. The function ϕ is
called a feature map
feature map
.
The second strategy is to transfor

m the response variable y and possibly also the ex-
planatory variable x such that the transformed variablesey,ex are related in a simpler (ideally
linear) way. For example, for the exponential prediction function y = a e−bx, we have
ln y = ln a −bx, which is a linear relation between ln y and [1, x]⊤.
Example 5.8 (Chlorine) Table 5.4 lists the free chlorine concentration (in mg per liter)
in a swimming pool, recorded every 8 hours for 4 days. A simple chemistry-based model
for the chlorine concentration y as a function of time t is y = a e−b t, where a is the initial
concentration and b > 0 is the reaction rate.
Table 5.4: Chlorine concentration (in mg/L) as a function of time (hours).
Hours
Concentration
0
1.0056
8
0.8497
16
0.6682
24
0.6056
32
0.4735
40
0.4745
48
0.3563
Hours
Concentration
56
0.3293
64
0.2617
72
0.2460
80
0.1839
88
0.1867
96
0.1688
The exponential relationship y = a e−bt suggests that a log transformation of y will result
in a linear relationship between ln y and the feature vector [1, t]⊤. Thus, if for some given
data (t1, y1), . . . , (tn, yn), we plot (t1, ln y1), . . . , (tn, ln yn), these points should approximately
lie on a straight line, and hence the simple linear regression model applies. The left panel of
Figure 5.5 illustrates that the transformed data indeed lie approximately on a straight line.
The estimated regression line is also drawn here. The intercept and slope are β0 = −0.0555
and β1 = −0.0190 here. The original (non-transformed) data is shown in the right panel
of Figure 5.5, along with the fitted curve y = ba e−bbt, where ba = exp(bβ0) = 0.9461 and
bb = −bβ1 = 0.0190.
190
Nonlinear Regression Models
0
50
100
t
-2
-1.5
-1
-0.5
0
0.5
log y
0
50
100
t
0
0.5
1
1.5
y
Figure 5.5: The chlorine concentration seems to have an exponential decay.
Recall that for a general regression problem the learner gτ(x) for a given training set τ
is obtained by minimizing the training (squared-error) loss
ℓτ(g(· | β)) = 1
n
n
X
i=1
(yi −g(xi | β))

2.
(5.30)
The third strategy for regression with nonlinear prediction functions is to directly minimize
(5.30) by any means possible, as illustrated in the next example.
Example 5.9 (Hougen Function) In [7] the reaction rate y of a certain chemical reac-
tion is posited to depend on three input variables: quantities of hydrogen x1, n-pentane x2,
and isopentane x3. The functional relationship is given by the Hougen function:
y =
β1 x2 −x3/β5
1 + β2 x1 + β3 x2 + β4 x3
,
where β1, . . . , β5 are the unknown parameters. The objective is to estimate the model para-
meters {βi} from the data, as given in Table 5.5.
Table 5.5: Data for the Hougen function.
x1
x2
x3
y
470
300
10
8.55
285
80
10
3.79
470
300
120
4.82
470
80
120
0.02
470
80
10
2.75
100
190
10
14.39
100
80
65
2.54
x1
x2
x3
y
470
190
65
4.35
100
300
54
13.00
100
300
120
8.50
100
80
120
0.05
285
300
10
11.32
285
190
120
3.13
The estimation is carried out via the least-squares method. The objective function to
minimize is thus
ℓτ(g(· | β)) = 1
13
13
X
i=1
 
yi −
β1 xi2 −xi3/β5
1 + β2 xi1 + β3 xi2 + β4 xi3
!2
,
(5.31)
Regression
191
where the {yi} and {xij} are given in Table 5.5.
This is a highly nonlinear optimization problem, for which standard nonlinear least-
☞414
squares methods do not work well. Instead, one can use global optimization methods such
as CE and SCO (see Sections 3.4.2 and 3.4.3). Using the CE method, we found the minimal
☞100
value 0.02299 for the objective function, which is attained at
bβ = [1.2526, 0.0628, 0.0400, 0.1124, 1.1914]⊤.
5.6
Linear Models in Python
In this section we describe how to define and analyze linear models using Python and the
data science module statsmodels. We encourage the reader to regularly refer back to
the theory in the preceding sections of this chapter, so as to avoid using Python merely
as a black box without understanding the underlying principles. To run the code start by
importing the following code snippet:
import matplotlib.pyplot as plt
import pandas as pd


import statsmodels.api as sm
from statsmodels.formula.api import ols
5.6.1
Modeling
Although specifying a normal3 linear model in Python is relatively easy, it requires some
subtlety. The main thing to realize is that Python treats quantitative and qualitative (that
is, categorical) explanatory variables differently. In statsmodels, ordinary least-squares
linear models are specified via the function ols (short for ordinary least-squares). The
main argument of this function is a formula of the form
y ∼x1 + x2 + · · · + xd,
(5.32)
where y is the name of the response variable and x1, ..., xd are the names of the explan-
atory variables. If all variables are quantitative, this describes the linear model
Yi = β0 + β1xi1 + β2xi2 + · · · + βdxid + εi,
i = 1, . . . , n,
(5.33)
where xij is the j-th explanatory variable for the i-th observation and the errors εi are
independent normal random variables such that Eεi = 0 and Var εi = σ2. Or, in matrix
form: Y = Xβ + ε, with
Y =

Y1
...
Yn
,
X =

1
x11
· · ·
x1d
1
x21
· · ·
x2d
...
...
...
...
1
xn1
· · ·
xnd

,
β =

β0
...
βd
, and ε =

ε1...
εn
.
3For the rest of this section, we assume all linear models to be normal.
192
Linear Models in Python
Thus, the first column is always taken as an “intercept” parameter, unless otherwise spe-
cified. To remove the intercept term, add -1 to the ols formula, as in ols(’y∼x-1’).
For any linear model, the model matrix can be retrieved via the construction:
model_matrix = pd.DataFrame(model.exog,columns=model.exog_names)
Let us look at some examples of linear models. In the first model the variables x1 and x2
are both considered (by Python) to be quantitative.
myData = pd.DataFrame({'y' : [10,9,4,2,4,9],
'x1' : [7.4,1.2,3.1,4.8,2.8,6.5],
'x2' : [1,1,2,2,3,3]})
mod = ols("y~x1+x2", data=myData)
mod_matrix = pd.DataFrame(mod.exog,columns=mod.exog_names)
print(mod_matrix)
Intercept
x1
x2
0
1.0


7.4
1.0
1
1.0
1.2
1.0
2
1.0
3.1
2.0
3
1.0
4.8
2.0
4
1.0
2.8
3.0
5
1.0
6.5
3.0
Suppose the second variable is actually qualitative; e.g., it represents a color, and the
levels 1, 2, and 3 stand for red, blue, and green. We can account for such a categorical
variable by using the astype method to redefine the data type (see Section 1.2).
☞3
myData['x2'] = myData['x2'].astype('category')
Alternatively, a categorical variable can be specified in the model formula by wrapping
it with C(). Observe how this changes the model matrix.
mod2 = ols("y~x1+C(x2)", data=myData)
mod2_matrix = pd.DataFrame(mod2.exog,columns=mod2.exog_names)
print(mod2_matrix)
Intercept
C(x2)[T.2]
C(x2)[T.3]
x1
0
1.0
0.0
0.0
7.4
1
1.0
0.0
0.0
1.2
2
1.0
1.0
0.0
3.1
3
1.0
1.0
0.0
4.8
4
1.0
0.0
1.0
2.8
5
1.0
0.0
1.0
6.5
Thus, if a statsmodels formula of the form (5.32) contains factor (qualitative) variables,
the model is no longer of the form (5.33), but contains indicator variables for each level of
the factor variable, except the first level.
For the case above, the corresponding linear model is
Yi = β0 + β1xi1 + α2 1{xi2 = 2} + α3 1{xi2 = 3} + εi,
i = 1, . . . , 6,
(5.34)
Regression
193
where we have used parameters α2 and α3 to correspond to the indicator features of the
qualitative variable. The parameter α2 describes how much the response is expected to
change if the factor x2 switches from level 1 to 2. A similar interpretation holds for α3.
Such parameters can thus be viewed as incremental effects.
It is also possible to model interaction
interaction
between two variables. For two continuous
variables, this simply adds the products of the original features to the model matrix. Adding
interaction terms in Python is achieved by replacing “+” in the formula with “*”, as the
following example illustrates.
mod3 = ols("y~x1*C(x2)", data=myData)
mod3_matrix = pd.DataFrame(mod3.exog,columns=mod3.exog_names)
print(mod3_matrix)
Intercept
C(x2)[T.2]
C(x2)[T.3]
x1
x1:C(x2)[T.2]
x1:C(x2)[T.3]
0
1.0
0.0
0.0


7.4
0.0
0.0
1
1.0
0.0
0.0
1.2
0.0
0.0
2
1.0
1.0
0.0
3.1
3.1
0.0
3
1.0
1.0
0.0
4.8
4.8
0.0
4
1.0
0.0
1.0
2.8
0.0
2.8
5
1.0
0.0
1.0
6.5
0.0
6.5
5.6.2
Analysis
Let us consider some easy linear regression models by using the student survey data set
survey.csv from the book’s GitHub site, which contains measurements such as height,
weight, sex, etc., from a survey conducted among n = 100 university students. Suppose we
wish to investigate the relation between the shoe size (explanatory variable) and the height
(response variable) of a person. First, we load the data and draw a scatterplot of the points
(height versus shoe size); see Figure 5.6 (without the fitted line).
survey = pd.read_csv('survey.csv')
plt.scatter(survey.shoe, survey.height)
plt.xlabel("Shoe size")
plt.ylabel("Height")
We observe a slight increase in the height as the shoe size increases, although this
relationship is not very distinct. We analyze the data through the simple linear regression
model Yi = β0 + β1xi + εi, i = 1, . . . , n. In statsmodels this is performed via the ols
☞169
method as follows:
model = ols("height~shoe", data=survey) # define the model
fit = model.fit()
#fit the model defined above
b0, b1 = fit.params
print(fit.params)
Intercept
145.777570
shoe
1.004803
dtype: float64
194
Linear Models in Python
15
20
25
30
35
Shoe size
150
160
170
180
190
200
Height
Figure 5.6: Scatterplot of height (cm) against shoe size (cm), with the fitted line.
The above output gives the least-squares estimates of β0 and β1. For this example, we
have bβ0 = 145.778 and bβ1 = 1.005. Figure 5.6, which includes the regression line, was
obtained as follows:
plt.plot(survey.shoe, b0 + b1*survey.shoe)
plt.scatter(survey.shoe, survey.height)
plt.xlabel("Shoe size")
plt.ylabel("Height")
Although ols performs a complete analysis of the linear model, not all its calculations
need to be presented. A summary of the results can be obtained with the method summary.
print(fit.summary())
Dep. Variable:
height
R-squared

:
0.178
Model:
OLS
Adj. R-squared:
0.170
Method:
Least Squares
F-statistic:
21.28
No. Observations:
100
Prob (F-statistic):
1.20e-05
Df Residuals:
98
Log-Likelihood:
-363.88
Df Model:
1
AIC:
731.8
Covariance Type:
nonrobust
BIC:
737.0
=====================================================================
coef
std err
t
P>|t|
[0.025
0.975]
--------------------------------------------------------------------
Intercept
145.7776
5.763
25.296
0.000
134.341
157.214
shoe
1.0048
0.218
4.613
0.000
0.573
1.437
=====================================================================
Omnibus:
1.958
Durbin-Watson:
1.772
Prob(Omnibus):
0.376
Jarque-Bera (JB):
1.459
Skew:
-0.072
Prob(JB):
0.482
Kurtosis:
2.426
Cond. No.
164.
The main output items are the following:
Regression
195
• coef: Estimates of the parameters of the regression line.
• std error: Standard deviations of the estimators of the regression line. These are
the square roots of the variances of the {bβi} obtained in (5.25).
☞186
• t: Realization of Student’s test statistics associated with the hypotheses H0 : βi = 0
and H1 : βi , 0, i = 0, 1. In particular, the outcome of T in (5.19).
☞183
• P>|t|: P-value of Student’s test (two-sided test).
• [0.025 0.975]: 95% confidence intervals for the parameters.
• R-Squared: Coefficient of determination R2 (percentage of variation explained by
the regression), as defined in (5.18).
☞181
• Adj. R-Squared: adjusted R2 (explained in Section 5.3.7).
• F-statistic: Realization of the F test statistic (5.21) associated with testing the
☞183
full model against the default model. The associated degrees of freedom (Df Model
= 1 and Df Residuals = n−2) are given, as is the P-value: Prob (F-statistic).
• AIC: The AIC number in (5.15); that is, minus two times the log-likelihood plus two
☞177
times the number of model parameters (which is 3 here).
You can access all the numerical values as they are attributes of the fit object. First
check which names are available, as in:
dir(fit)
Then acces

s the values via the dot construction. For example, the following extracts the
P-value for the slope.
fit.pvalues[1]
1.1994e-05
The results show strong evidence for a linear relationship between shoe size and height
(or, more accurately, strong evidence that the slope of the regression line is not zero), as
the P-value for the corresponding test is very small (1.2 · 10−5). The estimate of the slope
indicates that the difference between the average height of students whose shoe size is
different by one cm is 1.0048 cm.
Only 17.84% of the variability of student height is explained by the shoe size. We
therefore need to add other explanatory variables to the model (multiple linear regression)
to increase the model’s predictive power.
196
Linear Models in Python
5.6.3
Analysis of Variance (ANOVA)
We continue the student survey example of the previous section, but now add an extra
variable, and also consider an analysis of variance of the model. Instead of “explaining”
the student height via their shoe size, we include weight as an explanatory variable. The
corresponding ols formula for this model is
height∼shoe + weight,
meaning that each random height, denoted by Height, satisfies
Height = β0 + β1shoe + β2weight + ε,
where ε is a normally distributed error term with mean 0 and variance σ2. Thus, the model
has 4 parameters. Before analyzing the model we present a scatterplot of all pairs of vari-
ables, using scatter_matrix.
model = ols("height~shoe+weight", data=survey)
fit = model.fit()
axes = pd.plotting.scatter_matrix(
survey[['height','shoe','weight']])
plt.show()
150
175
height
20
30
shoe
150
175
height
50
100
weight
20
30
shoe
50
100
weight
Figure 5.7: Scatterplot of all pairs of variables: height (cm), shoe (cm), and weight (kg).
As for the simple linear regression model in the previous section, we can analyze the
model using the summary method (below we have omitted some output):
fit.summary()
Regression
197
Dep. Variable:
height
R-squared:
0.430
Model:
OLS
Adj

. R-squared:
0.418
Method:
Least Squares
F-statistic:
36.61
No. Observations:
100
Prob (F-statistic):
1.43e-12
Df Residuals:
97
Log-Likelihood:
-345.58
Df Model:
2
AIC:
697.2
BIC:
705.0
======================================================================
coef
std err
t
P>|t|
[0.025
0.975]
----------------------------------------------------------------------
Intercept
132.2677
5.247
25.207
0.000
121.853
142.682
shoe
0.5304
0.196
2.703
0.008
0.141
0.920
weight
0.3744
0.057
6.546
0.000
0.261
0.488
The F-statistic is used to test whether the full model (here with two explanatory
variables) is better at “explaining” the height than the default model. The corresponding
null hypothesis is H0 : β1 = β2 = 0. The assertion of interest is H1: at least one of the coeffi-
cients β j (j = 1, 2) is significantly different from zero. Given the result of this test (P-value
= 1.429·10−12), we can conclude that at least one of the explanatory variables is associated
with height. The individual Student tests indicate that:
• shoe size is linearly associated with student height, after adjusting for weight, with
P-value 0.0081. At the same weight, an increase of one cm in shoe size corresponds
to an increase of 0.53 cm in average student height;
• weight is linearly associated with student height, after adjusting for shoe size (the
P-value is actually 2.82 · 10−09; the reported value of 0.000 should be read as “less
than 0.001”). At the same shoe size, an increase of one kg in weight corresponds to
an increase of 0.3744 cm in average student height.
Further understanding is extracted from the model by conducting an analysis of vari-
ance. The standard statsmodels function is anova_lm. The input to this function is the
fit object (obtained from model.fit()) and the output is a DataFrame object.
table = sm.stats.anova_lm(fit)
print(table)
df
sum_sq
mean_sq
F
PR(>F)
shoe
1.0
1840.467359
1840.467359
30.371310
2.938651e-07
weight
1.0
2596.275747
2596.275747
42.843626
2.816065e-09
Residual


97.0
5878.091294
60.598879
NaN
NaN
The meaning of the columns is as follows.
• df : The degrees of freedom of the variables, according to the sum of squares decom-
position (5.17). As both shoe and weight are quantitative variables, their degrees
☞181
of freedom are both 1 (each corresponding to a single column in the overall model
matrix). The degrees of freedom for the residuals is n −p = 100 −3 = 97.
• sum sq: The sum of squares according to (5.17). The total sum of squares is the
sum of all the entries in this column. The residual error in the model that cannot be
explained by the variables is RSS ≈5878.
198
Linear Models in Python
• mean sq: The sum of squares divided by their degrees of freedom. Note that the
residual square error RSE = RSS/(n −p) = 60.6 is an unbiased estimate of the
model variance σ2; see Section 5.4.
☞182
• F: These are the outcomes of the test statistic (5.22).
☞184
• PR(>F): These are the P-values corresponding to the test statistic in the preceding
column and are computed using an F distribution whose degrees of freedom are
given in the df column.
The ANOVA table indicates that the shoe variable explains a reasonable amount of the
variation in the model, as evidenced by a sum of squares contribution of 1840 out of 1840+
2596+5878 = 10314 and a very small P-value. After shoe is included in the model, it turns
out that the weight variable explains even more of the remaining variability, with an even
smaller P-value. The remaining sum of squares (5878) is 57% of the total sum of squares,
yielding a 43% reduction, in accordance with the R2 value reported in the summary for the
ols method. As mentioned in Section 5.4.1, the order in which the ANOVA is conducted
is important. To illustrate this, consider the output of the following commands.
model = ols("height~weight+shoe", data=survey)
fit = model.fit()
table = sm.stats.anova_lm(fit)
print(table)
df
sum_sq
mean_sq
F
PR(>F)
weight
1.0
3993.860167
3993.860167
65.906502
1.503553e-12
shoe
1.0
44

2.882938
442.882938
7.308434
8.104688e-03
Residual
97.0
5878.091294
60.598879
NaN
NaN
We see that weight as a single model variable explains much more of the variability
than shoe did. If we now also include shoe, we only obtain a small (but according to the
P-value still significant) reduction in the model variability.
5.6.4
Confidence and Prediction Intervals
In statsmodels a method for computing confidence or prediction intervals from a dic-
tionary of explanatory variables is get_prediction. It simply executes formula (5.24) or
(5.26). A simpler version is predict, which only returns the predicted value.
☞186
Continuing the student survey example, suppose we wish to predict the height of a
person with shoe size 30 cm and weight 75 kg. Confidence and prediction intervals can
be obtained as given in the code below. The new explanatory variable is entered as a dic-
tionary. Notice that the 95% prediction interval (for the corresponding random response) is
much wider than the 95% confidence interval (for the expectation of the random response).
x = {'shoe': [30.0], 'weight': [75.0]}
# new input (dictionary)
pred = fit.get_prediction(x)
pred.summary_frame(alpha=0.05).unstack()
Regression
199
mean
0
176.261722
# predicted value
mean_se
0
1.054015
mean_ci_lower
0
174.169795
# lower bound for CI
mean_ci_upper
0
178.353650
# upper bound for CI
obs_ci_lower
0
160.670610
# lower bound for PI
obs_ci_upper
0
191.852835
# upper bound for PI
dtype: float64
5.6.5
Model Validation
We can perform an analysis of residuals to examine whether the underlying assumptions
of the (normal) linear regression model are verified. Various plots of the residuals can be
used to inspect whether the assumptions on the errors {εi} are satisfied. Figure 5.8 gives two
such plots. The first is a scatterplot of the residuals {ei} against the fitted valuesbyi. When the
model assumptions are valid, the residuals, as approximations of the model error, should
behave approximately as iid normal random var

iables for each of the fitted values, with a
constant variance. In this case we see no strong aberrant structure in this plot. The residuals
are fairly evenly spread and symmetrical about the y = 0 line (not shown). The second plot
is a quantile–quantile (or qq) plot. This is a useful way to check for normality of the error
terms, by plotting the sample quantiles of the residuals against the theoretical quantiles
of the standard normal distribution. Under the model assumptions, the points should lie
approximately on a straight line. For the current case there does not seem to be an extreme
departure from normality. Drawing a histogram or density plot of the residuals will also
help to verify the normality assumption. The following code was used.
plt.plot(fit.fittedvalues ,fit.resid,'.')
plt.xlabel("fitted values")
plt.ylabel("residuals")
sm.qqplot(fit.resid)
155
160
165
170
175
180
185
190
195
fitted values
25
20
15
10
5
0
5
10
15
20
residuals
3
2
1
0
1
2
3
Theoretical Quantiles
25
20
15
10
5
0
5
10
15
20
Sample Quantiles
Figure 5.8: Left: residuals against fitted values. Right: a qq plot of the residuals. Neither
shows clear evidence against the model assumptions of constant variance and normality.
200
Linear Models in Python
5.6.6
Variable Selection
Among the large number of possible explanatory variables, we wish to select those which
best explain the observed responses. By eliminating redundant explanatory variables, we
reduce the statistical error without increasing the approximation error, and thus reduce the
(expected) generalization risk of the learner.
In this section, we briefly present two methods for variable selection. They are illus-
trated on a few variables from the data set birthwt discussed in Section 1.5.3.2. The data
☞13
set contains information on the birth weights (masses) of babies, as well as various char-
acteristics of the mother, such as whether she smokes, her age, etc. We wish to explain
the child’s weight at birth using various characte

ristics of the mother, her family history,
and her behavior during pregnancy. The response variable is weight at birth (quantitative
variable bwt, expressed in grams); the explanatory variables are given below.
The data can be obtained as explained in Section 1.5.3.2, or from statsmodels in the
following way:
bwt = sm.datasets.get_rdataset("birthwt","MASS").data
Here is some information about the explanatory variables that we will investigate.
age:
mother's age in years
lwt:
mother's weight in lbs
race:
mother's race (1 = white, 2 = black, 3 = other)
smoke: smoking status during pregnancy (0 = no, 1 = yes)
ptl:
no. of previous premature labors
ht:
history of hypertension (0 = no, 1 = yes)
ui:
presence of uterine irritability (0 = no, 1 = yes)
ftv:
no. of physician visits during first trimester
bwt:
birth weight in grams
We can see the structure of the variables via bwt.info(). Check yourself that all
variables are defined as quantitative (int64). However, the variables race, smoke, ht,
and ui should really be interpreted as qualitative (factors). To fix this, we could redefine
them with the method astype, similar to what we did in Chapter 1. Alternatively, we could
use the C() construction in a statsmodels formula to let the program know that certain
variables are factors. We will use the latter approach.
For binary features it does not matter whether the variables are interpreted as
factorial or numerical as the numerical and summary results are identical.
We consider the explanatory variables lwt, age, ui, smoke, ht, and two recoded binary
variables ftv1 and ptl1. We define ftv1 = 1 if there was at least one visit to a physician,
and ftv1 = 0 otherwise. Similarly, we define ptl1 = 1 if there is at least one preterm birth
in the family history, and ptl1 = 0 otherwise.
Regression
201
ftv1 = (bwt['ftv']>=1).astype(int)
ptl1 = (bwt['ptl']>=1).astype(int)
5.6.6.1
Forward Selection and Backward Elimination
The forward selection
forward
selection
method is an iterative m

ethod for variable selection. In the first
iteration we consider which feature f1 is the most significant in terms of its P-value in the
models bwt∼f1, with f1 ∈{lwt, age, . . .}. This feature is then selected into the model. In
the second iteration, the feature f2 that has the smallest P-value in the models bwt∼f1+f2
is selected, where f2 , f1, and so on. Usually only features are selected that have a P-
value of at most 0.05. The following Python program automates this procedure. Instead of
selecting on the P-value one could select on the AIC or BIC value.
forwardselection.py
import statsmodels.api as sm
from statsmodels.formula.api import ols
bwt = sm.datasets.get_rdataset("birthwt","MASS").data
ftv1 = (bwt['ftv']>=1).astype(int)
ptl1 = (bwt['ptl']>=1).astype(int)
remaining_features = {'lwt', 'age', 'C(ui)', 'smoke',
'C(ht)', 'ftv1', 'ptl1'}
selected_features = []
while remaining_features:
PF = []
#list of (P value, feature)
for f in remaining_features:
temp = selected_features + [f]
#temporary list of features
formula = 'bwt~' + '+'.join(temp)
fit = ols(formula ,data=bwt).fit()
pval= fit.pvalues[-1]
if pval < 0.05:
PF.append((pval,f))
if PF:
#if not empty
PF.sort(reverse=True)
(best_pval , best_f) = PF.pop()
remaining_features.remove(best_f)
print('feature {} with P-value = {:.2E}'.
format(best_f, best_pval))
selected_features.append(best_f)
else:
break
feature C(ui) with P-value = 7.52E-05
feature C(ht) with P-value = 1.08E-02
feature lwt with P-value = 6.01E-03
feature smoke with P-value = 7.27E-03
In backward elimination
backward
elimination
we start with the complete model (all features included) and
at each step, we remove the variable with the highest P-value, as long as it is not significant
(greater than 0.05). We leave it as an exercise to verify that the order in which the fea-
202
Linear Models in Python
tures are removed is: age, ftv1, and ptl1. In this case, forward selection and backward
elimination result in the same model, but this need not be th

e case in general.
This way of model selection has the advantage of being easy to use and of treating the
question of variable selection in a systematic manner. The main drawback is that variables
are included or deleted based on purely statistical criteria, without taking into account the
aim of the study. This usually leads to a model which may be satisfactory from a statistical
point of view, but in which the variables are not necessarily the most relevant when it comes
to understanding and interpreting the data in the study.
Of course, we can choose to investigate any combination of features, not just the ones
suggested by the above variable selection methods. For example, let us see if the mother’s
weight, her age, her race, and whether she smokes explain the baby’s birthweight.
formula = 'bwt~lwt+age+C(race)+ smoke'
bwt_model = ols(formula , data=bwt).fit()
print(bwt_model.summary())
OLS Regression Results
======================================================================
Dep. Variable: bwt
R-squared: 0.148
Model: OLS
Adj. R-squared: 0.125
Method: Least Squares
F-statistic: 6.373
No. Observations: 189
Prob (F-statistic):
1.76e-05
Df Residuals: 183
Log-Likelihood: -1498.4
Df Model: 5
AIC: 3009.
BIC: 3028.
=====================================================================
coef
std err
t
P>|t|
[0.025
0.975]
----------------------------------------------------------------------
Intercept
2839.4334
321.435
8.834
0.000
2205.239
3473.628
C(race)[T.2]
-510.5015
157.077
-3.250
0.001
-820.416
-200.587
C(race)[T.3]
-398.6439
119.579
-3.334
0.001
-634.575
-162.713
smoke
-401.7205
109.241
-3.677
0.000
-617.254
-186.187
lwt
3.9999
1.738
2.301
0.022
0.571
7.429
age
-1.9478
9.820
-0.198
0.843
-21.323
17.427
======================================================================
Omnibus: 3.916
Durbin-Watson: 0.458
Prob(Omnibus): 0.141
Jarque-Bera (JB): 3.718
Skew: -0.343
Prob(JB): 0.156
Kurtosis: 3.038
Cond. No. 899.
Given the result of Fisher’s global test given by Prob

 (F-Statistic) in the summary
(P-value = 1.76 × 10−5), we can conclude that at least one of the explanatory variables is
associated with child weight at birth, after adjusting for the other variables. The individual
Student tests indicate that:
• the mother’s weight is linearly associated with child weight, after adjusting for age,
race, and smoking status (P-value = 0.022). At the same age, race, and smoking
status, an increase of one pound in the mother’s weight corresponds to an increase
of 4 g in the average child weight at birth;
• the age of the mother is not significantly linearly associated with child weight at
birth, when mother weight, race, and smoking status are already taken into account
Regression
203
(P-value = 0.843);
• weight at birth is significantly lower for a child born to a mother who smokes, com-
pared to children born to non-smoking mothers of the same age, race, and weight,
with a P-value of 0.00031 (to see this, inspect bwt_model.pvalues). At the same
age, race, and mother weight, the child’s weight at birth is 401.720 g less for a
smoking mother than for a non-smoking mother;
• regarding the interpretation of the variable race, we note that the first level of this
categorical variable corresponds to white mothers. The estimate of −510.501 g for
C(race)[T.2] represents the difference in the child’s birth weight between black
mothers and white mothers (reference group), and this result is significantly different
from zero (P-value = 0.001) in a model adjusted for the mother’s weight, age, and
smoking status.
5.6.6.2
Interaction
We can also include interaction terms in the model. Let us see whether there is any inter-
action effect between smoke and age via the model
Bwt = β0 + β1age + β2smoke + β3age × smoke + ε.
In Python this can be done as follows (below we have removed some output):
formula = 'bwt~age*smoke'
bwt_model = ols(formula , data=bwt).fit()
print(bwt_model.summary())
OLS Regression Results
=======================================

===============================
Dep. Variable: bwt
R-squared: 0.069
Model: OLS
Adj. R-squared: 0.054
Method: Least Squares
F-statistic: 4.577
No. Observations: 189
Prob (F-statistic):
0.00407
Df Residuals: 183
Log-Likelihood: -1506.8
Df Model: 5
AIC: 3009.
BIC: 3028.
======================================================================
coef
std err
t
P>|t|
[0.025
0.975]
----------------------------------------------------------------------
Intercept
2406.1
292.190
8.235
0.000
1829.6
2982.5
smoke
798.2
484.342
1.648
0.101
-157.4
1753.7
age
27.7
12.149
2.283
0.024
3.8
51.7
age:smoke
-46.6
20.447
-2.278
0.024
-86.9
-6.2
We observe that the estimate for β3 (−46.6) is significantly different from zero (P-value
= 0.024). We therefore conclude that the effect of the mother’s age on the child’s weight
depends on the smoking status of the mother. The results on association between mother
age and child weight must therefore be presented separately for the smoking and the non-
smoking group. For non-smoking mothers (smoke = 0), the mean child weight at birth
increases on average by 27.7 grams for each year of the mother’s age. This is statistically
204
Generalized Linear Models
significant, as can be seen from the 95% confidence intervals for the parameters (which
does not contain zero):
bwt_model.conf_int()
0
1
Intercept
1829.605754
2982.510194
age
3.762780
51.699977
smoke
-157.368023
1753.717779
age:smoke
-86.911405
-6.232425
Similarly, for smoking mothers, there seems to be a decrease in birthweight, bβ1 + bβ3 =
27.7 −46.6 = −18.9, but this is not statistically significant; see Exercise 6.
5.7
Generalized Linear Models
The normal linear model in Section 2.8 deals with continuous response variables — such
as height and crop yield — and continuous or discrete explanatory variables. Given the
feature vectors {xi}, the responses {Yi} are independent of each other, and each has a normal
distribution with mean x⊤
i β, where x⊤
i is the i-th row of the model matrix X. Generalized


linear models allow for arbitrary response distributions, including discrete ones.
Definition 5.2: Generalized Linear Model
In a generalized linear model
generalized
linear model
(GLM) the expected response for a given feature vec-
tor x = [x1, . . . , xp]⊤is of the form
E[Y | X = x] = h(x⊤β)
(5.35)
for some function h, which is called the activation function
activation
function
. The distribution of
Y (for a given x) may depend on additional dispersion parameters that model the
randomness in the data that is not explained by x.
The inverse of function h is called the link function
link function
. As for the linear model, (5.35) is
a model for a single pair (x, Y). Using the model simplification introduced at the end of
Section 5.1, the corresponding model for a whole training set T = {(xi, Yi)} is that the {xi}
are fixed and that the {Yi} are independent; each Yi satisfying (5.35) with x = xi. Writing
Y = [Y1, . . . , Yn]⊤and defining h as the multivalued function with components h, we have
EXY = h(Xβ),
where X is the (model) matrix with rows x⊤
1 , . . . , x⊤
n . A common assumption is that
Y1, . . . , Yn come from the same family of distributions, e.g., normal, Bernoulli, or Pois-
son. The central focus is the parameter vector β, which summarizes how the matrix of
explanatory variables X affects the response vector Y. The class of generalized linear mod-
els can encompass a wide variety of models. Obviously the normal linear model (2.34) is
a generalized linear model, with E[Y | X = x] = x⊤β, so that h is the identity function. In
this case, Y ∼N(x⊤β, σ2), i = 1, . . . , n, where σ2 is a dispersion parameter.
Regression
205
Example 5.10 (Logistic Regression) In a logistic regression
logistic
regression
or logit model, we as-
sume that the response variables Y1, . . . , Yn are independent and distributed according to
Yi ∼Ber(h(x⊤
i β)), where h here is defined as the cdf of the logistic distribution
logistic
distribution
:
h(x) =
1
1 + e−x.
Large values of x⊤
i β

 thus lead to a high probability that Yi = 1, and small (negative) values
of x⊤
i β cause Yi to be 0 with high probability. Estimation of the parameter vector β from
the observed data is not as straightforward as for the ordinary linear model, but can be
accomplished via the minimization of a suitable training loss, as explained below.
As the {Yi} are independent, the pdf of Y = [Y1, . . . , Yn]⊤is
g(y | β, X) =
n
Y
i=1
[h(x⊤
i β)]yi[1 −h(x⊤
i β)]1−yi.
Maximizing the log-likelihood ln g(y | β, X) with respect to β gives the maximum likeli-
hood estimator of β. In a supervised learning framework, this is equivalent to minimizing:
−1
n ln g(y | β, X) = −1
n
n
X
i=1
ln g(yi | β, xi)
= −1
n
n
X
i=1
yi ln h(x⊤
i β) + (1 −yi) ln(1 −h(x⊤
i β)) .
(5.36)
By comparing (5.36) with (4.4), we see that we can interpret (5.36) as the cross-entropy
☞123
training loss associated with comparing a true conditional pdf f(y | x) with an approxima-
tion pdf g(y | β, x) via the loss function
Loss( f(y | x), g(y | β, x)) := −ln g(y | β, x) = −y ln h(x⊤β) −(1 −y) ln(1 −h(x⊤β)).
Minimizing (5.36) in terms of β actually constitutes a convex optimization problem. Since
ln h(x⊤β) = −ln(1 + e−x⊤β) and ln(1 −h(x⊤β)) = −x⊤β −ln(1 + e−x⊤β), the cross-entropy
training loss (5.36) can be rewritten as
rτ(β) := 1
n
n
X
i=1
h
(1 −yi)x⊤
i β + ln

1 + e−x⊤
i βi
.
We leave it as Exercise 7 to show that the gradient ∇rτ(β) and Hessian H(β) of rτ(β) are
given by
∇rτ(β) = 1
n
n
X
i=1
(µi −yi) xi
(5.37)
and
H(β) = 1
n
n
X
i=1
µi(1 −µi) xi x⊤
i ,
(5.38)
respectively, where µi := h(x⊤
i β).
Notice that H(β) is a positive semidefinite matrix for all values of β, implying the
☞403
206
Generalized Linear Models
convexity of rτ(β). Consequently, we can find an optimal β efficiently; e.g., via Newton’s
method. Specifically, given an initial value β0, for t = 1, 2, . . . , iteratively compute
☞409
βt = βt−1 −H−1(βt−1) ∇rτ(βt−1),
(5.39)
until the sequence β0, β1, β2, . . . is deemed to have converged, using some pr

e-fixed con-
vergence criterion.
Figure 5.9 shows the outcomes of 100 independent Bernoulli random variables, where
each success probability, (1+exp(−(β0 +β1x)))−1, depends on x and β0 = −3, β1 = 10. The
true logistic curve is also shown (dashed line). The minimum training loss curve (red line)
is obtained via the Newton scheme (5.39), giving estimates bβ0 = −2.66 and bβ1 = 10.08.
The Python code is given below.
-1
-0.5
0
0.5
1
0
0.2
0.4
0.6
0.8
1
Figure 5.9: Logistic regression data (blue dots), fitted curve (red), and true curve (black
dashed).
logreg1d.py
import numpy as np
import matplotlib.pyplot as plt
from
numpy.linalg import lstsq
n = 100
# sample size
x = (2*np.random.rand(n)-1).reshape(n,1)
# explanatory variables
beta = np.array([-3, 10])
Xmat = np.hstack((np.ones((n,1)), x))
p = 1/(1 + np.exp(-Xmat @ beta))
y = np.random.binomial(1,p,n)
# response variables
# initial guess
betat = lstsq((Xmat.T @ Xmat),Xmat.T @ y, rcond=None)[0]
grad = np.array([2,1])
# gradient
while (np.sum(np.abs(grad)) > 1e-5) :
# stopping criteria
mu = 1/(1+np.exp(-Xmat @ betat))
# gradient
delta = (mu - y).reshape(n,1)
Regression
207
grad = np.sum(np.multiply( np.hstack((delta,delta)),Xmat), axis
=0).T
# Hessian
H = Xmat.T @ np.diag(np.multiply(mu,(1-mu))) @ Xmat
betat = betat - lstsq(H,grad,rcond=None)[0]
print(betat)
plt.plot(x,y, '.') # plot data
xx = np.linspace(-1,1,40).reshape(40,1)
XXmat = np.hstack( (np.ones((len(xx),1)), xx))
yy = 1/(1 + np.exp(-XXmat @ beta))
plt.plot(xx,yy,'r-')
#true logistic curve
yy = 1/(1 + np.exp(-XXmat @ betat));
plt.plot(xx,yy,'k--')
Further Reading
An excellent overview of regression is provided in [33] and an accessible mathematical
treatment of linear regression models can be found in [108]. For extensions to nonlinear
regression we refer the reader to [7]. A practical introduction to multilevel/hierarchical
models is given in [47]. For further discussion on regression with discrete responses (clas-
sification) we refer to Chapter 7 and the fur

ther reading therein. On the important question
☞251
of how to handle missing data, the classic reference is [80] (see also [85]) and a modern
applied reference is [120].
Exercises
1. Following his mentor Francis Galton, the mathematician/statistician Karl Pearson con-
ducted comprehensive studies comparing hereditary traits between members of the same
family. Figure 5.10 depicts the measurements of the heights of 1078 fathers and their
adult sons (one son per father). The data is available from the book’s GitHub site as
pearson.csv.
(a) Show that sons are on average 1 inch taller than the fathers.
(b) We could try to “explain” the height of the son by taking the height of his father and
adding 1 inch. The prediction line y = x + 1 (red dashed) is given Figure 5.10. The
black solid line is the fitted regression line. This line has a slope less than 1, and
demonstrates Galton’s “regression” to the average. Find the intercept and slope of the
fitted regression line.
2. For the simple linear regression model, show that the values for bβ1 and bβ0 that solve the
208
Exercises
58
60
62
64
66
68
70
72
74
76
Height Father (in)
60
65
70
75
Height Son (in)
Figure 5.10: A scatterplot of heights from Pearson’s data.
equations (5.9) are:
bβ1 =
Pn
i=1(xi −x)(yi −y)
Pn
i=1(xi −x)2
(5.40)
bβ0 = y −bβ1x,
(5.41)
provided that not all xi are the same.
3. Edwin Hubble discovered that the universe is expanding. If v is a galaxy’s recession ve-
locity (relative to any other galaxy) and d is its distance (from that same galaxy), Hubble’s
law states that
v = Hd,
where H is known as Hubble’s constant. The following are distance (in millions of light-
years) and velocity (thousands of miles per second) measurements made on five galactic
clusters.
distance
68
137
315
405
700
velocity
2.4
4.7
12.0
14.4
26.0
State the regression model and estimate H.
4. The multiple linear regression model (5.6) can be viewed as a first-order approximation
of the general model
Y = g(x) + ε,
(5.42)
where E ε = 0

, Var ε = σ2, and g(x) is some known or unknown function of a d-
dimensional vector x of explanatory variables. To see this, replace g(x) with its first-order
Taylor approximation around some point x0 and write this as β0 + x⊤β. Express β0 and β
in terms of g and x0.
Regression
209
5. Table 5.6 shows data from an agricultural experiment where crop yield was measured
for two levels of pesticide and three levels of fertilizer. There are three responses for each
combination.
Table 5.6: Crop yields for pesticide and fertilizer combinations.
Fertilizer
Pesticide
Low
Medium
High
No
3.23, 3.20, 3.16
2.99, 2.85, 2.77
5.72, 5.77, 5.62
Yes
6.78, 6.73, 6.79
9.07, 9.09, 8.86
8.12, 8.04, 8.31
(a) Organize the data in standard form, where each row corresponds to a single meas-
urement and the columns correspond to the response variable and the two factor vari-
ables.
(b) Let Yijk be the response for the k-th replication at level i for factor 1 and level j
for factor 2. To assess which factors best explain the response variable, we use the
ANOVA model
Yi jk = µ + αi + β j + γi j + εi jk,
(5.43)
where P
i αi = P
j β j = P
i γij = P
j γi j = 0. Define β = [µ, α1, α2, β1, β2, β3, γ11, γ12,
γ13, γ21, γ22, γ23]⊤. Give the corresponding 18 × 12 model matrix.
(c) Note that the parameters are linearly dependent in this case. For example, α2 = −α1
and γ13 = −(γ11 + γ12). To retain only 6 linearly independent variables consider the
6-dimensional parameter vector eβ = [µ, α1, β1, β2, γ11, γ12]⊤. Find the matrix M such
that Meβ = β.
(d) Give the model matrix corresponding to eβ.
6. Show that for the birthweight data in Section 5.6.6.2 there is no significant decrease
in birthweight for smoking mothers. [Hint: create a new variable nonsmoke = 1−smoke,
which reverses the encoding for the smoking and non-smoking mothers. Then, the para-
meter β1 + β3 in the original model is the same as the parameter β1 in the model
Bwt = β0 + β1age + β2nonsmoke + β3age × nonsmoke + ε.
Now find a 95% for β3 and 

see if it contains zero.]
7. Prove (5.37) and (5.38).
8. In the Tobit regression
Tobit
regression
model with normally distributed errors, the response is modeled
as:
Yi =

Zi,
if ui < Zi
ui,
if Zi ⩽ui
,
Z ∼N(Xβ, σ2In),
where the model matrix X and the thresholds u1, . . . , un are given. Typically, ui = 0, i =
1, . . . , n. Suppose we wish to estimate θ := (β, σ2) via the Expectation–Maximization
method, similar to the censored data Example 4.2. Let y = [y1, . . . , yn]⊤be the vector
☞130
of observed data.
210
Exercises
(a) Show that the likelihood of y is:
g(y | θ) =
Y
i:yi>ui
φσ2(yi −x⊤
i β) ×
Y
i:yi=ui
Φ((ui −x⊤
i β)/σ),
where Φ is the cdf of the N(0, 1) distribution and φσ2 the pdf of the N(0, σ2) distribu-
tion.
(b) Let y and y be vectors that collect all yi > ui and yi = ui, respectively. Denote the
corresponding matrix of predictors by X and X, respectively. For each observation
yi = ui introduce a latent variable zi and collect these into a vector z. For the same
indices i collect the corresponding ui into a vector c. Show that the complete-data
likelihood is given by
g(y, z | θ) =
1
(2πσ2)n/2 exp
−∥y −Xβ∥2
2σ2
−∥z −Xβ∥2
2σ2
1{z ⩽c}.
(c) For the E-step, show that, for a fixed θ,
g(z | y, θ) =
Y
i
g(zi | y, θ),
where each g(zi | y, θ) is the pdf of the N((Xβ)i, σ2) distribution, truncated to the in-
terval (−∞, ci].
(d) For the M-step, compute the expectation of the complete log-likelihood
−n
2 ln σ2 −n
2 ln(2π) −∥y −Xβ∥2
2σ2
−E∥Z −Xβ∥2
2σ2
.
Then, derive the formulas for β and σ2 that maximize the expectation of the complete
log-likelihood.
9. Dowload data set WomenWage.csv from the book’s website. This data set is a tidied-up
version of the women’s wages data set from [91]. The first column of the data (hours) is
the response variable Y. It shows the hours spent in the labor force by married women in
the 1970s. We want to understand what factors determine the participation rate of women
in the labor force. The predictor variables are:
Tab

le 5.7: Features for the women’s wage data set.
Feature
Description
kidslt6
Number of children younger than 6 years.
kidsge6
Number of children older than 6 years.
age
Age of the married woman.
educ
Number of years of formal education.
exper
Number of years of “work experience”.
nwifeinc
Non-wife income, that is, the income of the husband.
expersq
The square of exper, to capture any nonlinear relationships.
Regression
211
We observe that some of the responses are Y = 0, that is, some women did not particip-
ate in the labor force. For this reason, we model the data using the Tobit regression model,
in which the response Y is given as:
Yi =

Zi,
if Zi > 0
0,
if Zi ⩽0 ,
Z ∼N(Xβ, σ2In).
With θ = (β, σ2), the likelihood of the data y = [y1, . . . , yn]⊤is:
g(y | θ) = Q
i:yi>0 φσ2(yi −x⊤
i β) × Q
i:yi=0 Φ((ui −x⊤
i β)/σ),
where Φ is the standard normal cdf. In Exercise 8, we derived the EM algorithm for max-
imizing the log-likelihood.
(a) Write down the EM algorithm in pseudo code as it applies to this Tobit regression.
(b) Implement the EM algorithm pseudo code in Python. Comment on which factor you
think is important in determining the labor participation rate of women living in the
USA in the 1970s.
10. Let P be a projection matrix. Show that the diagonal elements of P all lie in the interval
[0, 1]. In particular, for P = XX+ in Theorem 5.1, the leverage value pi := Pii satisfies
0 ⩽pi ⩽1 for all i.
11. Consider the linear model Y = Xβ + ε in (5.8), with X being the n × p model matrix
and ε having expectation vector 0 and covariance matrix σ2In. Suppose that bβ−i is the
least-squares estimate obtained by omitting the i-th observation, Yi; that is,
bβ−i = argmin
β
X
j,i
(Y j −x⊤
j β)2,
where x⊤
j is the j-th row of X. Let bY−i = x⊤
i bβ−i be the corresponding fitted value at xi. Also,
define Bi as the least-squares estimator of β based on the response data
Y(i) := [Y1, . . . , Yi−1, bY−i, Yi+1, . . . , Yn]⊤.
(a) Prove thatbβ−i = Bi; that is, the linear model o

btained from fitting all responses except
the i-th is the same as the one obtained from fitting the data Y(i).
(b) Use the previous result to verify that
Yi −bY−i = (Yi −bYi)/(1 −Pii),
where P = XX+ is the projection matrix onto the columns of X. Hence, deduce the
PRESS formula in Theorem 5.1.
☞174
12. Take the linear model Y = Xβ + ε,where X is an n × p model matrix, ε = 0, and
Cov(ε) = σ2In. Let P = XX+ be the projection matrix onto the columns of X.
(a) Using the properties of the pseudo-inverse (see Definition A.2), show that PP⊤= P.
☞360
212
Exercises
(b) Let E = Y −bY be the (random) vector of residuals, where bY = PY. Show that the i-th
residual has a normal distribution with expectation 0 and variance σ2(1 −Pii) (that is,
σ2 times 1 minus the i-th leverage).
(c) Show that σ2 can be unbiasedly estimated via
S 2 :=
1
n −p∥Y −bY∥2 =
1
n −p∥Y −Xbβ∥2.
(5.44)
[Hint: use the cyclic property of the trace as in Example 2.3.]
13. Consider a normal linear model Y = Xβ + ε, where X is an n × p model matrix and
ε ∼N(0, σ2In). Exercise 12 shows that for any such model the i-th standardized residual
Ei/(σ √1 −Pii) has a standard normal distribution. This motivates the use of the leverage
Pii to assess whether the i-th observation is an outlier depending on the size of the i-th
residual relative to √1 −Pii. A more robust approach is to include an estimate for σ using
all data except the i-th observation. This gives rise to the studentized residual
studentized
residual
Ti, defined
as
Ti :=
Ei
S −i
√1 −Pii
,
where S −i is an estimate of σ obtained by fitting all the observations except the i-th and
Ei = Yi −bYi is the i-th (random) residual. Exercise 12 shows that we can take, for example,
S 2
−i =
1
n −1 −p∥Y−i −X−ibβ−i∥2,
(5.45)
where X−i is the model matrix X with the i-th row removed, is an unbiased estimator of
σ2. We wish to compute S 2
−i efficiently, using S 2 in (5.44), as the latter will typically be
available once we have fitted the linear model. To this end, defin

e ui as the i-th unit vector
[0, . . . , 0, 1, 0, . . . , 0]⊤, and let
Y(i) := Y −(Yi −bY−i)ui = Y −
Ei
1 −Pii
ui,
where we have used the fact that Yi −bY−i = Ei/(1 −Pii), as derived in the proof of The-
orem 5.1. Now apply Exercise 11 to prove that
S 2
−i = (n −p) S 2 −E2
i /(1 −Pii)
n −p −1
.
14. Using the notation from Exercises 11–13, Cook’s distance
Cook’s distance
for observation i is defined
as
Di := ∥bY −bY
(i)∥2
p S 2
.
It measures the change in the fitted values when the i-th observation is removed, relative to
the residual variance of the model (estimated via S 2).
By using similar arguments as those in Exercise 13, show that
Di =
Pii E2
i
(1 −Pii)2p S 2.
It follows that there is no need to “omit and refit” the linear model in order to compute
Cook’s distance for the i-th response.
Regression
213
15. Prove that if we add an additional feature to the general linear model, then R2, the
coefficient of determination, is necessarily non-decreasing in value and hence cannot be
used to compare models with different numbers of predictors.
16. Let X := [X1, . . . , Xn]⊤and µ := [µ1, . . . , µn]⊤. In the fundamental Theorem C.9, we
use the fact that if Xi ∼N(µi, 1), i = 1, . . . , n are independent, then ∥X∥2 has (per definition)
a noncentral χ2
n distribution. Show that ∥X∥2 has moment generating function
et∥µ∥2/(1−2t)
(1 −2t)n/2,
t < 1/2,
and so the distribution of ∥X∥2 depends on µ only through the norm ∥µ∥.
17. Carry out a logistic regression analysis on a (partial) wine data set classification prob-
lem. The data can be loaded using the following code.
from sklearn import datasets
import numpy as np
data = datasets.load_wine()
X = data.data[:, [9,10]]
y = np.array(data.target==1,dtype=np.uint)
X = np.append(np.ones(len(X)).reshape(-1,1),X,axis=1)
The model matrix has three features, including the constant feature. Instead of using
Newton’s method (5.39) to estimate β, implement a simple gradient descent procedure
βt = βt−1 −α∇rτ(βt−1),
with learning rate α = 0

.0001, and run it for 106 steps. Your procedure should deliver three
coefficients; one for the intercept and the rest for the explanatory variables. Solve the same
problem using the Logit method of statsmodels.api and compare the results.
18. Consider again Example 5.10, where we train the learner via the Newton iteration
(5.39). If X⊤:= [x1, . . . , xn] defines the matrix of predictors and µt := h(Xβt), then the
☞206
gradient (5.37) and Hessian (5.38) for Newton’s method can be written as:
∇rτ(βt) = 1
nX⊤(µt −y)
and
H(βt) = 1
nX⊤DtX,
where Dt := diag(µt ⊙(1 −µt)) is a diagonal matrix. Show that the Newton iteration (5.39)
can be written as the iterative reweighted least-squares
iterative
reweighted
least squares
method:
βt = argmin
β
(eyt−1 −Xβ)⊤Dt−1(eyt−1 −Xβ),
where eyt−1 := Xβt−1 + D−1
t−1(y −µt−1) is the so-called adjusted response. [Hint: use the fact
that (M⊤M)−1M⊤z is the minimizer of ∥Mβ −z∥2.]
214
Exercises
19. In multi-output linear regression
multi-output
linear
regression
, the response variable is a real-valued vector of di-
mension, say, m. Similar to (5.8), the model can be written in matrix notation:
Y = XB +

ε⊤
1...
ε⊤
n
,
where:
• Y is an n × m matrix of n independent responses (stored as row vectors of length m);
• X is the usual n × p model matrix;
• B is an p × m matrix of model parameters;
• ε1, . . . , εn ∈Rm are independent error terms with E ε = 0 and E εε⊤= Σ.
We wish to learn the matrix parameters B and Σ from the training set {Y, X}. To this end,
consider minimizing the training loss:
1
ntr

(Y −XB) Σ−1 (Y −XB)⊤
,
where tr(·) is the trace of a matrix.
☞357
(a) Show that the minimizer of the training loss, denoted bB, satisfies the normal equa-
tions:
X⊤X bB = X⊤Y.
(b) Noting that
(Y −XB)⊤(Y −XB) =
n
X
i=1
εiε⊤
i ,
explain why
bΣ := (Y −XbB)⊤(Y −XbB)
n
is a method-of-moments estimator of Σ, just like the one given in (5.10).
CHAPTER6
REGULARIZATION AND KERNEL
METHODS
The purpose of this chapter is to familiarize t

he reader with two central concepts
in modern data science and machine learning: regularization and kernel methods. Reg-
ularization provides a natural way to guard against overfitting and kernel methods of-
fer a broad generalization of linear models. Here, we discuss regularized regression
(ridge, lasso) as a bridge to the fundamentals of kernel methods. We introduce repro-
ducing kernel Hilbert spaces and show that selecting the best prediction function in
such spaces is in fact a finite-dimensional optimization problem. Applications to spline
fitting, Gaussian process regression, and kernel PCA are given.
6.1
Introduction
In this chapter we return to the supervised learning setting of Chapter 5 (regression) and ex-
pand its scope. Given training data τ = {(x1, y1), . . . , (xn, yn)}, we wish to find a prediction
function (the learner) gτ that minimizes the (squared-error) training loss
ℓτ(g) = 1
n
n
X
i=1
(yi −g(xi))2
within a class of functions G. As noted in Chapter 2, if G is the set of all possible functions
then choosing any function g with the property that g(xi) = yi for all i will give zero training
loss, but will likely have poor generalization performance (that is, suffer from overfitting).
Recall from Theorem 2.1 that the best possible prediction function (over all g) for
☞
21
the squared-error risk E(Y −g(X))2 is given by g∗(x) = E[Y | X = x]. The class G should
be simple enough to permit theoretical understanding and analysis but, at the same time,
rich enough to contain the optimal function g∗(or a function close to g∗). This ideal can
be realized by taking G to be a Hilbert space
Hilbert space
(i.e., a complete inner product space) of
functions; see Appendix A.7.
☞384
Many of the classes of functions that we have encountered so far are in fact Hilbert
spaces. In particular, the set G of linear functions on Rp is a Hilbert space. To see this,
215
216
Regularization
identify with each element β ∈Rp the linear function gβ : x 7→x⊤β and define the inn

er
product on G as ⟨gβ, gγ⟩:= β⊤γ. In this way, G behaves in exactly the same way as (is
isomorphic to) the space Rp equipped with the Euclidean inner product (dot product). The
☞360
latter is a Hilbert space, because it is complete
complete
vector space
with respect to the Euclidean norm. See
Exercise 12 for a further discussion.
Let us now turn to our “running” polynomial regression Example 2.1, where the feature
☞26
vector x = [1, u, u2, . . . , up−1]⊤=: ϕ(u) is itself a vector-valued function of another feature
u. Then, the space of functions hβ : u 7→ϕ(u)⊤β is a Hilbert space, through the identifica-
tion hβ ≡β. In fact, this is true for any feature mapping ϕ : u 7→[ϕ1(u), . . . , ϕp(u)]⊤.
This can be further generalized by considering feature maps u 7→κu, where each κu
feature maps
is a real-valued function v 7→κu(v) on the feature space. As we shall soon see (in Sec-
tion 6.3), functions of the form u 7→P∞
i=1 βiκvi(u) live in a Hilbert space of functions called
a reproducing kernel Hilbert space (RKHS).
RKHS
In Section 6.3 we introduce the notion of a
RKHS formally, give specific examples, including the linear and Gaussian kernels, and de-
rive various useful properties, the most important of which is the representer Theorem 6.6.
Applications of such spaces include the smoothing splines (Section 6.6), Gaussian pro-
☞235
cess regression (Section 6.7), kernel PCA (Section 6.8), and support vector machines for
classification (Section 7.7).
☞269
The RKHS formalism also makes it easier to treat the important topic of regularization.
regularization
The aim of regularization is to improve the predictive performance of the best learner in
some class of functions G by adding a penalty term to the training loss that penalizes
learners that tend to overfit the data. In the next section we introduce the main ideas behind
regularization, which then segues into a discussion of kernel methods in the subsequent
sections.
6.2
Regularization
Let G be the Hilbert space of func

tions over which we search for the minimizer, gτ, of the
training loss ℓτ(g). Often, the Hilbert space G is rich enough so that we can find a learner
gτ within G such that the training loss is zero or close to zero. Consequently, if the space of
functions G is sufficiently rich, we run the risk of overfitting. One way to avoid overfitting
is to restrict attention to a subset of the space G by introducing a non-negative functional
J : G →R+ which penalizes complex models (functions). In particular, we want to find
functions g ∈G such that J(g) < c for some “regularization” constant c > 0. Thus we can
formulate the quintessential supervised learning problem as:
min {ℓτ(g) : g ∈G , J(g) < c} ,
(6.1)
the solution (argmin) of which is our learner. When this optimization problem is convex, it
can be solved by first obtaining the Lagrangian dual function
L∗(λ) := min
g∈G {ℓτ(g) + λ(J(g) −c)} ,
and then maximizing L∗(λ) with respect to λ ⩾0; see Section B.2.3.
☞407
In order to introduce the overall ideas of kernel methods and regularization, we will
proceed by exploring (6.1) in the special case of ridge regression
ridge
regression
, with the following run-
ning example.
Regularization and Kernel Methods
217
Example 6.1 (Ridge Regression) Ridge regression is simply linear regression with a
squared-norm penalty functional (also called a regularization function, or regularizer
regularizer
).
Suppose we have a training set τ = {(xi, yi), i = 1, . . . , n}, with each xi ∈Rp and we use a
squared-norm penalty with regularization parameter
regularization
parameter
γ > 0. Then, the problem is to solve
min
g∈G
1
n
n
X
i=1
(yi −g(xi))2 + γ ∥g∥2,
(6.2)
where G is the Hilbert space of linear functions on Rp. As explained in Section 6.1, we
can identify each g ∈G with a vector β ∈Rp and, consequently, ∥g∥2 = ⟨β, β⟩= ∥β∥2. The
above functional optimization problem is thus equivalent to the parametric optimization
problem
min
β∈Rp
1
n
n
X
i=1
 yi −x⊤
i β2 + γ ∥β∥2,
(6.3)
which, in the no

tation of Chapter 5, further simplifies to
min
β∈Rp
1
n ∥y −Xβ ∥2 + γ ∥β∥2.
(6.4)
In other words, the solution to (6.2) is of the form x 7→x⊤β∗, where β∗solves (6.3) (or
equivalently (6.4)). Observe that as γ →∞, the regularization term becomes dominant and
consequently the optimal g becomes identically zero.
The optimization problem in (6.4) is convex, and by multiplying by the constant n/2
and setting the gradient equal to zero, we obtain
X⊤(Xβ −y) + n γ β = 0.
(6.5)
If γ = 0 these are simply the normal equations, albeit written in a slightly different form.
☞
28
If the matrix X⊤X + n γIp is invertible (which is the case for any γ > 0; see Exercise 13),
then the solution to these modified normal equations is
bβ = (X⊤X + n γIp)−1X⊤y.
When using regularization with respect to some Hilbert space G, it is sometimes useful
to decompose G into two orthogonal subspaces, H and C say, such that every g ∈G can
be uniquely written as g = h + c, with h ∈H, c ∈C, and ⟨h, c⟩= 0. Such a G is said to be
the direct sum
direct sum
of C and H, and we write G = H ⊕C. Decompositions of this form become
useful when functions in H are penalized but functions in C are not. We illustrate this
decomposition with the ridge regression example where one of the features is a constant
term, which we do not wish to penalize.
Example 6.2 (Ridge Regression (cont.)) Suppose one of the features in Example 6.1
is the constant 1, which we do not wish to penalize. The reason for this is to ensure that
when γ →∞, the optimal g becomes the “constant” model, g(x) = β0, rather than the
“zero” model, g(x) = 0. Let us alter the notation slightly by considering the feature vectors
to be of the form ex = [1, x⊤]⊤, where x = [x1, . . . , xp]⊤. We thus have p + 1 features, rather
218
Regularization
than p. Let G be the space of linear functions of ex. Each linear function g of ex can be
written as g : ex 7→β0 + x⊤β, which is the sum of the constant function c : ex 7→β0 and
h : ex 7→x⊤β. Moreover, the two functio

ns are orthogonal with respect to the inner product
on G : ⟨c, h⟩= [β0, 0⊤][0, β⊤]⊤= 0, where 0 is a column vector of zeros.
As subspaces of G, both C and H are again Hilbert spaces, and their inner products and
norms follow directly from the inner product on G. For example, each function h : ex 7→
x⊤β in H has norm ∥h∥H = ∥β∥, and the constant function c : ex 7→β0 in C has norm |β0|.
The modification of the regularized optimization problem (6.2) where the constant term
is not penalized can now be written as
min
g∈H⊕C
1
n
n
X
i=1
(yi −g(exi))2 + γ ∥g∥2
H,
(6.6)
which further simplifies to
min
β0,β
1
n ∥y −β01 −Xβ ∥2 + γ ∥β∥2,
(6.7)
where 1 is the n×1 vector of 1s. Observe that, in this case, as γ →∞the optimal g tends to
the sample mean y of the {yi}; that is, we obtain the “default” regression model, without ex-
planatory variables. Again, this is a convex optimization problem, and the solution follows
from
X⊤(β01 + Xβ −y) + n γ β = 0,
(6.8)
with
n β0 = 1⊤(y −Xβ).
(6.9)
This results in solving for β from
(X⊤X −n−1X⊤11⊤X + n γ Ip)β = (X⊤−n−1X⊤11⊤)y,
(6.10)
and determining β0 from (6.9).
As a precursor to the kernel methods in the following sections, let us assume that n ⩾p
and that X has full (column) rank p. Then any vector β ∈Rp can be written as a linear
combination of the feature vectors {xi}; that is, as linear combinations of the columns of
the matrix X⊤. In particular, let β = X⊤α, where α = [α1, . . . , αn]⊤∈Rn. In this case (6.10)
reduces to
(XX⊤−n−111⊤XX⊤+ n γ In)α = (In −n−111⊤)y.
Assuming invertibility of (XX⊤−n−111⊤XX⊤+ n γ In), we have the solution
bα = (XX⊤−n−111⊤XX⊤+ n γ In)−1(In −n−111⊤)y,
which depends on the training feature vectors {xi} only through the n × n matrix of inner
products: XX⊤= [⟨xi, xj⟩]. This matrix is called the Gram matrix
Gram matrix
of the {xi}. From (6.9),
the solution for the constant term is bβ0 = n−11⊤(y −XX⊤bα). It follows that the learner is a
linear combination of inner products {⟨xi, x⟩} plus a constant:
gτ(ex) = bβ0 + x

⊤X⊤bα = bβ0 +
n
X
i=1
bαi ⟨xi, x⟩,
Regularization and Kernel Methods
219
where the coefficients bβ0 and bαi only depend on the inner products {⟨xi, x j⟩}. We will see
shortly that the representer Theorem 6.6 generalizes this result to a broad class of regular-
☞231
ized optimization problems.
We illustrate in Figure 6.1 how the solutions of the ridge regression problems appearing
in Examples 6.1 and 6.2 are qualitatively affected by the regularization parameter γ for a
simple linear regression model. The data was generated from the model yi = −1.5+0.5xi +
εi, i = 1, . . . , 100, where each xi is drawn independently and uniformly from the interval
[0, 10] and each εi is drawn independently from the standard normal distribution.
. = 0:1
-2
-1
0
1
2
-1
. = 1
. = 10
-2
0
2
-0
-2
-1
0
1
2
-1
-2
0
2
-0
-2
0
2
-0
Figure 6.1: Ridge regression solutions for a simple linear regression problem. Each panel
shows contours of the loss function (log scale) and the effect of the regularization parameter
γ ∈{0.1, 1, 10}, appearing in (6.4) and (6.7). Top row: both terms are penalized. Bottom
row: only the non-constant term is penalized. Penalized (plus) and unpenalized (diamond)
solutions are shown in each case.
The contours are those of the squared-error loss (actually the logarithm thereof), which
is minimized with respect to the model parameters β0 and β1. The diamonds all repres-
ent the same minimizer of this loss. The plusses show each minimizer [β∗
0, β∗
1]⊤of the
regularized minimization problems (6.4) and (6.7) for three choices of the regularization
parameter γ. For the top three panels the regularization involves both β0 and β1, through
the squared norm β2
0 + β2
1. The circles show the points that have the same squared norm as
220
Regularization
the optimal solution. For the bottom three panels only β1 is regularized; there, horizontal
lines indicate vectors [β0, β1]⊤for which |β1| = |β∗
1|.
The problem of ridge regression discussed in Example 6.2 boils down to solving a


problem of the form in (6.7), involving a squared 2-norm penalty ∥β∥2. A natural ques-
tion to ask is whether we can replace the squared 2-norm penalty by a different penalty
term. Replacing it with a 1-norm gives the lasso (least absolute shrinkage and selection
☞408
lasso
operator). The lasso equivalent of the ridge regression problem (6.7) is thus:
min
β0,β
1
n ∥y −β01 −Xβ ∥2 + γ ∥β∥1,
(6.11)
where ∥β∥1 = Pp
i=1 |βi|.
This is again a convex optimization problem. Unlike ridge regression, the lasso gener-
ally does not have an explicit solution, and so numerical methods must be used to solve it.
Note that the problem (6.11) is of the form
min
x,z
f(x) + g(z)
subject to
Ax + Bz = c,
(6.12)
with x := [β0, β⊤]⊤, z := β, A := [0p, Ip], B := −Ip, and c := 0p (vector of zeros), and
convex functions f(x) := 1
n ∥y −[1n, X] x ∥2 and g(z) := γ∥z∥1. There exist efficient al-
gorithms for solving such problems, including the alternating direction method of mul-
tipliers (ADMM) [17]. We refer to Example B.11 for details on this algorithm.
☞416
We repeat the examples from Figure 6.1, but now using lasso regression and taking
the square roots of the previous regularization parameters. The results are displayed in
Figure 6.2.
. =
p
0:1
-2
-1
0
1
2
-1
. = 1
. =
p
10
-2
0
2
-0
-2
-1
0
1
2
-1
-2
0
2
-0
-2
0
2
-0
Figure 6.2: Lasso regression solutions. Compare with Figure 6.1.
Regularization and Kernel Methods
221
One advantage of using the lasso regularization is that the resulting optimal parameter
vector often has several components that are exactly 0. For example, in the top middle
and right panels of Figure 6.2, the optimal solution lies exactly at a corner point of the
square {[β0, β1]⊤: |β0| + |β1| = |β∗
0| + |β∗
1|}; in this case β∗
0 = 0. For statistical models with
many parameters, the lasso can provide a methodology for model selection. Namely, as the
regularization parameter increases (or, equivalently, as the L1 norm of the optimal solution
decreases), the solution vect

or will have fewer and fewer non-zero parameters. By plotting
the values of the parameters for each γ or L1 one obtains the so-called regularization paths
regularization
paths
(also called homotopy paths or coefficient profiles) for the variables. Inspection of such
paths may help assess which of the model parameters are relevant to explain the variability
in the observed responses {yi}.
Example 6.3 (Regularization Paths) Figure 6.3 shows the regularization paths for p =
60 coefficients from a multiple linear regression model
☞169
Yi =
60
X
j=1
β j xi j + εi,
i = 1, . . . , 150,
where β j = 1 for j = 1, . . . , 10 and β j = 0 for j = 11, . . . , 60. The error terms {εi} are inde-
pendent and standard normal. The explanatory variables {xi j} were independently generated
from a standard normal distribution. As it is clear from the figure, the estimates of the 10
non-zero coefficients are first selected, as the L1 norm of the solutions increases. By the
time the L1 norm reaches around 4, all 10 variables for which β j = 1 have been correctly
identified and the remaining 50 parameters are estimated as exactly 0. Only after the L1
norm reaches around 8, will these “spurious” parameters be estimated to be non-zero. For
this example, the regularization parameter γ varied from 10−4 to 10.
0
5
10
15
L1 norm
-0.5
0
0.5
1
1.5
b-
Figure 6.3: Regularization paths for lasso regression solutions as a function of the L1 norm
of the solutions.
222
Reproducing Kernel Hilbert Spaces
6.3
Reproducing Kernel Hilbert Spaces
In this section, we formalize the idea outlined at the end of Section 6.1 of extending finite
dimensional feature maps to those that are functions by introducing a special type of Hil-
bert space of functions known as a reproducing kernel Hilbert space (RKHS). Although
the theory extends naturally to Hilbert spaces of complex-valued functions, we restrict
attention to Hilbert spaces of real-valued functions here.
To evaluate the loss of a learner g in some class of fun

ctions G, we do not need to expli-
citly construct g — rather, it is only required that we can evaluate g at all the feature vectors
x1, . . . , xn of the training set. A defining property of an RKHS is that function evaluation
at a point x can be performed by simply taking the inner product of g with some feature
function κx associated with x. We will see that this property becomes particularly useful
in light of the representer theorem (see Section 6.5), which states that the learner g itself
☞230
can be represented as a linear combination of the set of feature functions {κxi, i = 1, . . . , n}.
Consequently, we can evaluate a learner g at the feature vectors {xi} by taking linear com-
binations of terms of the form κ(xi, xj) = ⟨κxi, κx j⟩G. Collecting these inner products into
a matrix K = [κ(xi, xj), i, j = 1, . . . , n] (the Gram matrix of the {κxi}), we will see that the
feature vectors {xi} only enter the loss minimization problem through K.
Definition 6.1: Reproducing Kernel Hilbert Space
For a non-empty set X, a Hilbert space G of functions g : X →R with inner product
⟨·, ·⟩G is called a reproducing kernel Hilbert space
reproducing
kernel Hilbert
space
(RKHS) with reproducing kernel
κ : X × X →R if:
1. for every x ∈X, κx := κ(x, ·) is in G,
2. κ(x, x) < ∞for all x ∈X,
3. for every x ∈X and g ∈G, g(x) = ⟨g, κx⟩G.
The reproducing kernel of a Hilbert space of functions, if it exists, is unique; see Exer-
cise 2. The main (third) condition in Definition 6.1 is known as the reproducing property
reproducing
property
.
This property allows us to evaluate any function g ∈G at a point x ∈X by taking the inner
product of g and κx; as such, κx is called the representer of evaluation. Further, by taking
g = κx′ and applying the reproducing property, we have ⟨κx′, κx⟩G = κ(x′, x), and so by sym-
metry of the inner product it follows that κ(x, x′) = κ(x′, x). As a consequence, reproducing
kernels are necessarily symmetric functions. Moreover, a reproducing kernel κ is a 

positive
semidefinite
positive
semidefinite
function, meaning that for every n ⩾1 and every choice of α1, . . . , αn ∈R and
x1, . . . , xn ∈X, it holds that
n
X
i=1
n
X
j=1
αi κ(xi, xj) α j ⩾0.
(6.13)
In other words, every Gram matrix K associated with κ is a positive semidefinite matrix;
that is α⊤Kα ⩾0 for all α. The proof is addressed in Exercise 1.
The following theorem gives an alternative characterization of an RKHS. The proof
uses the Riesz representation Theorem A.17. Also note that in the theorem below we could
☞390
Regularization and Kernel Methods
223
have replaced the word “bounded” with “continuous”, as the two are equivalent for linear
functionals; see Theorem A.16.
Theorem 6.1: Continuous Evaluation Functionals Characterize a RKHS
An RKHS G on a set X is a Hilbert space in which every evaluation functional
evaluation
functional
δx : g 7→g(x) is bounded. Conversely, a Hilbert space G of functions X →R for
which every evaluation functional is bounded is an RKHS.
Proof: Note that, since evaluation functionals δx are linear operators, showing bounded-
ness is equivalent to showing continuity. Given an RKHS with reproducing kernel κ, sup-
pose that we have a sequence gn ∈G converging to g ∈G, that is ∥gn −g∥G →0. We apply
the Cauchy–Schwarz inequality (Theorem A.15) and the reproducing property of κ to find
☞389
that for every x ∈X and any n:
|δxgn −δxg| = |gn(x) −g(x)| = |⟨gn −g, κx⟩G| ⩽∥gn −g∥G ∥κx∥G = ∥gn −g∥G
p
⟨κx, κx⟩G
= ∥gn −g∥G
p
κ(x, x).
Noting that √κ(x, x) < ∞by definition for every x ∈X, and that ∥gn −g∥G →0 as n →∞,
we have shown continuity of δx, that is |δxgn −δxg| →0 as n →∞for every x ∈X.
Conversely, suppose that evaluation functionals are bounded. Then from the Riesz
representation Theorem A.17, there exists some gδx ∈G such that δxg = ⟨g, gδx⟩G for all
g ∈G — the representer of evaluation. If we define κ(x, x′) = gδx(x′) for all x, x′ ∈X, then
κx := κ(x, ·) = gδx is an element of G for every x ∈X and ⟨g, κx⟩G = δxg = g(x), so that the
re

producing property in Definition 6.1 is verified.
□
The fact that an RKHS has continuous evaluation functionals means that if two func-
tions g, h ∈G are “close” with respect to ∥· ∥G, then their evaluations g(x), h(x) are close
for every x ∈X. Formally, convergence in ∥· ∥G norm implies pointwise convergence for
all x ∈X.
The following theorem shows that any finite function κ : X × X →R can serve as a
reproducing kernel as long as it is finite, symmetric, and positive semidefinite. The cor-
responding (unique!) RKHS G is the completion of the set of all functions of the form
Pn
i=1 αi κxi where αi ∈R for all i = 1, . . . , n.
Theorem 6.2: Moore–Aronszajn
Given a non-empty set X and any finite symmetric positive semidefinite function
κ : X × X →R, there exists an RKHS G of functions g : X →R with reproducing
kernel κ. Moreover, G is unique.
Proof: (Sketch) As the proof of uniqueness is treated in Exercise 2, the objective is to
prove existence. The idea is to construct a pre-RKHS G0 from the given function κ that has
the essential structure and then to extend G0 to an RKHS G.
In particular, define G0 as the set of finite linear combinations of functions κx, x ∈X:
G0 :=

g =
n
X
i=1
αi κxi
 x1, . . . , xn ∈X, αi ∈R, n ∈N

.
224
Construction of Reproducing Kernels
Define on G0 the following inner product:
⟨f, g⟩G0 :=
* n
X
i=1
αi κxi,
m
X
j=1
β j κx′
j
+
G0
:=
n
X
i=1
m
X
j=1
αi βj κ(xi, x′
j).
Then G0 is an inner product space. In fact, G0 has the essential structure we require, namely
that (i) evaluation functionals are bounded/continuous (Exercise 4) and (ii) Cauchy se-
quences in G0 that converge pointwise also converge in norm (see Exercise 5).
We then enlarge G0 to the set G of all functions g : X →R for which there exists a
Cauchy sequence in G0 converging pointwise to g and define an inner product on G as the
limit
⟨f, g⟩G := lim
n→∞⟨fn, gn⟩G0,
(6.14)
where fn →f and gn →g. To show that G is an RKHS it remains to be shown that (1) this
inner product is 

well defined; (2) evaluation functionals remain bounded; and (3) the space
G is complete. A detailed proof is established in Exercises 6 and 7.
□
6.4
Construction of Reproducing Kernels
In this section we describe various ways to construct a reproducing kernel κ : X × X →
R for some feature space X. Recall that κ needs to be a finite, symmetric, and positive
semidefinite function (that is, it satisfies (6.13)). In view of Theorem 6.2, specifying the
space X and a reproducing kernel κ : X × X →R corresponds to uniquely specifying an
RKHS.
6.4.1
Reproducing Kernels via Feature Mapping
Perhaps the most fundamental way to construct a reproducing kernel κ is via a feature
map ϕ : X →Rp. We define κ(x, x′) := ⟨ϕ(x), ϕ(x′)⟩, where ⟨, ⟩denotes the Euclidean
inner product. The function is clearly finite and symmetric. To verify that κ is positive
semidefinite, let Φ be the matrix with rows ϕ(x1)⊤, . . . , ϕ(xn)⊤and let α = [α1, . . . , αn]⊤∈
Rn. Then,
n
X
i=1
n
X
j=1
αi κ(xi, xj) α j =
n
X
i=1
n
X
j=1
αi ϕ⊤(xi) ϕ(xj) α j = α⊤ΦΦ⊤α = ∥Φ⊤α∥2 ⩾0.
Example 6.4 (Linear Kernel) Taking the identity feature map ϕ(x) = x on X = Rp,
gives the linear kernel
linear kernel
κ(x, x′) = ⟨x, x′⟩= x⊤x′.
As can be seen from the proof of Theorem 6.2, the RKHS of functions corresponding to
the linear kernel is the space of linear functions on Rp. This space is isomorphic to Rp
itself, as discussed in the introduction (see also Exercise 12).
It is natural to wonder whether a given kernel function corresponds uniquely to a feature
map. The answer is no, as we shall see by way of example.
Regularization and Kernel Methods
225
Example 6.5 (Feature Maps and Kernel Functions) Let X = R and consider feature
maps ϕ1 : X →R and ϕ2 : X →R2, with ϕ1(x) := x and ϕ2(x) := [x, x]⊤/
√
2. Then
κϕ1(x, x′) = ⟨ϕ1(x), ϕ1(x′)⟩= xx′,
but also
κϕ2(x, x′) = ⟨ϕ2(x), ϕ2(x′)⟩= xx′.
Thus, we arrive at the same kernel function defined for the same underlying set X via two
different feature maps.
6.4.2
Kernels from Characteris

tic Functions
Another way to construct reproducing kernels on X = Rp makes use of the properties of
characteristic functions. In particular, we have the following result. We leave its proof as
☞441
Exercise 10.
Theorem 6.3: Reproducing Kernel from a Characteristic Function
Let X ∼µ be an Rp-valued random vector that is symmetric about the origin (that
is, X and −X are identically distributed), and let ψ be its characteristic function:
ψ(t) = E eit⊤X =
R
eit⊤x µ(dx) for t ∈Rp. Then κ(x, x′) := ψ(x −x′) is a valid repro-
ducing kernel on Rp.
Example 6.6 (Gaussian Kernel) The multivariate normal distribution with mean vec-
tor 0 and covariance matrix b2 Ip is clearly symmetric around the origin. Its characteristic
function is
ψ(t) = exp
 
−1
2b2 ∥t∥2
!
,
t ∈Rp.
Taking b2 = 1/σ2, this gives the popular Gaussian kernel
Gaussian
kernel
on Rp:
κ(x, x′) = exp
 
−1
2
∥x −x′∥2
σ2
!
.
(6.15)
The parameter σ is sometimes called the bandwidth
bandwidth
. Note that in the machine learning
literature, the Gaussian kernel is sometimes referred to as “the” radial basis function (rbf)
kernel
radial basis
function (rbf)
kernel
.1
From the proof of Theorem 6.2, we see that the RKHS G determined by the Gaussian
kernel κ is the space of pointwise limits of functions of the form
g(x) =
n
X
i=1
αi exp
 
−1
2
∥x −xi∥2
σ2
!
.
We can think of each point xi having a feature κxi that is a scaled multivariate Gaussian pdf
centered at xi.
1The term radial basis function is sometimes used more generally to mean kernels of the form κ(x, x′) =
f(∥x −x′∥) for some function f : R →R.
226
Construction of Reproducing Kernels
Example 6.7 (Sinc Kernel) The characteristic function of a Uniform[−1, 1] random
variable (which is symmetric around 0) is ψ(t) = sinc(t) := sin(t)/t, so κ(x, x′) = sinc(x−x′)
is a valid kernel.
Inspired by kernel density estimation (Section 4.4), we may be tempted to use the pdf
☞131
of a random variable that is symmetric about the origin to construct a reproducing kernel.
However,

 doing so will not work in general, as the next example illustrates.
Example 6.8 (Uniform pdf Does not Construct a Valid Reproducing Kernel) Take
the function ψ(t) = 1
21{|t| ⩽1}, which is the pdf of X ∼Uniform[−1, 1]. Unfortunately, the
function κ(x, x′) = ψ(x −x′) is not positive semidefinite, as can be seen for example by
constructing the matrix A = [κ(ti, tj), i, j = 1, 2, 3] for the points t1 = 0, t2 = 0.75, and
t3 = 1.5 as follows:
A =

ψ(0)
ψ(−0.75)
ψ(−1.5)
ψ(0.75)
ψ(0)
ψ(−0.75)
ψ(1.5)
ψ(0.75)
ψ(0)
=

0.5
0.5
0
0.5
0.5
0.5
0
0.5
0.5
.
The eigenvalues of A are {1/2 −√1/2, 1/2, 1/2 + √1/2} ≈{−0.2071, 0.5, 1.2071} and so
by Theorem A.9, A is not a positive semidefinite matrix, since it has a negative eigenvalue.
☞367
Consequently, κ is not a valid reproducing kernel.
One of the reasons why the Gaussian kernel (6.15) is popular is that it enjoys the uni-
versal approximation property
universal
approximation
property
[88]: the space of functions spanned by the Gaussian kernel
is dense in the space of continuous functions with support Z ⊂Rp. Naturally, this is a
desirable property especially if there is little prior knowledge about the properties of g∗.
However, note that every function g in the RKHS G associated with a Gaussian kernel κ is
infinitely differentiable. Moreover, a Gaussian RKHS does not contain non-zero constant
functions. Indeed, if A ⊂Z is non-empty and open, then the only function of the form
g(x) = c 1{x ∈A} contained in G is the zero function (c = 0).
Consequently, if it is known that g is differentiable only to a certain order, one may
prefer the Matérn kernel
Mat´ern kernel
with parameters ν, σ > 0:
κν(x, x′) = 21−ν
Γ(ν)
√
2ν ∥x −x′∥/σ
ν Kν
√
2ν ∥x −x′∥/σ

,
(6.16)
which gives functions that are (weakly) differentiable to order ⌊ν⌋(but not necessarily to
order ⌈ν⌉). Here, Kν denotes the modified Bessel function of the second kind; see (4.49).
The particular form of the Matérn kernel appearing in (6.1

6) ensures that limν→∞κν(x, x′) =
☞164
κ(x, x′), where κ is the Gaussian kernel appearing in (6.15).
We remark that Sobolev spaces are closely related to the Matérn kernel. Up to constants
(which scale the unit ball in the space), in dimension p and for a parameter s > p/2, these
spaces can be identified with ψ(t) = 21−s
Γ(s)∥t∥s−p/2Kp/2−s(∥t∥), which in turn can be viewed as
the characteristic function corresponding to the (radially symmetric) multivariate Student’s
t distribution with s degrees of freedom: that is, with pdf f(x) ∝(1 + ∥x∥2)−s.
☞162
Regularization and Kernel Methods
227
6.4.3
Reproducing Kernels Using Orthonormal Features
We have seen in Sections 6.4.1 and 6.4.2 how to construct reproducing kernels from feature
maps and characteristic functions. Another way to construct kernels on a space X is to work
directly from the function class L2(X; µ); that is, the set of square-integrable2 functions
on X with respect to µ; see also Definition A.4. For simplicity, in what follows, we will
☞385
consider µ to be the Lebesgue measure, and will simply write L2(X) rather than L2(X; µ).
We will also assume that X ⊆Rp.
Let {ξ1, ξ2, . . .} be an orthonormal basis of L2(X) and let c1, c2, . . . be a sequence of
positive numbers. As discussed in Section 6.4.1, the kernel corresponding to a feature map
ϕ : X →Rp is κ(x, x′) = ϕ(x)⊤ϕ(x′) = Pp
i=1 ϕi(x) ϕi(x′). Now consider a (possibly infinite)
sequence of feature functions ϕi = ci ξi, i = 1, 2, . . . and define
κ(x, x′) :=
X
i⩾1
ϕi(x) ϕi(x′) =
X
i⩾1
λi ξi(x) ξi(x′),
(6.17)
where λi = c2
i , i = 1, 2, . . .. This is well-defined as long as P
i⩾1 λi < ∞, which we assume
from now on. Let H be the linear space of functions of the form f = P
i⩾1 αiξi, where
P
i⩾1 α2
i /λi < ∞. As every function f ∈L2(X) can be represented as f = P
i⩾1⟨f, ξi⟩ξi, we
see that H is a linear subspace of L2(X). On H define the inner product
⟨f, g⟩H :=
X
i⩾1
⟨f, ξi⟩⟨g, ξi⟩
λi
.
With this inner product, the squared norm of f = P
i⩾1 αi ξi is ∥f∥2


H = P
i⩾1 α2
i /λi < ∞.
We show that H is actually an RKHS with kernel κ by verifying the conditions of Defini-
tion 6.1. First,
κx =
X
i⩾1
λi ξi(x) ξi ∈H,
as P
i λi < ∞by assumption, and so κ is finite. Second, the reproducing property holds.
Namely, let f = P
i⩾1 αi ξi. Then,
⟨κx, f⟩H =
X
i⩾1
⟨κx, ξi⟩⟨f, ξi⟩
λi
=
X
i⩾1
λi ξi(x) αi
λi
=
X
i⩾1
αiξi(x) = f(x).
The discussion above demonstrates that kernels can be constructed via (6.17). In fact,
(under mild conditions) any given reproducing kernel κ can be written in the form (6.17),
where this series representation enjoys desirable convergence properties. This result is
known as Mercer’s theorem, and is given below. We leave the full proof including the
precise conditions to, e.g., [40], but the main idea is that a reproducing kernel κ can be
thought of as a generalization of a positive semidefinite matrix K, and can also be writ-
ten in spectral form (see also Section A.6.5). In particular, by Theorem A.9, we can write
☞367
K = VDV⊤, where V is a matrix of orthonormal eigenvectors [vℓ] and D the diagonal
matrix of the (positive) eigenvalues [λℓ]; that is,
K(i, j) =
X
ℓ⩾1
λℓvℓ(i) vℓ( j).
2A function f : X →R is said to be square-integrable if
R
f 2(x) µ(dx) < ∞, where µ is a measure on X.
228
Construction of Reproducing Kernels
In (6.18) below, x, x′ play the role of i, j, and ξℓplays the role of vℓ.
Theorem 6.4: Mercer
Let κ : X × X →R be a reproducing kernel for a compact set X ⊂Rp. Then
(under mild conditions) there exists a countable sequence of non-negative numbers
{λℓ} decreasing to zero and functions {ξℓ} orthonormal in L2(X) such that
κ(x, x′) =
X
ℓ⩾1
λℓξℓ(x) ξℓ(x′) ,
for all x, x′ ∈X,
(6.18)
where (6.18) converges absolutely and uniformly on X × X.
Further, if λℓ> 0, then (λℓ, ξℓ) is an (eigenvalue, eigenfunction) pair for the integral
operator K : L2(X) →L2(X) defined by [K f](x) :=
R
X κ(x, y)f(y) dy for x ∈X.
Theorem 6.4 holds if (i) the kernel κ is continuous on X × X, (ii) the function eκ(x) :=
κ(x, x)

 defined for x ∈X is integrable. Extensions of Theorem 6.4 to more general spaces
X and measures µ hold; see, e.g., [115] or [40].
The key importance of Theorem 6.4 lies in the fact that the series representation (6.18)
converges absolutely and uniformly on X×X. The uniform convergence is a much stronger
condition than pointwise convergence, and means for instance that properties of the se-
quence of partial sums, such as continuity and integrability, are transferred to the limit.
Example 6.9 (Mercer) Suppose X = [−1, 1] and the kernel is κ(x, x′) = 1 + xx′ which
corresponds to the RKHS G of affine functions from X →R. To find the (eigenvalue,
eigenfunction) pairs for the integral operator appearing in Theorem 6.4, we need to find
numbers {λℓ} and orthonormal functions {ξℓ(x)} that solve
Z 1
−1
(1 + xx′) ξℓ(x′) dx′ = λℓξℓ(x) ,
for all x ∈[−1, 1].
Consider first a constant function ξ1(x) = c. Then, for all x ∈[−1, 1], we have that 2c = λ1c,
and the normalization condition requires that
R 1
−1 c2 dx = 1. Together, these give λ1 = 2 and
c = ±1/
√
2. Next, consider an affine function ξ2(x) = a + bx. Orthogonality requires that
Z 1
−1
c(a + bx) dx = 0,
which implies a = 0 (since c , 0). Moreover, the normalization condition then requires
Z 1
−1
b2x2 dx = 1,
or, equivalently, 2b2/3 = 1, implying b = ± √3/2. Finally, the integral equation reads
Z 1
−1
(1 + xx′) bx′ dx′ = λ2 bx ⇐⇒2bx
3
= λ2bx,
Regularization and Kernel Methods
229
implying that λ2 = 2/3. We take the positive solutions (i.e., c > 0 and b > 0), and note that
λ1 ξ1(x) ξ1(x′) + λ2 ξ2(x) ξ2(x′) = 2 1√
2
1√
2
+ 2
3
√
3
√
2
x
√
3
√
2
x′ = 1 + xx′ = κ(x, x′),
and so we have found the decomposition appearing in (6.18). As an aside, observe that ξ1
and ξ2 are orthonormal versions of the first two Legendre polynomials. The corresponding
☞387
feature map can be explicitly identified as ϕ1(x) =
√
λ1 ξ1(x) = 1 and ϕ2(x) = √λ2 ξ2(x) =
x.
6.4.4
Kernels from Kernels
The following theorem lists some useful properties for con

structing reproducing kernels
from existing reproducing kernels.
Theorem 6.5: Rules for Constructing Kernels from Other Kernels
1. If κ : Rp × Rp →R is a reproducing kernel and ϕ : X →Rp is a function, then
κ(ϕ(x), ϕ(x′)) is a reproducing kernel from X × X →R.
2. If κ : X × X →R is a reproducing kernel and f : X →R+ is a function, then
f(x)κ(x, x′) f(x′) is also a reproducing kernel from X × X →R.
3. If κ1 and κ2 are reproducing kernels from X × X →R, then so is their sum κ1 + κ2.
4. If κ1 and κ2 are reproducing kernels from X × X →R, then so is their product
κ1κ2.
5. If κ1 and κ2 are reproducing kernels from X × X →R and Y × Y →R re-
spectively, then κ+((x, y), (x′, y′)) := κ1(x, x′) + κ2(y, y′) and κ×((x, y), (x′, y′)) :=
κ1(x, x′)κ2(y, y′) are reproducing kernels from (X × Y) × (X × Y) →R.
Proof: For Rules 1, 2, and 3 it is easy to verify that the resulting function is finite, sym-
metric, and positive semidefinite, and so is a valid reproducing kernel by Theorem 6.2.
For example, for Rule 1 we have Pn
i=1
Pn
j=1 αi κ(yi, yj)α j ⩾0 for every choice of {αi}n
i=1
and {yi}n
i=1 ∈Rp, since κ is a reproducing kernel. In particular, it holds true for yi = ϕ(xi),
i = 1, . . . , n. Rule 4 is easy to show for kernels κ1, κ2 that admit a representation of the form
(6.17), since
κ1(x, x′) κ2(x, x′) =

X
i⩾1
ϕ(1)
i (x) ϕ(1)
i (x′)


X
j⩾1
ϕ(2)
j (x) ϕ(2)
j (x′)

=
X
i,j⩾1
ϕ(1)
i (x) ϕ(2)
j (x) ϕ(1)
i (x′) ϕ(2)
j (x′)
=
X
k⩾1
ϕk(x) ϕk(x′) =: κ(x, x′),
showing that κ = κ1κ2 also admits a representation of the form (6.17), where the new (pos-
sibly infinite) sequence of features (ϕk) is identified in a one-to-one way with the sequence
(ϕ(1)
i ϕ(2)
j ). We leave the proof of rule 5 as an exercise (Exercise 8).
□
230
Representer Theorem
Example 6.10 (Polynomial Kernel) Consider x, x′ ∈R2 with
κ(x, x′) = (1 + ⟨x, x′⟩)2,
where ⟨x, x′⟩= x⊤x′. This is an example of a polynomial kernel
polynomial
kernel
. Combining the fact that
sums and products of kerne

ls are again kernels (rules 3 and 4 of Theorem 6.5), we find that,
since ⟨x, x′⟩and the constant function 1 are kernels, so are 1 + ⟨x, x′⟩and (1 + ⟨x, x′⟩)2. By
writing
κ(x, x′) = (1 + x1x′
1 + x2x′
2)2
= 1 + 2x1x′
1 + 2x2x′
2 + 2x1x2x′
1x′
2 + (x1x′
1)2 + (x2x′
2)2,
we see that κ(x, x′) can be written as the inner product in R6 of the two feature vectors ϕ(x)
and ϕ(x′), where the feature map ϕ : R2 →R6 can be explicitly identified as
ϕ(x) = [1,
√
2x1,
√
2x2,
√
2x1x2, x2
1, x2
2]⊤.
Thus, the RKHS determined by κ can be explicitly identified with the space of functions
x 7→ϕ(x)⊤β for some β ∈R6.
In the above example we could explicitly identify the feature map. However, in general
a feature map need not be explicitly available. Using a particular reproducing kernel cor-
responds to using an implicit (possibly infinite dimensional!) feature map that never needs
to be explicitly computed.
6.5
Representer Theorem
Recall the setting discussed at the beginning of this chapter: we are given training data
τ = {(xi, yi)}n
i=1 and a loss function that measures the fit to the data, and we wish to find
a function g that minimizes the training loss, with the addition of a regularization term,
as described in Section 6.2. To do this, we assume first that the class G of prediction
functions can be decomposed as the direct sum of an RKHS H, defined by a kernel function
κ : X × X →R, and another linear space of real-valued functions H0 on X; that is,
G = H ⊕H0,
meaning that any element g ∈G can be written as g = h + h0, with h ∈H and h0 ∈H0.
In minimizing the training loss we wish to penalize the h term of g but not the h0 term.
Specifically, the aim is to solve the functional optimization problem
min
g∈H⊕H0
1
n
n
X
i=1
Loss(yi, g(xi)) + γ ∥g∥2
H.
(6.19)
Here, we use a slight abuse of notation: ∥g∥H means ∥h∥H if g = h + h0, as above. In this
way, we can view H0 as the null space of the functional g 7→∥g∥H. This null space may be
empty, but typically has a small dimension m; for ex

ample it could be the one-dimensional
space of constant functions, as in Example 6.2.
☞217
Regularization and Kernel Methods
231
Example 6.11 (Null Space) Consider again the setting of Example 6.2, for which we
have feature vectors ex = [1, x⊤]⊤and G consists of functions of the form g : ex 7→β0 + x⊤β.
Each function g can be decomposed as g = h + h0, where h : ex 7→x⊤β, and h0 : ex 7→β0.
Given g ∈G, we have ∥g∥H = ∥β∥, and so the null space H0 of the functional g 7→∥g∥H
(that is, the set of all functions g ∈G for which ∥g∥H = 0) is the set of constant functions
here, which has dimension m = 1.
Regularization favors elements in H0 and penalizes large elements in H. As the reg-
ularization parameter γ varies between zero and infinity, solutions to (6.19) vary from
“complex” (g ∈H ⊕H0) to “simple” (g ∈H0).
A key reason why RKHSs are so useful is the following. By choosing H to be an
RKHS in (6.19) this functional optimization problem effectively becomes a parametric
optimization problem. The reason is that any solution to (6.19) can be represented as a
finite-dimensional linear combination of kernel functions, evaluated at the training sample.
This is known as the kernel trick
kernel trick
.
Theorem 6.6: Representer Theorem
The solution to the penalized optimization problem (6.19) is of the form
g(x) =
n
X
i=1
αi κ(xi, x) +
m
X
j=1
η j qj(x),
(6.20)
where {q1, . . . , qm} is a basis of H0.
Proof: Let F = Span κxi, i = 1, . . . , n	. Clearly, F ⊆H. Then, the Hilbert space H can
be represented as H = F ⊕F ⊥, where F ⊥is the orthogonal complement of F . In other
words, F ⊥is the class of functions
{f ⊥∈H : ⟨f ⊥, f⟩H = 0, f ∈F } ≡{ f ⊥: ⟨f ⊥, κxi⟩H = 0, ∀i}.
It follows, by the reproducing kernel property, that for all f ⊥∈F ⊥:
f ⊥(xi) = ⟨f ⊥, κxi⟩H = 0,
i = 1, . . . , n.
Now, take any g ∈H ⊕H0, and write it as g = f + f ⊥+ h0, with f ∈F , f ⊥∈F ⊥, and
h0 ∈H0. By the definition of the null space H0, we have ∥g∥2
H = ∥f + f ⊥∥2
H. Moreover, by
Pythagoras’ theorem, the latte

r is equal to ∥f∥2
H + ∥f ⊥∥2
H. It follows that
1
n
n
X
i=1
Loss(yi, g(xi)) + γ∥g∥2
H = 1
n
n
X
i=1
Loss(yi, f(xi) + h0(xi)) + γ

∥f∥2
H + ∥f ⊥∥2
H

⩾1
n
n
X
i=1
Loss(yi, f(xi) + h0(xi)) + γ ∥f∥2
H.
Since we can obtain equality by taking f ⊥= 0, this implies that the minimizer of the pen-
alized optimization problem (6.19) lies in the subspace F ⊕H0 of G = H ⊕H0, and hence
is of the form (6.20).
□
232
Representer Theorem
Substituting the representation (6.20) of g into (6.19) gives the finite-dimensional op-
timization problem:
min
α∈Rn, η∈Rm
1
n
n
X
i=1
Loss(yi, (Kα + Qη)i) + γ α⊤Kα,
(6.21)
where
• K is the n × n (Gram) matrix with entries [κ(xi, xj), i = 1, . . . , n, j = 1, . . . , n].
• Q is the n × m matrix with entries [qj(xi), i = 1, . . . , n, j = 1, . . . , m].
In particular, for the squared-error loss we have
min
α∈Rn, η∈Rm
1
n
 y −(Kα + Qη)

2 + γ α⊤Kα.
(6.22)
This is a convex optimization problem, and its solution is found by differentiating (6.22)
with respect to α and η and equating to zero, leading to the following system of (n + m)
linear equations:
"KK⊤+ n γK
KQ
Q⊤K⊤
Q⊤Q
# "α
η
#
=
"K⊤
Q⊤
#
y.
(6.23)
As long as Q is of full column rank, the minimizing function is unique.
Example 6.12 (Ridge Regression (cont.)) We return to Example 6.2 and identify that
H is the RKHS with linear kernel function κ(x, x′) = x⊤x′ and C = H0 is the linear space of
constant functions. In this case, H0 is spanned by the function q1 ≡1. Moreover, K = XX⊤
and Q = 1.
If we appeal to the representer theorem directly, then the problem in (6.6) becomes, as
a result of (6.21):
min
α,η0
1
n
 y −η0 1 −XX⊤α

2 + γ ∥X⊤α∥2.
This is a convex optimization problem, and so the solution follows by taking derivatives
and setting them to zero. This gives the equations
XX⊤ (XX⊤+ n γ In) α + η0 1 −y = 0,
and
n η0 = 1⊤(y −XX⊤α).
Note that these are equivalent to (6.8) and (6.9) (once again assuming that n ⩾p and X has
full rank p). Equivalently, the solution is found by solving 

(6.23):
"XX⊤XX⊤+ n γ XX⊤
XX⊤1
1⊤XX⊤
n
# "α
η0
#
=
"XX⊤
1⊤
#
y.
This is a system of (n + 1) linear equations, and is typically of much larger dimension than
the (p + 1) linear equations given by (6.8) and (6.9). As such, one may question the prac-
ticality of reformulating the problem in this way. However, the benefit of this formulation
is that the problem can be expressed entirely through the Gram matrix K, without having
to explicitly compute the feature vectors — in turn permitting the (implicit) use of infinite
dimensional feature spaces.
Regularization and Kernel Methods
233
Example 6.13 (Estimating the Peaks Function) Figure 6.4 shows the surface plot of
the peaks function:
f(x1, x2) = 3(1 −x1)2e−x2
1−(x2+1)2 −10
x1
5 −x3
1 −x5
2

e−x2
1−x2
2 −1
3e−(x1+1)2−x2
2.
(6.24)
The goal is to learn the function y = f(x) based on a small set of training data (pairs of
(x, y) values). The red dots in the figure represent data τ = {(xi, yi)}20
i=1, where yi = f(xi) and
the {xi} have been chosen in a quasi-random
quasi-random
way, using Hammersley points (with bases 2
and 3) on the square [−3, 3]2. Quasi-random point sets have better space-filling properties
than either a regular grid of points or a set of pseudo-random points. We refer to [71] for
details. Note that there is no observation noise in this particular problem.
-5
2
-2
0
0
0
5
-2
2
Figure 6.4: Peaks function sampled at 20 Hammersley points.
The purpose of this example is to illustrate how, using the small data set of size n = 20,
the entire peaks function can be approximated well using kernel methods. In particular, we
use the Gaussian kernel (6.15) on R2, and denote by H the unique RKHS corresponding
to this kernel. We omit the regularization term in (6.19), and thus our objective is to find
the solution to
min
g∈H
1
n
n
X
i=1
(yi −g(xi))2.
By the representer theorem, the optimal function is of the form
g(x) =
n
X
i=1
αi exp
 
−1
2
∥x −xi∥2
σ2
!
,
where α := [α1, . . . , αn]⊤is, by (6.23), the solution to t

he set of linear equations KK⊤α =
Ky.
Note that we are performing regression over the class of functions H with an implicit
feature space. Due to the representer theorem, the solution to this problem coincides with
the solution to the linear regression problem for which the i-th feature (for i = 1, . . . , n) is
chosen to be the vector [κ(x1, xi), . . . , κ(xn, xi)]⊤.
234
Representer Theorem
The following code performs these calculations and gives the contour plots of g and
the peaks functions, shown in Figure 6.5. We see that the two are quite close. Code for the
generation of Hammersley points is available from the book’s GitHub site as genham.py.
peakskernel.py
from genham import hammersley
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm
from numpy.linalg import norm
import numpy as np
def peaks(x,y):
z =
(3*(1-x)**2 * np.exp(-(x**2) - (y+1)**2)
- 10*(x/5 - x**3 - y**5) * np.exp(-x**2 - y**2)
- 1/3 * np.exp(-(x+1)**2 - y**2))
return(z)
n = 20
x = -3 + 6*hammersley([2,3],n)
z = peaks(x[:,0],x[:,1])
xx, yy = np.mgrid[-3:3:150j,-3:3:150j]
zz = peaks(xx,yy)
plt.contour(xx,yy,zz,levels=50)
fig=plt.figure()
ax = fig.add_subplot(111,projection='3d')
ax.plot_surface(xx,yy,zz,rstride=1,cstride=1,color='c',alpha=0.3,
linewidth=0)
ax.scatter(x[:,0],x[:,1],z,color='k',s=20)
plt.show()
sig2 = 0.3 # kernel parameter
def k(x,u):
return(np.exp(-0.5*norm(x- u)**2/sig2))
K = np.zeros((n,n))
for i in range(n):
for j in range(n):
K[i,j] = k(x[i,:],x[j])
alpha = np.linalg.solve(K@K.T, K@z)
N, = xx.flatten().shape
Kx = np.zeros((n,N))
for i in range(n):
for j in range(N):
Kx[i,j] = k(x[i,:],np.array([xx.flatten()[j],yy.flatten()[j
]]))
g = Kx.T @ alpha
dim = np.sqrt(N).astype(int)
Regularization and Kernel Methods
235
yhat = g.reshape(dim,dim)
plt.contour(xx,yy,yhat,levels=50)
-2
0
2
-3
-2
-1
0
1
2
3
-2
0
2
Figure 6.5: Contour plots for the prediction function g (left) and the peaks function given
in (6.24) (right).


6.6
Smoothing Cubic Splines
A striking application of kernel methods is to fitting “well-behaved” functions to data.
Key examples of “well-behaved” functions are those that do not have large second-
order derivatives. Consider functions g : [0, 1] →R that are twice differentiable and define
∥g′′∥2 :=
R 1
0 (g′′(x))2 dx as a measure of the size of the second derivative.
Example 6.14 (Behavior of ∥g′′∥2) Intuitively, the larger ∥g′′∥2 is, the more “wiggly”
the function g will be. As an explicit example, consider g(x) = sin(ωx) for x ∈[0, 1], where
ω is a free parameter. We can explicitly compute g′′(x) = −ω2 sin(ωx), and consequently
∥g′′∥2 =
Z 1
0
ω4 sin2(ωx) dx = ω4
2 (1 −sinc(2ω)) .
As |ω| →∞, the frequency of g increases and we have ∥g′′∥2 →∞.
Now, in the context of data fitting, consider the following penalized least-squares op-
timization problem on [0, 1]:
min
g∈G
1
n
n
X
i=1
(yi −g(xi))2 + γ ∥g′′∥2,
(6.25)
where we will specify G in what follows. In order to apply the kernel machinery, we want
to write this in the form (6.19), for some RKHS H and null space H0. Clearly, the norm on
H should be of the form ∥g∥H = ∥g′′∥and should be well-defined (i.e., finite and ensuring
g and g′ are absolutely continuous). This suggests that we take
H = {g ∈L2[0, 1] : ∥g′′∥< ∞, g, g′ absolutely continuous, g(0) = g′(0) = 0},
236
Smoothing Cubic Splines
with inner product
⟨f, g⟩H :=
Z 1
0
f ′′(x) g′′(x) dx.
One rationale for imposing the boundary conditions g(0) = g′(0) = 0 is as follows: when
expanding g about the point x = 0, Taylor’s theorem (with integral remainder term) states
that
g(x) = g(0) + g′(0) x +
Z x
0
g′′(s) (x −s) ds.
Imposing the condition that g(0) = g′(0) = 0 for functions in H will ensure that G =
H ⊕H0 where the null space H0 contains only linear functions, as we will see.
To see that this H is in fact an RKHS, we derive its reproducing kernel. Using integra-
tion by parts (or directly from the Taylor expansion above), write
g(x) =
Z x
0
g′(s) ds =
Z x
0
g′

′(s) (x −s) ds =
Z 1
0
g′′(s) (x −s)+ ds.
If κ is a kernel, then by the reproducing property it must hold that
g(x) = ⟨g, κx⟩H =
Z 1
0
g′′(s) κ′′
x (s) ds,
so that κ must satisfy
∂2
∂s2κ(x, s) = (x −s)+, where y+ := max{y, 0}. Therefore, noting that
κ(x, u) = ⟨κx, κu⟩H, we have (see Exercise 15)
κ(x, u) =
Z 1
0
∂2κ(x, s)
∂s2
∂2κ(u, s)
∂s2
ds = max{x, u} min{x, u}2
2
−min{x, u}3
6
.
The last expression is a cubic function with quadratic and cubic terms that misses the
constant and linear monomials. This is not surprising considering the Taylor’s theorem
interpretation of a function g ∈H. If we now take H0 as the space of functions of the
following form (having zero second derivative):
h0 = η1 + η2 x,
x ∈[0, 1],
then (6.25) is exactly of the form (6.19).
As a consequence of the representer Theorem 6.6, the optimal solution to (6.25) is a
linear combination of piecewise cubic functions:
g(x) = η1 + η2 x +
n
X
i=1
αi κ(xi, x).
(6.26)
Such a function is called a cubic spline
cubic spline
with n knots (with one knot at each data point xi)
— so called, because the piecewise cubic function between knots is required to be “tied
together” at the knots. The parameters α, η are determined from (6.21) for instance by
solving (6.23) with matrices K = [κ(xi, xj)]n
i,j=1 and Q with i-th row of the form [1, xi] for
i = 1, . . . , n.
Regularization and Kernel Methods
237
Example 6.15 (Smoothing Spline) Figure 6.6 shows various cubic smoothing splines
for the data (0.05, 0.4), (0.2, 0.2), (0.5, 0.6), (0.75, 0.7), (1, 1). In the figure, we use the re-
parameterization r = 1/(1 + n γ) for the smoothing parameter. Thus r ∈[0, 1], where r = 0
means an infinite penalty for curvature (leading to the ordinary linear regression solution)
and r = 1 does not penalize curvature at all and leads to a perfect fit via the so-called nat-
ural spline. Of course the latter will generally lead to overfitting. For r from 0 up to 0.8 the
solutions will be close to the simple linear regression line, while

 only for r very close to 1,
the shape of the curve changes significantly.
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Figure 6.6: Various cubic smoothing splines for smoothing parameter r = 1/(1 + n γ) ∈
{0.8, 0.99, 0.999, 0.999999}. For r = 1, the natural spline through the data points is ob-
tained; for r = 0, the simple linear regression line is found.
The following code first computes the matrices K and Q, and then solves the linear
system (6.23). Finally, the smoothing curve is determined via (6.26), for selected points,
and then plotted. Note that the code plots only a single curve corresponding to the specified
value of p.
smoothspline.py
import matplotlib.pyplot as plt
import numpy as np
x = np.array([[0.05, 0.2, 0.5, 0.75, 1.]]).T
y = np.array([[0.4, 0.2, 0.6, 0.7, 1.]]).T
n = x.shape[0]
r = 0.999
ngamma = (1-r)/r
k = lambda x1, x2 : (1/2)* np.max((x1,x2)) * np.min((x1,x2)) ** 2 \
- ((1/6)* np.min((x1,x2))**3)
K = np.zeros((n,n))
for i in range(n):
for j in range(n):
238
Gaussian Process Regression
K[i,j] = k(x[i], x[j])
Q = np.hstack((np.ones((n,1)), x))
m1 = np.hstack((K @ K.T + (ngamma * K), K @ Q))
m2 = np.hstack((Q.T @ K.T, Q.T @ Q))
M = np.vstack((m1,m2))
c = np.vstack((K, Q.T)) @ y
ad = np.linalg.solve(M,c)
# plot the curve
xx = np.arange(0,1+0.01,0.01).reshape(-1,1)
g = np.zeros_like(xx)
Qx = np.hstack((np.ones_like(xx), xx))
g = np.zeros_like(xx)
N = np.shape(xx)[0]
Kx = np.zeros((n,N))
for i in range(n):
for j in range(N):
Kx[i,j] = k(x[i], xx[j])
g = g + np.hstack((Kx.T, Qx)) @ ad
plt.ylim((0,1.15))
plt.plot(xx, g, label = 'r = {}'.format(r), linewidth = 2)
plt.plot(x,y, 'b.', markersize=15)
plt.xlabel('$x$')
plt.ylabel('$y$')
plt.legend()
6.7
Gaussian Process Regression
Another application of the kernel machinery is to Gaussian process regression. A Gaussian
process
Gaussian
process
(GP) on a space X is a stochastic process {Zx, x ∈X} where, for any choice of
indices x1, . . . , xn, the vector [Zx1, . . . Zxn]⊤has a multivariate Gaussian distributi

on. As
such, the distribution of a GP is completely specified by its mean and covariance functions
µ : X →R and κ : X × X →R, respectively. The covariance function is a finite positive
semidefinite function, and hence, in view of Theorem 6.2, can be viewed as a reproducing
kernel on X.
As for ordinary regression, the objective of GP regression is to learn a regression func-
☞168
tion g that predicts a response y = g(x) for each feature vector x. This is done in a Bayesian
fashion, by establishing (1) a prior pdf for g and (2) the likelihood of the data, for a given
g. From these two we then derive, via Bayes’ formula, the posterior distribution of g given
the data. We refer to Section 2.9 for the general Bayesian framework.
☞48
A simple Bayesian model for GP regression is as follows. First, the prior distribution of
Regularization and Kernel Methods
239
g is taken to be the distribution of a GP with some known mean function µ and covariance
function (that is, kernel) κ. Most often µ is taken to be a constant, and for simplicity of
exposition, we take it to be 0. The Gaussian kernel (6.15) is often used for the covariance
function. For radial basis function kernels (including the Gaussian kernel), points that are
closer will be more highly correlated or “similar” [97], independent of translations in space.
Second, similar to standard regression, we view the observed feature vectors x1, . . . , xn
as fixed and the responses y1, . . . , yn as outcomes of random variables Y1, . . . , Yn. Specific-
ally, given g, we model the {Yi} as
Yi = g(xi) + εi ,
i = 1, . . . , n,
(6.27)
where {εi}
iid∼N(0, σ2). To simplify the analysis, let us assume that σ2 is known, so no prior
needs to be specified for σ2. Let g = [g(x1), . . . , g(xn)]⊤be the (unknown) vector of re-
gression values. Placing a GP prior on the function g is equivalent to placing a multivariate
Gaussian prior on the vector g:
g ∼N(0, K),
(6.28)
where the covariance matrix K of g is a Gram matrix (implicitly associ

ated with a feature
map through the kernel κ), given by:
K =

κ(x1, x1)
κ(x1, x2)
. . .
κ(x1, xn)
κ(x2, x1)
κ(x2, x2)
. . .
κ(x2, xn)
...
...
...
...
κ(xn, x1)
κ(xn, x2)
. . .
κ(xn, xn)

.
(6.29)
The likelihood of our data given g, denoted p(y | g), is obtained directly from the model
(6.27):
(Y | g) ∼N(g, σ2In).
(6.30)
Solving this Bayesian problem involves deriving the posterior distribution of (g | Y). To
do so, we first note that since Y has covariance matrix K + σ2In (which can be seen from
(6.27)), the joint distribution of Y and g is again normal, with mean 0 and covariance
matrix:
Ky,g =
"K + σ2In
K
K
K
#
.
(6.31)
The posterior can then be found by conditioning on Y = y, via Theorem C.8, giving
☞436
(g | y) ∼N

K⊤(K + σ2In)−1y, K −K⊤(K + σ2In)−1K

.
This only gives information about g at the observed points x1, . . . , xn. It is more interesting
to consider the posterior predictive distribution of eg := g(ex) for a new input ex. We can find
the corresponding posterior predictive pdf p(eg | y) by integrating out the joint posterior pdf
p(eg, g | y), which is equivalent to taking the expectation of p(eg | g) when g is distributed
according to the posterior pdf p(g | y); that is,
p(eg | y) =
Z
p(eg | g) p(g | y) dg.
240
Gaussian Process Regression
To do so more easily than direct evaluation via the above integral representation of p(eg | y),
we can begin with the joint distribution of [y⊤,eg]⊤, which is multivariate normal with mean
0 and covariance matrix
eK =
"K + σ2In
κ
κ⊤
κ(ex,ex)
#
,
(6.32)
where κ = [κ(ex, x1), . . . , κ(ex, xn)]⊤. It now follows, again by using Theorem C.8, that (eg | y)
has a normal distribution with mean and variance given respectively by
µ(ex) = κ⊤(K + σ2In)−1y
(6.33)
and
σ2(ex) = κ(ex,ex) −κ⊤(K + σ2In)−1κ.
(6.34)
These are sometimes called the predictive
predictive
mean and variance. It is important to note that
we are predicting the expected response EeY = g(ex) here, and not the actual response e

Y.
Example 6.16 (GP Regression) Suppose the regression function is
g(x) = 2 sin(2πx),
x ∈[0, 1].
We use GP regression to estimate g, using a Gaussian kernel of the form (6.15) with band-
width parameter 0.2. The explanatory variables x1, . . . , x30 were drawn uniformly on the
interval [0, 1], and the responses were obtained from (6.27), with noise level σ = 0.5. Fig-
ure 6.7 shows 10 samples from the prior distribution for g as well as the data points and
the true sinusoidal regression function g.
0
0.2
0.4
0.6
0.8
1
x
-3
-2
-1
0
1
2
3
y
0
0.2
0.4
0.6
0.8
1
x
-3
-2
-1
0
1
2
3
y
Figure 6.7: Left: samples drawn from the GP prior distribution. Right: the true regression
function with the data points.
Again assuming that the variance σ2, is known, the predictive distribution as determ-
ined by (6.33) and (6.34) is shown in Figure 6.8 for bandwidth 0.2 (left) and 0.02 (right).
Clearly, decreasing the bandwidth leads to the covariance between points x and x′ decreas-
ing at a faster rate with respect to the squared distance ∥x −x′∥2, leading to a predictive
mean that is less smooth.
Regularization and Kernel Methods
241
0
0.2
0.4
0.6
0.8
1
x
-3
-2
-1
0
1
2
3
y
g(x)
Predictive Mean
0
0.2
0.4
0.6
0.8
1
x
-3
-2
-1
0
1
2
3
y
g(x)
Predictive Mean
Figure 6.8: GP regression of synthetic data set with bandwidth 0.2 (left) and 0.02 (right).
The black dots represent the data and the blue curve is the latent function g(x) = 2 sin(2πx).
The red curve is the mean of the GP predictive distribution given by (6.33), and the shaded
region is the 95% confidence band, corresponding to the predictive variance given in (6.34).
In the above exposition, we have taken the mean function for the prior distribution
of g to be identically zero. If instead we have a general mean function m and write
m = [m(x1), . . . , m(xn)]⊤then the predictive variance (6.34) remains unchanged, and the
predictive mean (6.33) is modified to read
µ(ex) = m(ex) + κ⊤(K + σ2In)−1 (y −m) .
(6.35)
Typically, the variance

 σ2 appearing in (6.27) is not known, and the kernel κ itself
depends on several parameters — for instance a Gaussian kernel (6.15) with an unknown
bandwidth parameter. In the Bayesian framework, one typically specifies a hierarchical
model by introducing a prior p(θ) for the vector θ of such hyperparameters
hyperparamet-
ers
. Now, the
GP prior (g | θ) (equivalently, specifying p(g | θ)) and the model for the likelihood of the
data given Y|g, θ, namely p(y | g, θ), are both dependent on θ. The posterior distribution of
(g | y, θ) is as before.
One approach to setting the hyperparameter θ is to determine its posterior p(θ | y) and
obtain a point estimate, for instance via its maximum a posteriori estimate. However, this
can be a computationally demanding exercise. What is frequently done in practice is to
consider instead the marginal likelihood p(y | θ) and maximize this with respect to θ. This
procedure is called empirical Bayes
empirical Bayes
.
Considering again the mean function m to be identically zero, from (6.31), we have
that (Y | θ) is multivariate normal with mean 0 and covariance matrix Ky = K + σ2In,
immediately giving an expression for the marginal log-likelihood:
ln p(y | θ) = −n
2 ln(2π) −1
2 ln | det(Ky)| −1
2 y⊤K−1
y y.
(6.36)
We notice that only the second and third terms in (6.36) depend on θ. Considering a partial
242
Kernel PCA
derivative of (6.36) with respect to a single element θ of the hyperparameter vector θ yields
∂
∂θ ln p(y | θ) = −1
2tr
 
K−1
y
" ∂
∂θKy
#!
+ 1
2 y⊤K−1
y
" ∂
∂θKy
#
K−1
y y,
(6.37)
where
h
∂
∂θKy
i
is the element-wise derivative of matrix Ky with respect to θ. If these partial
derivatives can be computed for each hyperparameter θ, gradient information could be used
when maximizing (6.36).
Example 6.17 (GP Regression (cont.)) Continuing Example 6.16, we plot in Fig-
ure 6.9 the marginal log-likelihood as a function of the noise level σ and bandwidth para-
meter.
10-2
10-1
100
10-1
100
Figure 6.9: Contours of the marginal 

log-likelihood for the GP regression example. The
maximum is denoted by a cross.
The maximum is attained for a bandwidth parameter around 0.20 and σ ≈0.44, which
is very close to the left panel of Figure 6.8 for the case where σ was assumed to be known
(and equal to 0.5). We note here that the marginal log-likelihood is extremely flat, perhaps
owing to the small number of points.
6.8
Kernel PCA
In its basic form, kernel PCA (principal component analysis) can be thought of as PCA in
feature space. The main motivation for PCA introduced in Section 4.8 was as a dimension-
☞153
ality reduction technique. There, the analysis rested on an SVD of the matrix bΣ = 1
nX⊤X,
where the data in X was first centered via x′
i,j = xi,j −x j where xi = 1
n
Pn
i=1 xi,j.
What we shall do is to first re-cast the problem in terms of the Gram matrix K = XX⊤=
[⟨xi, xj⟩] (note the different order of X and X⊤), and subsequently replace the inner product
⟨x, x′⟩with κ(x, x′) for a general reproducing kernel κ. To make the link, let us start with
an SVD of X⊤:
X⊤= UDV⊤.
(6.38)
Regularization and Kernel Methods
243
The dimensions of X⊤, U, D, and V are d × n, d × d, d × n, and n × n, respectively. Then an
SVD of X⊤X is
X⊤X = (UDV⊤)(UDV⊤)⊤= U(DD⊤)U⊤
and an SVD of K is
K = (UDV⊤)⊤(UDV⊤) = V(D⊤D)V⊤.
Let λ1 ⩾· · · ⩾λr > 0 denote the non-zero eigenvalues of X⊤X (or, equivalently, of K) and
denote the corresponding r × r diagonal matrix by Λ. Without loss of generality we can
assume that the eigenvector of X⊤X corresponding to λk is the k-th column of U and that
the k-th column of V is an eigenvector of K. Similar to Section 4.8, let Uk and Vk contain
☞153
the first k columns of U and V, respectively, and let Λk be the corresponding k×k submatrix
of Λ, k = 1, . . . , r.
By the SVD (6.38), we have X⊤Vk = UDV⊤Vk = UkΛ1/2
k . Next, consider the projection
of a point x onto the k-dimensional linear space spanned by the columns of Uk — the first
k principal components. We saw in Section 4.8 that this proj

ection simply is the linear
mapping x 7→U⊤
k x. Using the fact that Uk = X⊤VkΛ−1/2, we find that x is projected to a
point z given by
z = Λ−1/2
k
V⊤
k Xx = Λ−1/2
k
V⊤
k κx,
where we have (suggestively) defined κx := [⟨x1, x⟩, . . . , ⟨xn, x⟩]⊤. The important point
is that z is completely determined by the vector of inner products κx and the k principal
eigenvalues and (right) eigenvectors of the Gram matrix K. Note that each component zm
of z is of the form
zm =
n
X
i=1
αm,i κ(xi, x),
m = 1, . . . , k.
(6.39)
The preceding discussion assumed centering of the columns of X. Consider now an
uncentered data matrix eX. Then the centered data can be written as X = eX −1
nEneX, where
En is the n × n matrix of ones. Consequently,
XX⊤= eXeX
⊤−1
nEneXeX
⊤−1
n
eXeX
⊤En + 1
n2EneXeX
⊤En,
or, more compactly, XX⊤= H eXeX
⊤H, where H = In−1
n1n1⊤
n , In is the n×n identity matrix,
and 1n is the n × 1 vector of ones.
To generalize to the kernel setting, we replace eXeX
⊤by K = [κ(xi, xj), i, j = 1, . . . , n]
and set κx = [κ(x1, x), . . . , κ(xn, x)]⊤, so that Λk is the diagonal matrix of the k largest eigen-
values of HKH and Vk is the corresponding matrix of eigenvectors. Note that the “usual”
PCA is recovered when we use the linear kernel κ(x, y) = x⊤y. However, instead of having
only kernels that are explicitly inner products of feature vectors, we are now permitted to
implicitly use infinite feature maps (functions) by using kernels.
Example 6.18 (Kernel PCA) We simulated 200 points, x1, . . . , x200, from the uniform
distribution on the set B1 ∪(B4 ∩Bc
3), where Br := {(x, y) ∈R2 : x2 + y2 ⩽r2} (disk with
radius r). We apply kernel PCA with Gaussian kernel κ(x, x′) = exp

−∥x −x′∥2
and
compute the functions zm(x), m = 1, . . . , 9 in (6.39). Their density plots are shown in Fig-
ure 6.10. The data points are superimposed in each plot. From this we see that the principal
components identify the radial structure present in the data. Finally, Figure 6.11 shows
244
Kernel PCA
t

he projections [z1(xi), z2(xi)]⊤, i = 1, . . . , 200 of the original data points onto the first two
principal components. We see that the projected points can be separated by a straight line,
whereas this is not possible for the original data; see also, Example 7.6 for a related prob-
☞272
lem.
Figure 6.10: First nine eigenfunctions using a Gaussian kernel for the two-dimensional
data set formed by the red and cyan points.
Regularization and Kernel Methods
245
-0.4
-0.2
0
0.2
0.4
0.6
0.8
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
Figure 6.11: Projection of the data onto the first two principal components. Observe that
already the projections of the inner and outer points are well separated.
Further Reading
For a good overview of the ridge regression and the lasso, we refer the reader to [36, 56].
For overviews of the theory of RKHS we refer to [3, 115, 126], and for in-depth background
on splines and their connection to RKHSs we refer to [123]. For further details on GP
regression we refer to [97] and for kernel PCA in particular we refer to [12, 92]. Finally,
many facts about kernels and their corresponding RKHSs can be found in [115].
Exercises
1. Let G be an RKHS with reproducing kernel κ. Show that κ is a positive semidefinite
function.
2. Show that a reproducing kernel, if it exists, is unique.
3. Let G be a Hilbert space of functions g : X →R. Recall that the evaluation func-
tional is the map δx : g 7→g(x) for a given x ∈X. Show that evaluation functionals
are linear operators.
4. Let G0 be the pre-RKHS G0 constructed in the proof of Theorem 6.2. Thus, g ∈G0
is of the form g = Pn
i=1 αi κxi and
⟨g, κx⟩G0 =
n
X
i=1
αi ⟨κxi, κx⟩G0 =
n
X
i=1
αi κ(xi, x) = g(x).
246
Exercises
Therefore, we may write the evaluation functional of g ∈G0 at x as δxg := ⟨g, κx⟩G0.
Show that δx is bounded on G0 for every x; that is, |δx f| < γ ∥f∥G0, for some γ < ∞.
5. Continuing Exercise 4, let (fn) be a Cauchy sequence in G0 such that | fn(x)| →0 for
all x. Show that ∥fn∥G0 →0.
6. Continuing 

Exercises 5 and 4, to show that the inner product (6.14) is well defined,
a number of facts have to be checked.
(a) Verify that the limit converges.
(b) Verify that the limit is independent of the Cauchy sequences used.
(c) Verify that the properties of an inner product are satisfied. The only non-trivial
property to verify is that ⟨f, f⟩G = 0 if and only if f = 0.
7. Exercises 4–6 show that G defined in the proof of Theorem 6.2 is an inner product
space. It remains to prove that G is an RKHS. This requires us to prove that the inner
product space G is complete (and thus Hilbert), and that its evaluation functionals
are bounded and hence continuous (see Theorem A.16). This is done in a number of
☞389
steps.
(a) Show that G0 is dense in G in the sense that every f ∈G is a limit point (with
respect to the norm on G) of a Cauchy sequence (fn) in G0.
(b) Show that every evaluation functional δx on G is continuous at the 0 function.
That is,
∀ε > 0 : ∃δ > 0 : ∀f ∈G : ∥f∥G < δ ⇒|f(x)| < ε.
(6.40)
Continuity of δx at all functions g ∈G then follows automatically from linearity.
(c) Show that G is complete; that is, every Cauchy sequence ( fn) ∈G converges in
the norm || · ||G.
8. If κ1 and κ2 are kernels on X and Y, then κ+((x, y), (x′, y′)) := κ1(x, x′) + κ2(y, y′)
and κ×((x, y), (x′, y′) := κ1(x, x′)κ2(y, y′) are kernels on the Cartesian product X × Y.
Prove this.
9. An RKHS enjoys the following desirable smoothness property: if (gn) is a sequence
belonging to RKHS G on X, and ∥gn −g∥G →0, then g(x) = limn gn(x) for all x ∈X.
Prove this, using Cauchy–Schwarz.
10. Let X be an Rd-valued random variable that is symmetric about the origin (that is,
X and (−X) are identically distributed). Denote by µ is its distribution and ψ(t) =
E eit⊤X =
R
eit⊤x µ(dx) for t ∈Rd is its characteristic function. Verify that κ(x, x′) =
ψ(x −x′) is a real-valued positive semidefinite function.
11. Suppose an RKHS G of functions from X →R (with kernel κ) is invariant under a
group T of transfor

mations T : X →X; that is, for all f, g ∈G and T ∈T , we have
(i) f ◦T ∈G and (ii) ⟨f ◦T, g ◦T⟩G = ⟨f, g⟩G. Show that κ(T x, T x′) = κ(x, x′) for
all x, x′ ∈X and T ∈T .
Regularization and Kernel Methods
247
12. Given two Hilbert spaces H and G, we call a mapping A : H →G a Hilbert space
isomorphism
Hilbert space
isomorphism
if it is
(i) a linear map; that is, A(af +bg) = aA(f)+bA(g) for any f, g ∈H and a, b ∈R.
(ii) a surjective map; and
(iii) an isometry; that is, for all f, g ∈H, it holds that ⟨f, g⟩H = ⟨Af, Ag⟩G.
Let H = Rp (equipped with the usual Euclidean inner product) and construct its
(continuous) dual space G, consisting of all continuous linear functions from Rp to
R, as follows: (a) For each β ∈Rp, define gβ : Rp →R via gβ(x) = ⟨β, x⟩= β⊤x, for
all x ∈Rp. (b) Equip G with the inner product ⟨gβ, gγ⟩G := β⊤γ.
Show that A : H →G defined by A(β) = gβ for β ∈Rp is a Hilbert space isomorph-
ism.
13. Let X be an n × p model matrix. Show that X⊤X + n γIp for γ > 0 is invertible.
14. As Example 6.8 clearly illustrates, the pdf of a random variable that is symmetric
about the origin is not in general a valid reproducing kernel. Take two such iid ran-
dom variables X and X′ with common pdf f, and define Z = X + X′. Denote by ψZ
and fZ the characteristic function and pdf of Z, respectively.
Show that if ψZ is in L1(R), fZ is a positive semidefinite function. Use this to show
that κ(x, x′) = fZ(x−x′) = 1{|x−x′| ⩽2}(1−|x−x′|/2) is a valid reproducing kernel.
15. For the smoothing cubic spline of Section 6.6, show that κ(x, u) = max{x,u} min{x,u}2
2
−
min{x,u}3
6
.
16. Let X be an n × p model matrix and let u ∈Rp be the unit-length vector with k-th
entry equal to one (uk = ∥u∥= 1). Suppose that the k-th column of X is v and that it
is replaced with a new predictor w, so that we obtain the new model matrix:
eX = X + (w −v)u⊤.
(a) Denoting
δ := X⊤(w −v) + ∥w −v∥2
2
u,
show that
eX
⊤eX = X⊤X + uδ⊤+ δu⊤= X⊤X + (u + δ)(u + δ)⊤
2
−(u −δ)(u −δ)⊤
2
.
In other words, eX
⊤eX di

ffers from X⊤X by a symmetric matrix of rank two.
(b) Suppose that B := (X⊤X + n γIp)−1 is already computed. Explain how the
Sherman–Morrison formulas in Theorem A.10 can be applied twice to com-
☞371
pute the inverse and log-determinant of the matrix eX
⊤eX + n γIp in O((n + p)p)
computing time, rather than the usual O((n + p2)p) computing time.3
3This Sherman–Morrison updating is not always numerically stable. A more numerically stable method
will perform two consecutive rank-one updates of the Cholesky decomposition of X⊤X + n γIp.
248
Exercises
(c) Write a Python program for updating a matrix B = (X⊤X + n γIp)−1 when we
change the k-th column of X, as shown in the following pseudo-code.
Algorithm 6.8.1: Updating via Sherman–Morrison Formula
input: Matrices X and B, index k, and replacement w for the k-th column of X.
output: Updated matrices X and B.
1 Set v ∈Rn to be the k-th column of X.
2 Set u ∈Rp to be the unit-length vector such that uk = ∥u∥= 1.
3 B ←B −Buδ⊤B
1 + δ⊤Bu
4 B ←B −Bδu⊤B
1 + u⊤Bδ
5 Update the k-th column of X with w.
6 return X, B
17. Use Algorithm 6.8.1 from Exercise 16 to write Python code that computes the ridge
regression coefficient β in (6.5) and use it to replicate the results on Figure 6.1. The
☞217
following pseudo-code (with running cost of O((n+ p)p2)) may help with the writing
of the Python code.
Algorithm 6.8.2: Ridge Regression Coefficients via Sherman–Morrison Formula
input: Training set {X, y} and regularization parameter γ > 0.
output: Solution bβ = (n γ Ip + X⊤X)−1X⊤y.
1 Set A to be an n × p matrix of zeros and B ←(n γ Ip)−1.
2 for j = 1, . . . , p do
3
Set w to be the j-th column of X.
4
Update {A, B} via Algorithm 6.8.1 with inputs {A, B, j, w}.
5 bβ ←B(X⊤y)
6 return bβ
18. Consider Example 2.10 with D = diag(λ1, . . . , λp) for some nonnegative vector λ ∈
☞56
Rp, so that twice the negative logarithm of the model evidence can be written as
−2 ln g(y) = l(λ) := n ln[y⊤(I −XΣX⊤)y] + ln |D| −ln |Σ| + c,
where c is a constant th

at depends only on n.
(a) Use the Woodbury identities (A.15) and (A.16) to show that
☞371
I −XΣX⊤= (I + XDX⊤)−1
ln |D| −ln |Σ| = ln |I + XDX⊤|.
Deduce that l(λ) = n ln[y⊤Cy] −ln |C| + c, where C := (I + XDX⊤)−1.
Regularization and Kernel Methods
249
(b) Let [v1, . . . , vp] := X denote the p columns/predictors of X. Show that
C−1 = I +
p
X
k=1
λkvkv⊤
k .
Explain why setting λk = 0 has the effect of excluding the k-th predictor from
the regression model. How can this observation be used for model selection?
(c) Prove the following formulas for the gradient and Hessian elements of l(λ):
∂l
∂λi
= v⊤
i Cvi −n(v⊤
i Cy)2
y⊤Cy
∂2l
∂λi ∂λ j
= (n −1)(v⊤
i Cvj)2 −n
v⊤
i Cvj −
(v⊤
i Cy)(v⊤
j Cy)
y⊤Cy

2
.
(6.41)
(d) One method to determine which predictors in X are important is to compute
λ∗:= argmin
λ⩾0
l(λ)
using, for example, the interior-point minimization Algorithm B.4.1 with gradi-
☞419
ent and Hessian computed from (6.41). Write Python code to compute λ∗and
use it to select the best polynomial model in Example 2.10.
19. (Exercise 18 continued.) Consider again Example 2.10 with D = diag(λ1, . . . , λp) for
☞
56
some nonnegative model-selection parameter λ ∈Rp. A Bayesian choice for λ is the
maximizer of the marginal likelihood g(y | λ); that is,
λ∗= argmax
λ⩾0
"
g(β, σ2, y | λ) dβ dσ2,
where
ln g(β, σ2, y | λ) = −∥y −Xβ∥2 + β⊤D−1β
2σ2
−1
2 ln |D| −n + p
2
ln(2πσ2) −ln σ2.
To maximize g(y | λ), one can use the EM algorithm with β and σ2 acting as latent
☞128
variables in the complete-data log-likelihood ln g(β, σ2, y | λ). Define
Σ := (D−1 + X⊤X)−1
β := ΣX⊤y
bσ2 :=

∥y∥2 −y⊤Xβ
 n.
(6.42)
(a) Show that the conditional density of the latent variables β and σ2 is such that

σ−2  λ, y

∼Gamma
n
2, n
2 bσ2

β
 λ, σ2, y

∼N

β, σ2Σ

.
250
Exercises
(b) Use Theorem C.2 to show that the expected complete-data log-likelihood is
☞430
−β
⊤D−1β
2bσ2
−tr(D−1Σ) + ln |D|
2
+ c1,
where c1 is a constant that does not depend on λ.
(c) Use Theorem A.2 to simplif

y the expected complete-data log-likelihood and to
☞359
show that it is maximized at λi = Σii + (βi/bσ)2 for i = 1, . . . , p. Hence, deduce
the following E and M steps in the EM algorithm:
E-step. Given λ, update (Σ, β, bσ2) via the formulas (6.42).
M-step. Given (Σ, β, bσ2), update λ via λi = Σii + (βi/bσ)2, i = 1, . . . , p.
(d) Write Python code to compute λ∗via the EM algorithm, and use it to select
the best polynomial model in Example 2.10. A possible stopping criterion is to
terminate the EM iterations when
ln g(y | λt+1) −ln g(y | λt) < ε
for some small ε > 0, where the marginal log-likelihood is
ln g(y | λ) = −n
2 ln(nπbσ2) −1
2 ln |D| + 1
2 ln |Σ| + ln Γ(n/2).
20. In this exercise we explore how the early stopping of the gradient descent iterations
(see Example B.10),
☞412
xt+1 = xt −α∇f(xt),
t = 0, 1, . . . ,
is (approximately) equivalent to the global minimization of f(x)+ 1
2γ∥x∥2 for certain
values of the ridge regularization parameter γ > 0 (see Example 6.1). We illustrate
the early stopping
early stopping
idea on the quadratic function f(x) = 1
2(x −µ)⊤H(x −µ), where
H ∈Rn×n is a symmetric positive-definite (Hessian) matrix with eigenvalues {λk}n
k=1.
(a) Verify that for a symmetric matrix A ∈Rn such that I −A is invertible, we have
I + A + · · · + At−1 = (I −At)(I −A)−1.
(b) Let H = QΛQ⊤be the diagonalization of H as per Theorem A.8. If x0 = 0,
☞366
show that the formula for xt is
xt = µ −Q(I −αΛ)tQ⊤µ.
Hence, deduce that a necessary condition for xt to converge is α < 2/ maxk λk.
(c) Show that the minimizer of f(x) + 1
2γ∥x∥2 can be written as
x∗= µ −Q(I + γ−1Λ)−1Q⊤µ.
(d) For a fixed value of t, let the learning rate α ↓0. Using part (b) and (c), show
that if γ ≃1/(t α) as α ↓0, then xt ≃x∗. In other words, xt is approximately
equal to x∗for small α, provided that γ is inversely proportional to t α.
CHAPTER7
CLASSIFICATION
The purpose of this chapter is to explain the mathematical ideas behind well-known
classification techniques such as the naïve B

ayes method, linear and quadratic discrim-
inant analysis, logistic/softmax classification, the K-nearest neighbors method, and
support vector machines.
7.1
Introduction
Classification methods are supervised learning methods in which a categorical response
variable Y takes one of c possible values (for example whether a person is sick or healthy),
which is to be predicted from a vector X of explanatory variables (for example, the blood
pressure, age, and smoking status of the person), using a prediction function g. In this
sense, g classifies the input X into one of the classes, say in the set {0, . . . , c −1}. For this
reason, we will call g a classification function or simply classifier
classifier
. As with any supervised
learning technique (see Section 2.3), the goal is to minimize the expected loss or risk
ℓ(g) = E Loss(Y, g(X))
(7.1)
for some loss function, Loss(y,by), that quantifies the impact of classifying a response y via
by = g(x). The natural loss function is the zero–one (also written 0–1) or indicator loss
indicator loss
:
Loss(y,by) := 1{y , by}; that is, there is no loss for a correct classification (y = by) and a
unit loss for a misclassification (y , by). In this case the optimal classifier g∗is given in the
following theorem.
Theorem 7.1: Optimal classifier
For the loss function Loss(y,by) = 1{y , by}, an optimal classification function is
g∗(x) = argmax
y∈{0,...,c−1}
P[Y = y | X = x].
(7.2)
Proof: The goal is to minimize ℓ(g) = E 1{Y , g(X)} over all functions g taking values in
{0, . . . , c −1}. Conditioning on X gives, by the tower property, ℓ(g) = E (P[Y , g(X) | X] ),
☞431
and so minimizing ℓ(g) with respect to g can be accomplished by maximizing P[Y =
251
252
Introduction
g(x) | X = x] with respect to g(x), for every fixed x. In other words, take g(x) to be equal
to the class label y for which P[Y = y | X = x] is maximal.
□
The formulation (7.2) allows for “ties”, when there is an equal probability between
optimal classes for a feature vec

tor x. Assigning one of these tied classes arbitrarily (or
randomly) to x does not affect the loss function and so we assume for simplicity that g∗(x)
is always a scalar value.
Note that, as was the case for the regression (see, e.g., Theorem 2.1), the optimal pre-
☞21
diction function depends on the conditional pdf f(y | x) = P[Y = y | X = x]. However, since
we assign x to class y if f(y | x) ⩾f(z | x) for all z, we do not need to learn the entire sur-
face of the function f(y | x); we only need to estimate it well enough near the decision
boundary {x : f(y | x) = f(z | x)} for any choice of classes y and z. This is because the as-
signment (7.2) divides the feature space into c regions, Ry = {x : f(y | x) = maxz f(z | x)},
y = 0, . . . , c −1.
Recall that for any supervised learning problem the smallest possible expected loss
(that is, the irreducible risk) is given by ℓ∗= ℓ(g∗). For the indicator loss, the irreducible
risk is equal to P[Y , g∗(X)]. This smallest possible probability of misclassification is
often called the Bayes error rate
Bayes error
rate
.
For a given training set τ, a classifier is often derived from a pre-classifier gτ, which
is a prediction function (learner) that can take any real value, rather than only values
in the set of class labels. A typical situation is the case of binary classification with
labels −1 and 1, where the prediction function gτ is a function taking values in the
interval [−1, 1] and the actual classifier is given by sign(gτ). It will be clear from
the context whether a prediction function gτ should be interpreted as a classifier or
pre-classifier.
The indicator loss function may not always be the most appropriate choice of loss
function for a given classification problem. For example, when diagnosing an illness, the
mistake in misclassifying a person as being sick when in fact the person is healthy may
be less serious than classifying the person as healthy when in fact the person is sick. In
Section 7.2 we consider vari

ous classification metrics.
There are many ways to fit a classifier to a training set τ = {(x1, y1), . . . , (xn, yn)}. The
approach taken in Section 7.3 is to use a Bayesian framework for classification. Here the
conditional pdf f(y | x) is viewed as a posterior pdf f(y | x) ∝f(x | y)f(y) for a given class
prior f(y) and likelihood f(x | y). Section 7.4 discusses linear and quadratic discriminant
analysis for classification, which assumes that the class of approximating functions for the
conditional pdf f(x | y) is a parametric class G of Gaussian densities. As a result of this
choice of G, the marginal f(x) is approximated via a Gaussian mixture density.
In contrast, in the logistic or soft-max classification in Section 7.5, the conditional
pdf f(y | x) is approximated using a more flexible class of approximating functions. As a
result of this, the approximation to the marginal density f(x) does not belong to a simple
parametric class (such as a Gaussian mixture). As in unsupervised learning, the cross-
entropy loss is the most common choice for training the learner.
The K-nearest neighbors method, discussed in Section 7.6, is yet another approach to
classification that makes minimal assumptions on the class G. Here the aim is to directly
Classification
253
estimate the conditional pdf f(y | x) from the training data, using only feature vectors in
the neighborhood of x. In Section 7.7 we explain the support vector methodology for clas-
sification; this is based on the same Reproducing Kernel Hilbert Space ideas that proved
successful for regression analysis in Section 6.3. Finally, a versatile way to do both clas-
☞222
sification and regression is to use classification and regression trees. This is the topic of
Chapter 8. Neural networks (Chapter 9) provide yet another way to perform classification.
☞287
☞323
7.2
Classification Metrics
The effectiveness of a classifier g is, theoretically, measured in terms of the risk (7.1), which
depends on the loss function use

d. Fitting a classifier to iid training data τ = {(xi, yi)}n
i=1 is
established by minimizing the training loss
ℓτ(g) = 1
n
n
X
i=1
Loss(yi, g(xi))
(7.3)
over some class of functions G. As the training loss is often a poor estimator of the risk,
the risk is usually estimated as in (7.3), using instead a test set τ′ = {(x′
i, y′
i)}n′
i=1} that is
independent of the training set, as explained in Section 2.3. To measure the performance
☞
23
of a classifier on a training or test set, it is convenient to introduce the notion of a loss
matrix
loss matrix
. Consider a classification problem with classifier g, loss function Loss, and classes
0, . . . , c −1. If an input feature vector x is classified as by = g(x) when the observed class
is y, the loss incurred is, by definition, Loss(y,by). Consequently, we may identify the loss
function with a matrix L = [Loss( j, k), j, k ∈{0, . . . , c −1}]. For the indicator loss function,
the matrix L has 0s on the diagonal and 1s everywhere else. Another useful matrix is the
confusion matrix
confusion
matrix
, denoted by M, where the ( j, k)-th element of M counts the number of
times that, for the training or test data, the actual (observed) class is j whereas the predicted
class is k. Table 7.1 shows the confusion matrix of some Dog/Cat/Possum classifier.
Table 7.1: Confusion matrix for three classes.
Predicted
Actual
Dog
Cat
Possum
Dog
30
2
6
Cat
8
22
15
Possum
7
4
41
We can now express the classifier performance (7.3) in terms of L and M as
1
n
X
j,k
[L ⊙M] jk,
(7.4)
where L ⊙M is the elementwise product of L and M. Note that for the indicator loss, (7.4)
is simply 1 −tr(M)/n, and is called the misclassification error. The expression (7.4) makes
misclassification
error
it clear that both the counts and the loss are important in determining the performance of a
classifier.
254
Classification Metrics
In the spirit of Table C.4 for hypothesis testing, it is sometimes useful to divide the
☞459
elements of a confusion matrix into four 

groups. The diagonal elements are the true positive
true positive
counts; that is, the numbers of correct classifications for each class. The true positive counts
for the Dog, Cat, and Possum classes in Table 7.1 are 30, 22, and 41, respectively. Similarly,
the true negative
true negative
count for a class is the sum of all matrix elements that do not belong to the
row or the column of this particular class. For the Dog class it is 22+15+4+41 = 82. The
false positive
false positive
count for a class is the sum of the corresponding column elements without
the diagonal element. For the Dog class it is 8 + 7 = 15. Finally, the false negative
false negative
count
for a specific class, can be calculated by summing over the corresponding row elements
(again, without counting the diagonal element). For the Dog class it is 2 + 6 = 8.
In terms of the elements of the confusion matrix, we have the following counts for class
j = 0, . . . , c −1:
True positive
tpj = Mj j,
False positive
fpj =
X
k, j
Mk j,
(column sum)
False negative
fnj =
X
k, j
Mjk,
(row sum)
True negative
tnj = n −fn j −fpj −tp j.
Note that in the binary classification case (c = 2), and using the indicator loss function,
the misclassification error (7.4) can be written as
error j =
fpj + fnj
n
.
(7.5)
This does not depend on which of the two classes is considered, as fp0 + fn0 = fp1 + fn1.
Similarly, the accuracy
accuracy
measures the fraction of correctly classified objects:
accuracy j = 1 −error j =
tpj + tn j
n
.
(7.6)
In some cases, classification error (or accuracy) alone is not sufficient to adequately
describe the effectiveness of a classifier. As an example, consider the following two classi-
fication problems based on a fingerprint detection system:
1. Identification of authorized personnel in a top-secret military facility.
2. Identification to get an online discount for some retail chain.
Both problems are binary classification problems. However, a false positive in the first
problem is extremely da

ngerous, while a false positive in the second problem will make
a customer happy. Let us examine a classifier in the top-secret facility. The corresponding
confusion matrix is given in Table 7.2.
Table 7.2: Confusion matrix for authorized personnel classification.
Predicted
Actual
authorized
non-authorized
authorized
100
400
non-authorized
50
100,000
Classification
255
From (7.6), we conclude that the accuracy of classification is equal to
accuracy =
tp + tn
tp + tn + fp + fn =
100 + 100, 000
100 + 100, 000 + 50 + 400 ≈99.55%.
However, we can see that in this particular case, accuracy is a problematic metric, since the
algorithm allowed 50 non-authorized personnel to enter the facility. One way to deal with
this issue is to modify the loss function to give a much higher loss to non-authorized access.
Thus, instead of an (indicator) loss matrix, we could for example take the loss matrix
L =
 0
1
1000
0
!
.
An alternative approach is to keep the indicator loss function and consider additional clas-
sification metrics. Below we give a list of commonly used metrics. For simplicity we call
an object whose actual class is j a “j-object”.
• The precision
precision
(also called positive predictive value) is the fraction of all objects
classified as j that are actually j-objects. Specifically,
precisionj =
tp j
tpj + fpj
.
• The recall
recall
(also called sensitivity) is the fraction of all j-objects that are correctly
classified as such. That is,
recallj =
tpj
tp j + fn j
.
• The specificity
specificity
measures the fraction of all non-j-objects that are correctly classified
as such. Specifically,
specificityj =
tn j
fpj + tnj
.
• The Fβ score
Fβ score
is a combination of the precision and the recall and is used as a single
measurement for a classifier’s performance. The Fβ score is given by
Fβ, j =
(β2 + 1) tpj
(β2 + 1) tpj + β2 fnj + fpj
.
For β = 0 we obtain the precision and for β →∞we obtain the recall.
The particular choice of metric is clearly application dependent. 

For example, in the
classification of authorized personnel in a top-secret military facility, suppose we have
two classifiers. The first (Classifier 1) has a confusion matrix given in Table 7.2, and the
second (Classifier 2) has a confusion matrix given in Table 7.3. Various metrics for these
two classifiers are show in Table 7.4. In this case we prefer Classifier 1, which has a much
higher precision.
256
Classification Metrics
Table 7.3: Confusion matrix for authorized personnel classification, using a different clas-
sifier (Classifier 2).
Predicted
Actual
Authorized
Non-Authorized
authorized
50
10
non-authorized
450
100,040
Table 7.4: Comparing the metrics for the confusion matrices in Tables 7.2 and 7.3.
Metric
Classifier 1
Classifier 2
accuracy
9.955 × 10−1
9.954 × 10−1
precision
6.667 × 10−1
1.000 × 10−1
recall
2.000 × 10−1
8.333 × 10−1
specificity
9.995 × 10−1
9.955 × 10−1
F1
3.077 × 10−1
1.786 × 10−1
Remark 7.1 (Multilabel and Hierarchical Classification) In standard classification
the classes are assumed to be mutually exclusive. For example a satellite image could
be classified as “cloudy”, “clear”, or “foggy”. In multilabel classification
multilabel
classification
the classes (often
called labels) do not have to be mutually exclusive. In this case the response is a subset
Y of some collection of labels {0, . . . , c −1}. Equivalently, the response can be viewed as
a binary vector of length c, where the y-th element is 1 if the response belongs to label y
and 0 otherwise. Again, consider the satellite image example and add two labels, such as
“road” and “river” to the previous three labels. Clearly, an image can contain both a road
and a river. In addition, the image can be clear, cloudy, or foggy.
In hierarchical classification
hierarchical
classification
a hierarchical relation between classes/labels is taken into
account during the classification process. Usually, the relations are modeled via a tree or a
directed acyclic graph. A visual comparison betw

een the hierarchical and non-hierarchical
(flat) classification tasks for satellite image data is presented in Figure 7.1.
root
rural
farm
barn
urban
skyscraper
root
rural
barn
farm
urban
skyscraper
Figure 7.1: Hierarchical (left) and non-hierarchical (right) classification schemes. Barns
and farms are common in rural areas, while skyscrapers are generally located in cities.
While this relation can be clearly observed in the hierarchical model scheme, the connec-
tion is missing in the non-hierarchical design.
In multilabel classification, both the prediction bY := g(x) and the true response Y are
subsets of the label set {0, . . . , c−1}. A reasonable metric is the so-called exact match ratio
exact match
ratio
,
Classification
257
defined as
exact match ratio =
Pn
i=1 1{bYi = Yi}
n
.
The exact match ratio is rather stringent, as it requires a full match. In order to consider
partial correctness, the following metrics could be used instead.
• The accuracy is defined as the ratio of correctly predicted labels and the total number
of predicted and actual labels. The formula is given by
accuracy =
Pn
i=1 |Yi ∩bYi|
Pn
i=1 |Yi ∪bYi|
.
• The precision is defined as the ratio of correctly predicted labels and the total number
of predicted labels. Specifically,
precision =
Pn
i=1 |Yi ∩bYi|
Pn
i=1 |bYi|
.
(7.7)
• The recall is defined as the ratio of correctly predicted labels and the total number of
actual labels. Specifically,
recall =
Pn
i=1 |Yi ∩bYi|
Pn
i=1 |Yi|
.
(7.8)
• The Hamming loss counts the average number of incorrect predictions for all classes,
calculated as
Hamming = 1
n c
n
X
i=1
c−1
X
y=0
1{y ∈bYi} 1{y < Yi} + 1{y < bYi} 1{y ∈Yi}.
7.3
Classification via Bayes’ Rule
We saw from Theorem 7.1 that the optimal classifier for classes 0, . . . , c −1 divides the
feature space into c regions, depending on f(y | x): the conditional pdf of the response Y
given the feature vector X = x. In particular, if f(y | x) > f(z | x) for all z , y, the feature
vector x is class

ified as y. Classifying feature vectors on the basis of their conditional class
probabilities is a natural thing to do, especially in a Bayesian learning context; see Sec-
tion 2.9 for an overview of Bayesian terminology and usage. Specifically, the conditional
☞
48
probability f(y | x) is interpreted as a posterior probability, of the form
f(y | x) ∝f(x | y)f(y),
(7.9)
where f(x | y) is the likelihood of obtaining feature vector x from class y and f(y) is the
prior probability1 of class y. By making various modeling assumptions about the prior
1Here we have used the Bayesian notation convention of “overloading” the notation f.
258
Classification via Bayes’ Rule
(e.g., all classes are a priori equally likely) and the likelihood function, one obtains the
posterior pdf via Bayes’ formula (7.9). A class by is then assigned to a feature vector x
according to the highest posterior probability; that is, we classify according to the Bayes
optimal decision rule
Bayes optimal
decision rule
:
by = argmax
y
f(y | x),
(7.10)
which is exactly (7.2). Since the discrete density f(y | x), y = 0, . . . , c −1 is usually not
known, the aim is to approximate it well with a function g(y | x) from some class of func-
tions G. Note that in this context, g(· | x) refers to a discrete density (a probability mass
function) for a given x.
Suppose a feature vector x = [x1, . . . , xp]⊤of p features has to be classified into one of
the classes 0, . . . , c −1. For example, the classes could be different people and the features
could be various facial measurements, such as the width of the eyes divided by the distance
between the eyes, or the ratio of the nose height and mouth width. In the naïve Bayes
na¨ive Bayes
method, the class of approximating functions G is chosen such that g(x | y) = g(x1 | y) · · ·
g(xp | y), that is, conditional on the label, all features are independent. Assuming a uniform
prior for y, the posterior pdf can thus be written as
g(y | x) ∝
p
Y
j=1
g(xj | y),
where the m

arginal pdfs g(x j | y), j = 1, . . . , p belong to a given class of approximating
functions G. To classify x, simply take the y that maximizes the unnormalized posterior
pdf.
For instance, suppose that the approximating class G is such that (Xj | y) ∼N(µyj, σ2),
y = 0, . . . , c −1, j = 1, . . . , p. The corresponding posterior pdf is then
g(y | θ, x) ∝exp
−1
2
p
X
j=1
(xj −µy j)2
σ2
= exp
−1
2
∥x −µy∥2
σ2
,
where µy := [µy1, . . . , µyp]⊤and θ := {µ0, . . . , µc−1, σ2} collects all model parameters. The
probability g(y | θ, x) is maximal when ∥x −µy∥is minimal. Thus by = argminy ∥x −µy∥is
the classifier that maximizes the posterior probability. That is, classify x as y when µy is
closest to x in Euclidean distance. Of course, the parameters (here, the {µy} and σ2) are
unknown and have to be estimated from the training data.
We can extend the above idea to the case where also the variance σ2 depends on the
class y and feature j, as in the next example.
Example 7.1 (Naïve Bayes Classification) Table 7.5 lists the means µ and standard de-
viations σ of p = 3 normally distributed features, for c = 4 different classes. How should
a feature vector x = [1.67, 2.00, 4.23]⊤be classified? The posterior pdf is
g(y | θ, x) ∝(σy1σy2σy3)−1 exp
−1
2
3
X
j=1
(x j −µy j)2
σ2
y j
,
where θ := {σj, µj}c−1
j=0 again collects all model parameters. The (unscaled) values for
g(y | θ, x), y = 0, 1, 2, 3 are 53.5, 0.24, 8.37, and 3.5 × 10−6, respectively. Hence, the feature
vector should be classified as 0. The code follows.
Classification
259
Table 7.5: Feature parameters.
Feature 1
Feature 2
Feature 3
Class
µ
σ
µ
σ
µ
σ
0
1.6
0.1
2.4
0.5
4.3
0.2
1
1.5
0.2
2.9
0.6
6.1
0.9
2
1.8
0.3
2.5
0.3
4.2
0.3
3
1.1
0.2
3.1
0.7
5.6
0.3
naiveBayes.py
import numpy as np
x = np.array([1.67,2,4.23]).reshape(1,3)
mu = np.array([1.6, 2.4, 4.3,
1.5, 2.9, 6.1,
1.8, 2.5, 4.2,
1.1, 3.1, 5.6]).reshape(4,3)
sig = np.array([0.1, 0.5, 0.2,
0.2, 0.6, 0.9,
0.3, 0.3, 0.3,
0.

2, 0.7, 0.3]).reshape(4,3)
g = lambda y: 1/np.prod(sig[y,:]) * np.exp(
-0.5*np.sum((x-mu[y,:])**2/sig[y,:]**2));
for y in range(0,4):
print('{:3.2e}'.format(g(y)))
5.35e+01
2.42e-01
8.37e+00
3.53e-06
7.4
Linear and Quadratic Discriminant Analysis
The Bayesian viewpoint for classification of the previous section (not limited to naïve
Bayes) leads in a natural way to the well-established technique of discriminant analysis
discriminant
analysis
.
We discuss the binary classification case first, with classes 0 and 1.
We consider a class of approximating functions G such that, conditional on the class
y ∈{0, 1}, the feature vector X = [X1, . . . , Xp]⊤has a N(µy, Σy) distribution (see (2.33)):
☞
46
g(x | θ, y) =
1
p(2π)p |Σy|
e−1
2 (x−µy)⊤Σ−1
y (x−µy),
x ∈Rp,
y ∈{0, 1},
(7.11)
where θ = {α j, µj, Σj}c−1
j=0 collects all model parameters, including the probability vector α
(that is, P
i αi = 1 and αi ⩾0) which helps define the prior density: g(y | θ) = αy, y ∈{0, 1}.
Then, the posterior density is
g(y | θ, x) ∝αy × g(x | θ, y),
260
Linear and Quadratic Discriminant Analysis
and, according to the Bayes optimal decision rule (7.10), we classify x to come from class
0 if α0g(x | θ, 0) > α1g(x | θ, 1) or, equivalently (by taking logarithms) if,
ln α0 −1
2 ln |Σ0| −1
2(x −µ0)⊤Σ−1
0 (x −µ0) > ln α1 −1
2 ln |Σ1| −1
2(x −µ1)⊤Σ−1
1 (x −µ1).
The function
δy(x) = ln αy −1
2 ln |Σy| −1
2(x −µy)⊤Σ−1
y (x −µy),
x ∈Rp
(7.12)
is called the quadratic discriminant function
quadratic
discriminant
function
for class y = 0, 1. A point x is classified to
class y for which δy(x) is largest. The function is quadratic in x and so the decision bound-
ary {x ∈Rp : δ0(x) = δ1(x)} is quadratic as well. An important simplification arises for the
case where the assumption is made that Σ0 = Σ1 = Σ. Now, the decision boundary is the
set of x for which
ln α0 −1
2(x −µ0)⊤Σ−1(x −µ0) = ln α1 −1
2(x −µ1)⊤Σ−1(x −µ1).
Expanding the above expression shows that the quadratic term in x is eliminated, giving a
line

ar decision boundary in x:
ln α0 −1
2µ⊤
0 Σ−1µ0 + x⊤Σ−1µ0 = ln α1 −1
2µ⊤
1 Σ−1µ1 + x⊤Σ−1µ1.
The corresponding linear discriminant function
linear
discriminant
function
for class y is
δy(x) = ln αy −1
2µ⊤
y Σ−1µy + x⊤Σ−1µy,
x ∈Rp.
(7.13)
Example 7.2 (Linear Discriminant Analysis) Consider the case where α0 = α1 = 1/2
and
Σ =
" 2
0.7
0.7
2
#
,
µ0 =
"0
0
#
,
µ1 =
"2
4
#
.
The distribution of X is a mixture of two bivariate normal distributions. Its pdf,
☞135
1
2g(x | θ, y = 0) + 1
2g(x | θ, y = 1),
is depicted in Figure 7.2.
Figure 7.2: A Gaussian mixture density where the two mixture components have the same
covariance matrix.
Classification
261
We used the following Python code to make this figure.
LDAmixture.py
import numpy as np, matplotlib.pyplot as plt
from scipy.stats import multivariate_normal
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.colors import LightSource
mu0, mu1 = np.array([0,0]), np.array([2,4])
Sigma = np.array([[2,0.7],[0.7, 2]])
x, y = np.mgrid[-4:6:150j,-5:8:150j]
mvn0 = multivariate_normal( mu0, Sigma )
mvn1 = multivariate_normal( mu1, Sigma )
xy = np.hstack((x.reshape(-1,1),y.reshape(-1,1)))
z = 0.5*mvn0.pdf(xy).reshape(x.shape) +
0.5*mvn1.pdf(xy).reshape(x.
shape)
fig = plt.figure()
ax = fig.gca(projection='3d')
ls = LightSource(azdeg=180, altdeg=65)
cols = ls.shade(z, plt.cm.winter)
surf = ax.plot_surface(x, y, z, rstride=1, cstride=1, linewidth=0,
antialiased=False, facecolors=cols)
plt.show()
The following Python code, which imports the previous code, draws a contour plot of
the mixture density, simulates 1000 data points from the mixture density, and draws the
decision boundary. To compute and display the linear decision boundary, let [a1, a2]⊤=
2Σ−1(µ1 −µ0) and b = µ⊤
0 Σ−1µ0 −µ⊤
1 Σ−1µ1. Then, the decision boundary can be written
as a1x1 + a2x2 + b = 0 or, equivalently, x2 = −(a1x1 + b)/a2. We see in Figure 7.3 that the
decision boundary nicely separates the two modes of the mixture density.
LDA.py
from LDAmixture import *
fro

m numpy.random import rand
from numpy.linalg import inv
fig = plt.figure()
plt.contourf(x, y,z, cmap=plt.cm.Blues, alpha= 0.9,extend='both')
plt.ylim(-5.0,8.0)
plt.xlim(-4.0,6.0)
M = 1000
r = (rand(M,1) < 0.5)
for i in range(0,M):
if r[i]:
u = np.random.multivariate_normal(mu0,Sigma ,1)
plt.plot(u[0][0],u[0][1],'.r',alpha = 0.4)
else:
u = np.random.multivariate_normal(mu1,Sigma ,1)
plt.plot(u[0][0],u[0][1],'+k',alpha = 0.6)
262
Linear and Quadratic Discriminant Analysis
a = 2*inv(Sigma) @ (mu1-mu0);
b = ( mu0.reshape(1,2) @ inv(Sigma) @ mu0.reshape(2,1)
- mu1.reshape(1,2) @ inv(Sigma) @mu1.reshape(2,1) )
xx = np.linspace(-4,6,100)
yy = (-(a[0]*xx +b)/a[1])[0]
plt.plot(xx,yy,'m')
plt.show()
4
2
0
2
4
6
4
2
0
2
4
6
8
Figure 7.3: The linear discriminant boundary lies between the two modes of the mixture
density and is linear.
To illustrate the difference between the linear and quadratic case, we specify different
covariance matrices for the mixture components in the next example.
Example 7.3 (Quadratic Discriminant Analysis) As in Example 7.2 we consider a
mixture of two Gaussians, but now with different covariance matrices. Figure 7.4 shows
the quadratic decision boundary. The Python code follows.
2
1
0
1
2
3
4
3
2
1
0
1
2
3
4
5
Figure 7.4: A quadratic decision boundary.
Classification
263
QDA.py
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal
mu1 = np.array([0,0])
mu2 = np.array([2,2])
Sigma1 = np.array([[1,0.3],[0.3, 1]])
Sigma2 = np.array([[0.3,0.3],[0.3, 1]])
x, y = np.mgrid[-2:4:150j,-3:5:150j]
mvn1 = multivariate_normal( mu1, Sigma1 )
mvn2 = multivariate_normal( mu2, Sigma2 )
xy = np.hstack((x.reshape(-1,1),y.reshape(-1,1)))
z = ( 0.5*mvn1.pdf(xy).reshape(x.shape) +
0.5*mvn2.pdf(xy).reshape(x.shape) )
plt.contour(x,y,z)
z1 = ( 0.5*mvn1.pdf(xy).reshape(x.shape) -
0.5*mvn2.pdf(xy).reshape(x.shape))
plt.contour(x,y,z1, levels=[0],linestyles ='dashed',
linewidths = 2, colors = 'm')
plt.show()
Of course, in practice the 

true parameter θ = {α j, Σj, µj}c
j=1 is not known and must be
estimated from the training data — for example, by minimizing the cross-entropy training
loss (4.4) with respect to θ:
☞123
1
n
n
X
i=1
Loss( f(xi, yi), g(xi, yi | θ)) = −1
n
n
X
i=1
ln g(xi, yi | θ),
where
ln g(x, y | θ) = ln αy −1
2 ln |Σy| −1
2 (x −µy)⊤Σ−1
y (x −µy) −p
2 ln(2π).
The corresponding estimates of the model parameters (see Exercise 2) are:
bαy = ny
n
bµy = 1
ny
X
i:yi=y
xi
bΣy = 1
ny
X
i:yi=y
(xi −bµy)(xi −bµy)⊤
(7.14)
for y = 0, . . . , c −1, where ny := Pn
i=1 1{yi = y}. For the case where Σy = Σ for all y, we
have bΣ = P
y bαy bΣy.
When c > 2 classes are involved, the classification procedure carries through in exactly
the same way, leading to quadratic and linear discriminant functions (7.12) and (7.13) for
each class. The space Rp now is partitioned into c regions, determined by the linear or
quadratic boundaries determined by each pair of Gaussians.
264
Linear and Quadratic Discriminant Analysis
For the linear discriminant case (that is, when Σy = Σ for all y), it is convenient to first
“whiten” or sphere the data
sphere the data
as follows. Let B be an invertible matrix such that Σ = BB⊤,
obtained, for example, via the Cholesky method. We linearly transform each data point x
☞373
to x′ := B−1x and each mean µy to µ′
y := B−1µy, y = 0, . . . , c −1. Let the random vector X
be distributed according to the mixture pdf
gX(x | θ) :=
X
y
αy
1
p(2π)p |Σy|
e−1
2 (x−µy)⊤Σ−1
y (x−µy).
Then, by the transformation Theorem C.4, the vector X′ = B−1X has density
☞433
gX′(x′ | θ) = gX(x | θ)
|B−1|
=
c−1
X
y=0
αy
√(2π)p e−1
2 (x−µy)⊤(BB⊤)−1(x−µy)
=
c−1
X
y=0
αy
√(2π)p e−1
2 (x′−µ′
y)⊤(x′−µ′
y) =
c−1
X
y=0
αy
√(2π)p e−1
2 ∥x′−µ′
y∥2.
This is the pdf of a mixture of standard p-dimensional normal distributions. The name
“sphering” derives from the fact that the contours of each mixture component are perfect
spheres. Classification of the transformed data is now particularly easy: classify x as by :=
ar

gminy{ ∥x′ −µ′
y∥2 −2 ln αy}. Note that this rule only depends on the prior probabilities and
the distance from x′ to the transformed means {µ′
y}. This procedure can lead to a significant
dimensionality reduction of the data. Namely, the data can be projected onto the space
spanned by the differences between the mean vectors {µ′
y}. When there are c classes, this
is a (c −1)-dimensional space, as opposed to the p-dimensional space of the original data.
We explain the precise ideas via an example.
Example 7.4 (Classification after Data Reduction) Consider an equal mixture of
three 3-dimensional Gaussian distributions with identical covariance matrices. After spher-
ing the data, the covariance matrices are all equal to the identity matrix. Suppose the mean
vectors of the sphered data are µ1 = [2, 1, −3]⊤, µ2 = [1, −4, 0]⊤, and µ3 = [2, 4, 6]⊤. The
left panel of Figure 7.5 shows the 3-dimensional (sphered) data from each of the three
classes.
4
2
0
2
4
6
4
2
0
2
4
5
4
3
2
1
0
1
2
6
4
2
0
2
4
6
4
2
0
2
4
6
Figure 7.5: Left: original data. Right: projected data.
The data are stored in three 1000×3 matrices X1, X2, and X3. Here is how the data was
generated and plotted.
Classification
265
datared.py
import numpy as np
from numpy.random import randn
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
n=1000
mu1 = np.array([2,1,-3])
mu2 = np.array([1,-4,0])
mu3 = np.array([2,4,0])
X1 = randn(n,3) + mu1
X2 = randn(n,3) + mu2
X3 = randn(n,3) + mu3
fig = plt.figure()
ax = fig.gca(projection='3d',)
ax.plot(X1[:,0],X1[:,1],X1[:,2],'r.',alpha=0.5,markersize=2)
ax.plot(X2[:,0],X2[:,1],X2[:,2],'b.',alpha=0.5,markersize=2)
ax.plot(X3[:,0],X3[:,1],X3[:,2],'g.',alpha=0.5,markersize=2)
ax.set_xlim3d(-4,6)
ax.set_ylim3d(-5,5)
ax.set_zlim3d(-5,2)
plt.show()
Since we have equal mixtures, we classify each data point x according to the closest
distance to µ1, µ2, or µ3. We can achieve a reduction in the dimensionality of the data by
projecting the data onto the two-dim

ensional affine space spanned by the {µi}; that is, all
vectors are of the form
µ1 + β1(µ2 −µ1) + β2(µ3 −µ1),
β1, β2 ∈R.
In fact, one may just as well project the data onto the subspace spanned by the vectors
µ21 = µ2 −µ1 and µ31 = µ3 −µ1. Let W = [µ21, µ31] be the 3 × 2 matrix whose columns
are µ21 and µ31. The orthogonal projection matrix onto the subspace W spanned by the
columns of W is (see Theorem A.4):
☞362
P = WW+ = W(W⊤W)−1W⊤.
Let UDV⊤be the singular value decomposition of W. Then P can also be written as
P = UD(D⊤D)−1D⊤U⊤.
Note that D has dimension 3 × 2, so is not square. The first two columns of U, say u1
and u2, form an orthonormal basis of the subspace W. What we want to do is rotate this
subspace to the x−y plane, mapping u1 and u2 to [1, 0, 0]⊤and [0, 1, 0]⊤, respectively. This
is achieved via the rotation matrix U−1 = U⊤, giving the skewed projection matrix
R = U⊤P = D(D⊤D)−1D⊤U⊤,
whose 3rd row only contains zeros. Applying R to all the data points, and ignoring the
3rd component of the projected points (which is 0), gives the right panel of Figure 7.5.
We see that the projected points are much better separated than the original ones. We have
achieved dimensionality reduction of the data while retaining all the necessary information
required for classification. Here is the rest of the Python code.
266
Logistic Regression and Softmax Classification
dataproj.py
from datared import *
from numpy.linalg import svd, pinv
mu21 = (mu2 - mu1).reshape(3,1)
mu31 = (mu3 - mu1).reshape(3,1)
W = np.hstack((mu21, mu31))
U,_,_ = svd(W)
# we only need U
P = W @ pinv(W)
R = U.T @ P
RX1 = (R @ X1.T).T
RX2 = (R @ X2.T).T
RX3 = (R @ X3.T).T
plt.plot(RX1[:,0],RX1[:,1],'b.',alpha=0.5,markersize=2)
plt.plot(RX2[:,0],RX2[:,1],'g.',alpha=0.5,markersize=2)
plt.plot(RX3[:,0],RX3[:,1],'r.',alpha=0.5,markersize=2)
plt.show()
7.5
Logistic Regression and Softmax Classification
In Example 5.10 we introduced the logistic (logit) regression model as a generalized linear
☞205
model wh

ere, conditional on a p-dimensonal feature vector x, the random response Y has
a Ber(h(x⊤β)) distribution with h(u) = 1/(1 + e−u). The parameter β was then learned from
the training data by maximizing the likelihood of the training responses or, equivalently,
by minimizing the supervised version of the cross-entropy training loss (4.4):
☞123
−1
n
n
X
i=1
ln g(yi | β, xi),
where g(y = 1 | β, x) = 1/(1 + e−x⊤β) and g(y = 0 | β, x) = e−x⊤β/(1 + e−x⊤β). In particular,
we have
ln g(y = 1 | β, x)
g(y = 0 | β, x) = x⊤β.
(7.15)
In other words, the log-odds ratio
log-odds ratio
is a linear function of the feature vector. As a con-
sequence, the decision boundary {x : g(y = 0 | β, x) = g(y = 1 | β, x)} is the hyperplane
x⊤β = 0. Note that x typically includes the constant feature. If the constant feature is con-
sidered separately, that is x = [1,ex⊤]⊤, then the boundary is an affine hyperplane in ex.
Suppose that training on τ = {(xi, yi)} yields the estimate bβ with the corresponding
learner gτ(y = 1 | x) = 1/(1 + e−x⊤bβ). The learner can be used as a pre-classifier from which
we obtain the classifier 1{gτ(y = 1 | x) > 1/2} or, equivalently,
by := argmax
j∈{0,1}
gτ(y = j | x),
in accordance with the fundamental classification rule (7.2).
The above classification methodology for the logit model can be generalized to the
multi-logit
multi-logit
model where the response takes values in the set {0, . . . , c −1}. The key idea is
Classification
267
to replace (7.15) with
ln g(y = j | W, b, x)
g(y = 0 | W, b, x) = x⊤βj,
j = 1, . . . , c −1,
(7.16)
where the matrix W ∈R(c−1)×(p−1) and vector b ∈Rc−1 reparameterize all βj ∈Rp such that
(recall x = [1,ex⊤]⊤):
Wex + b = [β1, . . . , βc−1]⊤x.
Observe that the random response Y is assumed to have a conditional probability distri-
bution for which the log-odds ratio with respect to class j and a “reference” class (in this
case 0) is linear. The separating boundaries between two pairs of classes are again affine
hyperplanes.
The model (7

.16) completely specifies the distribution of Y, namely:
g(y | W, b, x) =
exp(zy+1)
Pc
k=1 exp(zk),
y = 0, . . . , c −1,
where z1 is an arbitrary constant, say 0, corresponding to the “reference” class y = 0, and
[z2, . . . , zc]⊤:= Wex + b.
Note that g(y | W, b, x) is the (y + 1)-st component of a = softmax(z), where
softmax : z 7→
exp(z)
P
k exp(zk)
is the softmax
softmax
function and z = [z1, . . . , zc]⊤. Finally, we can write the classifier as
by = argmax
j∈{0,...,c−1}
aj+1.
In summary, we have the sequence of mappings transforming the input x into the outputby:
x →Wex + b →softmax(z) →argmax
j ∈{0,...,c−1}
aj+1 →by.
In Example 9.4 we will revisit the multi-logit model and reinterpret this sequence of map-
☞334
pings as a neural network. In the context of neural networks, W is called a weight matrix
and b is called a bias vector.
The parameters W and b have to be learned from the training data, which involves
minimization of the supervised version of the cross-entropy training loss (4.4):
☞123
1
n
n
X
i=1
Loss(f(yi | xi), g(yi | W, b, xi)) = −1
n
n
X
i=1
ln g(yi | W, b, xi).
Using the softmax function, the cross-entropy loss can be simplified to:
Loss( f(y | x), g(y | W, b, x)) = −zy+1 + ln
c
X
k=1
exp(zk).
(7.17)
The discussion on training is postponed until Chapter 9, where we reinterpret the multi-
logit model as a neural net, which can be trained using the limited-memory BFGS method
(Exercise 11). Note that in the binary case (c = 2), where there is only one vector β to
☞353
be estimated, Example 5.10 already established that minimization of the cross-entropy
training loss is equivalent to likelihood maximization.
268
K-Nearest Neighbors Classification
7.6
K-Nearest Neighbors Classification
Let τ = {(xi, yi)}n
i=1 be the training set, with yi ∈{0, . . . , c −1}, and let x be a new feature
vector. Define x(1), x(2), . . . , x(n) as the feature vectors ordered by closeness to x in some dis-
tance dist(x, xi), e.g., the Euclidean distance ∥x−x′∥. Let τ(x) := {

(x(1), y(1)) . . . , (x(K), y(K))}
be the subset of τ that contains K feature vectors xi that are closest to x. Then the K-nearest
neighbors
K-nearest
neighbors
classification rule classifies x according to the most frequently occurring class
labels in τ(x). If two or more labels receive the same number of votes, the feature vector
is classified by selecting one of these labels randomly with equal probability. For the case
K = 1 the set τ(x) contains only one element, say (x′, y′), and x is classified as y′. This
divides the space into n regions
Ri = {x : dist(x, xi) ⩽dist(x, xj), j , i},
i = 1, . . . , n.
For a feature space Rp with the Euclidean distance, this gives a Voronoi tessellation of the
feature space, similar to what was done for vector quantization in Section 4.6.
☞142
Example 7.5 (Nearest Neighbor Classification) The Python program below simulates
80 random points above and below the line x2 = x1. Points above the line x2 = x1 have
label 0 and points below this line have label 1. Figure 7.6 shows the Voronoi tessellation
obtained from the 1-nearest neighbor classification.
2
1
0
1
2
3
4
3
2
1
0
1
2
3
4
Figure 7.6: The 1-nearest neighbor algorithm divides up the space into Voronoi cells.
nearestnb.py
import numpy as np
from numpy.random import rand,randn
import matplotlib.pyplot as plt
from scipy.spatial import Voronoi , voronoi_plot_2d
Classification
269
np.random.seed(12345)
M = 80
x = randn(M,2)
y = np.zeros(M) # pre-allocate list
for i in range(M):
if rand() <0.5:
x[i,1], y[i] = x[i,0] + np.abs(randn()), 0
else:
x[i,1], y[i] = x[i,0] - np.abs(randn()), 1
vor = Voronoi(x)
plt_options = {'show_vertices':False, 'show_points':False,
'line_alpha':0.5}
fig = voronoi_plot_2d(vor, **plt_options)
plt.plot(x[y==0,0], x[y==0,1],'bo',
x[y==1,0], x[y==1,1],'rs', markersize=3)
7.7
Support Vector Machine
Suppose we are given the training set τ = {(xi, yi)}n
i=1, where each response2 yi takes either
the value −1 or 1, and we wish to construct a classifier taking val

ues in {−1, 1}. As this
merely involves a relabeling of the 0–1 classification problem in Section 7.1, the optimal
classification function for the indicator loss, 1{y , by}, is, by Theorem 7.1, equal to
g∗(x) =

1
if
P[Y = 1 | X = x] ⩾1/2,
−1
if
P[Y = 1 | X = x] < 1/2.
It is not difficult to show, see Exercise 5, that the function g∗can be viewed as the minimizer
of the risk for the hinge loss
hinge loss
function, Loss(y,by) = (1 −yby)+ := max{0, 1 −yby}, over all
prediction functions g (not necessarily taking values only in the set {−1, 1}). That is,
g∗= argmin
g
E (1 −Y g(X))+.
(7.18)
Given the training set τ, we can approximate the risk ℓ(g) = E (1 −Y g(X))+ with the train-
ing loss
ℓτ(g) = 1
n
n
X
i=1
(1 −yi g(xi))+,
and minimize this over a (smaller) class of functions to obtain the optimal prediction func-
tion gτ. Finally, as the prediction function gτ generally is not a classifier by itself (it usually
does not only take values −1 or 1), we take the classifier
sign gτ(x).
2The reason why we use responses −1 and 1 here, instead of 0 and 1, is that the notation becomes easier.
270
Support Vector Machine
Therefore, a feature vector x is classified according to 1 or −1 depending on whether
gτ(x) ⩾0 or < 0, respectively. The optimal decision boundary
optimal decision
boundary
is given by the set of x for
which gτ(x) = 0.
Similar to the cubic smoothing spline or RKHS setting in (6.19), we can consider find-
ing the best classifier, given the training data, via the penalized goodness-of-fit optimiza-
tion:
min
g∈H⊕H0
1
n
n
X
i=1
[1 −yi g(xi)]+ + eγ ∥g∥2
H,
for some regularization parameter eγ. It will be convenient to define γ := 2neγ and to solve
the equivalent problem
min
g∈H⊕H0
n
X
i=1
[1 −yi g(xi)]+ + γ
2 ∥g∥2
H.
We know from the Representer Theorem 6.6 that if κ is the reproducing kernel cor-
☞231
responding to H, then the solution is of the form (assuming that the null space H0 has a
constant term only):
g(x) = α0 +
n
X
i=1
αi κ(xi, x).
(7.19)
Substitu

ting into the minimization expression yields the analogue of (6.21):
☞232
min
α,α0
n
X
i=1
[1 −yi(α0 + {Kα}i)]+ + γ
2 α⊤Kα,
(7.20)
where K is the Gram matrix. This is a convex optimization problem, as it is the sum of a
convex quadratic and piecewise linear term in α. Defining λi := γαi/yi, i = 1, . . . , n and
λ := [λ1, . . . , λn]⊤, we show in Exercise 10 that the optimal α and α0 in (7.20) can be
obtained by solving the “dual” convex optimization problem
max
λ
n
X
i=1
λi −1
2γ
n
X
i=1
n
X
j=1
λiλ jyiy j κ(xi, xj)
subject to:
λ⊤y = 0, 0 ⩽λ ⩽1,
(7.21)
and α0 = y j −P
i=1 αi κ(xi, xj) for any j for which λj ∈(0, 1). In view of (7.19), the optimal
prediction function (pre-classifier) gτ is then given by
gτ(x) = α0 +
n
X
i=1
αi κ(xi, x) = α0 + 1
γ
n
X
i=1
yiλi κ(xi, x).
(7.22)
To mitigate possible numerical problems in the calculation of α0 it is customary to take
an overall average:
α0 = 1
|J|
X
j∈J
y j −
n
X
i=1
αi κ(xi, xj)
,
where J := {j : λ j ∈(0, 1)}.
Classification
271
Note that, from (7.22), the optimal pre-classifier g(x) and the classifier sign g(x) only
depend on vectors xi for which λi , 0. These vectors are called the support vectors
support vectors
of the
support vector machine. It is also important to note that the quadratic function in (7.21)
depends on the regularization parameter γ. By defining νi := λi/γ, i = 1, . . . , n, we can
rewrite (7.21) as
min
ν
1
2
X
i,j
νiν jyiyj κ(xi, xj) −
n
X
i=1
νi
subject to:
n
X
i=1
νiyi = 0, 0 ⩽νi ⩽1/γ =: C,
i = 1, . . . , n.
(7.23)
For perfectly separable data, that is, data for which an affine plane can be drawn to perfectly
separate the two classes, we may take C = ∞, as explained below. Otherwise, C needs to
be chosen via cross-validation or a test data set, for example.
Geometric interpretation
For the linear kernel function κ(x, x′) = x⊤x′, we have
gτ(x) = β0 + β⊤x,
with β0 = α0 and β = γ−1 Pn
i=1 λiyixi = Pn
i=1 αixi, and so the decision boundary is an affine
plane. The situation is illustrated

 in Figure 7.7. The decision boundary is formed by the
points x such that gτ(x) = 0. The two sets {x : gτ(x) = −1} and {x : gτ(x) = 1} are called
the margins. The distance from the points on a margin to the decision boundary is 1/∥β∥.
1
2
3
Figure 7.7: Classifying two classes (red and blue) using SVM.
Based on the “multipliers” {λi}, we can divide the training samples {(xi, yi)} into three
categories (see Exercise 11):
• Points for which λi ∈(0, 1). These are the support vectors on the margins (green
encircled in the figure) and are correctly classified.
272
Support Vector Machine
• Points for which λi = 1. These points, which are also support vectors, lie strictly
inside the margins (points 1, 2, and 3 in the figure). Such points may or may not be
correctly classified.
• Points for which λi = 0. These are the non-support vectors, which all lie outside the
margins. Every such point is correctly classified.
If the classes of points {xi : yi = 1} and {xi : yi = −1} are perfectly separable by some
affine plane, then there will be no points strictly inside the margins, so all support vectors
will lie exactly on the margins. In this case (7.20) reduces to
min
β,β0 ∥β∥2
subject to:
yi(β0 + x⊤
i β) ⩾1, i = 1, . . . , n,
(7.24)
using the fact that α0 = β0 and Kα = XX⊤α = Xβ. We may replace min ∥β∥2 in (7.24) with
max 1/∥β∥, as this gives the same optimal solution. As 1/∥β∥is equal to half the margin
width, the latter optimization problem has a simple interpretation: separate the points via
an affine hyperplane such that the margin width is maximized.
Example 7.6 (Support Vector Machine) The data in Figure 7.8 was uniformly gener-
ated on the unit disc. Class-1 points (blue dots) have a radius less than 1/2 (y-values 1) and
class-2 points (red crosses) have a radius greater than 1/2 (y-values −1).
-1
-0.5
0
0.5
1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
Figure 7.8: Separate the two classes.
Of course it is not possible to separate the two groups of points via a straight line in


R2. However, it is possible to separate them in R3 by considering three-dimensional feature
vectors z = [z1, z2, z3]⊤= [x1, x2, x2
1 + x2
2]⊤. For any x ∈R2, the corresponding feature vec-
tor z lies on a quadratic surface. In this space it is possible to separate the {zi} points into
two groups by means of a planar surface, as illustrated in Figure 7.9.
Classification
273
Figure 7.9: In feature space R3 the points can be separated by a plane.
We wish to find a separating plane in R3 using the transformed features. The following
Python code uses the SVC function of the sklearn module to solve the quadratic optimiz-
ation problem (7.23) (with C = ∞). The results are summarized in Table 7.6. The data is
available from the book’s GitHub site as svmcirc.csv.
svmquad.py
import numpy as np
from numpy import genfromtxt
from sklearn.svm import SVC
data = genfromtxt('svmcirc.csv', delimiter=',')
x = data[:,[0,1]] #vectors are rows
y = data[:,[2]].reshape(len(x),) #labels
tmp = np.sum(np.power(x,2),axis=1).reshape(len(x),1)
z = np.hstack((x,tmp))
clf = SVC(C = np.inf, kernel='linear')
clf.fit(z,y)
print("Support Vectors \n", clf.support_vectors_)
print("Support Vector Labels ",y[clf.support_])
print("Nu",clf.dual_coef_)
print("Bias",clf.intercept_)
Support Vectors
[[ 0.038758
0.53796
0.29090314]
[-0.49116
-0.20563
0.28352184]
[-0.45068
-0.04797
0.20541358]
[-0.061107
-0.41651
0.17721465]]
Support Vector Labels
[-1. -1.
1.
1.]
Nu [[ -46.49249413 -249.01807328
265.31805855
30.19250886]]
Bias
[5.617891]
274
Support Vector Machine
Table 7.6: Optimal support vector machine parameters for the R3 data.
z⊤
y
α = νy
0.0388
0.5380
0.2909
−1
−46.4925
−0.4912
−0.2056
0.2835
−1
−249.0181
−0.4507
−0.0480
0.2054
1
265.3181
−0.0611
−0.4165
0.1772
1
30.1925
It follows that the normal vector of the plane is
β =
X
i∈S
αizi = [−0.9128, 0.8917, −24.2764]⊤,
where S is the set of indices of the support vectors. We see that the plane is almost per-
pendicular to the z1, z2 plane. The bias term β0 c

an also be found from the table above. In
particular, for any x⊤and y in Table 7.6, we have y −β⊤z = β0 = 5.6179.
To draw the separating boundary in R2 we need to project the intersection of the sep-
arating plane with the quadratic surface onto the z1, z2 plane. That is, we need to find all
points (z1, z2) such that
5.6179 −0.9128z1 + 0.8917z2 = 24.2764 (z2
1 + z2
2).
(7.25)
This is the equation of a circle with (approximate) center (0.019, −0.018) and radius 0.48,
which is very close to the true circular boundary between the two groups, with center (0, 0)
and radius 0.5. This circle is drawn in Figure 7.10.
-1
0
1
-1
0
1
Figure 7.10: The circular decision boundary can be viewed equivalently as (a) the pro-
jection onto the x1, x2 plane of the intersection of the separating plane with the quadratic
surface (both in R3), or (b) the set of points x = (x1, x2) for which gτ(x) = β0 +β⊤ϕ(x) = 0.
An equivalent way to derive this circular separating boundary is to consider the feature
map ϕ(x) = [x1, x2, x2
1 + x2
2]⊤on R2, which defines a reproducing kernel
κ(x, x′) = ϕ(x)⊤ϕ(x′),
Classification
275
on R2, which in turn gives rise to a (unique) RKHS H. The optimal prediction function
(7.19) is now of the form
gτ(x) = α0 + 1
γ
n
X
i=1
yi λi ϕ(xi)⊤ϕ(x) = β0 + β⊤ϕ(x),
(7.26)
where α0 = β0 and
β = 1
γ
n
X
i=1
yi λi ϕ(xi).
The decision boundary, {x : gτ(x) = 0}, is again a circle in R2. The following code de-
termines the fitted model parameters and the decision boundary. Figure 7.10 shows the
optimal decision boundary, which is identical to (7.25). The function mykernel specifies
the custom kernel above.
svmkern.py
import numpy as np, matplotlib.pyplot as plt
from numpy import genfromtxt
from sklearn.svm import SVC
def mykernel(U,V):
tmpU = np.sum(np.power(U,2),axis=1).reshape(len(U),1)
U = np.hstack((U,tmpU))
tmpV = np.sum(np.power(V,2),axis=1).reshape(len(V),1)
V = np.hstack((V,tmpV))
K = U @ V.T
print(K.shape)
return K
# read in the data
inp = genfromtxt('svmcirc.csv', del

imiter=',')
data = inp[:,[0,1]] #vectors are rows
y = inp[:,[2]].reshape(len(data),) #labels
clf = SVC(C = np.inf, kernel=mykernel , gamma='auto') # custom kernel
# clf = SVC(C = np.inf, kernel="rbf", gamma='scale') # inbuilt
clf.fit(data,y)
print("Support Vectors \n", clf.support_vectors_)
print("Support Vector Labels ",y[clf.support_])
print("Nu ",clf.dual_coef_)
print("Bias ",clf.intercept_)
# plot
d = 0.001
x_min, x_max = -1,1
y_min, y_max = -1,1
xx, yy = np.meshgrid(np.arange(x_min, x_max, d), np.arange(y_min,
y_max, d))
plt.plot(data[clf.support_ ,0],data[clf.support_ ,1],'go')
plt.plot(data[y==1,0],data[y==1,1],'b.')
276
Support Vector Machine
plt.plot(data[y==-1,0],data[y==-1,1],'rx')
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contour(xx, yy, Z,colors ="k")
plt.show()
Finally, we illustrate the use of the Gaussian kernel
κ(x, x′) = e−c ∥x−x′∥2,
(7.27)
where c > 0 is some tuning constant. This is an example of a radial basis function kernel,
which are reproducing kernels of the form κ(x, x′) = f(∥x −x′∥), for some positive real-
valued function f. Each feature vector x is now transformed to a function κx = κ(x, ·). We
can think of it as the (unnormalized) pdf of a Gaussian distribution centered around x, and
gτ is a (signed) mixture of these pdfs, plus a constant; that is,
gτ(x) = α0 +
n
X
i=1
αi e−c ∥xi−x∥2.
Replacing in Line 2 of the previous code mykernel with ’rbf’ produces the SVM
parameters given in Table 7.7. Figure 7.11 shows the decision boundary, which is not ex-
actly circular, but is close to the true (circular) boundary {x : ∥x∥= 1/2}. There are now
seven support vectors, rather than the four in Figure 7.10.
Table 7.7: Optimal support vector machine parameters for the Gaussian kernel case.
x⊤
y
α (×109)
0.0388
0.5380
−1
−0.0635
−0.4912
−0.2056
−1
−9.4793
0.5086
0.1576
−1
−0.5240
−0.4507
−0.0480
1
5.5405
x⊤
y
α (×109)
−0.4374
0.3854
−1
−1.4399
0.3402
−0.5740
−1
−0.1000
−0.4098
−0.1763
1
6.0662
-1
0
1
-1
0
1
Figure 7

.11: Left: The decision boundary {x : gτ(x) = 0} is roughly circular, and separates
the two classes well. There are seven support vectors, indicated by green circles. Right:
The graph of gτ is a scaled mixture of Gaussian pdfs plus a constant.
Classification
277
Remark 7.2 (Scaling and Penalty Parameters) When using a radial basis function in
SVC in sklearn, the scaling c (7.27) can be set via the parameter gamma. Note that large
values of gamma lead to highly peaked predicted functions, and small values lead to highly
smoothed predicted functions. The parameter C in SVC refers C = 1/γ in (7.23).
7.8
Classification with Scikit-Learn
In this section we apply several classification methods to a real-world data set, using the
Python module sklearn (the package name is Scikit-Learn). Specifically, the data is ob-
tained from UCI’s Breast Cancer Wisconsin data set. This data set, first published and
analyzed in [118], contains the measurements related to 569 images of 357 benign and
212 malignant breast masses. The goal is to classify a breast mass as benign or malig-
nant based on 10 features: Radius, Texture, Perimeter, Area, Smoothness, Compactness,
Concavity, Concave Points, Symmetry, and Fractal Dimension of each mass. The mean,
standard error, and “worst” of these attributes were computed for each image, resulting in
30 features. For instance, feature 1 is Mean Radius, feature 11 is Radius SE, feature 21 is
Worst Radius.
The following Python code reads the data, extracts the response vector and model (fea-
ture) matrix and divides the data into a training and test set.
skclass1.py
from numpy import genfromtxt
from sklearn.model_selection import train_test_split
url1 = "http://mlr.cs.umass.edu/ml/machine -learning -databases/"
url2 = "breast-cancer-wisconsin/"
name = "wdbc.data"
data = genfromtxt(url1 + url2 + name, delimiter=',', dtype=str)
y = data[:,1] #responses
X = data[:,2:].astype('float') #features as an ndarray matrix
X_train , X_test , y_train , y_test = t

rain_test_split(
X, y, test_size = 0.4, random_state = 1234)
To visualize the data we create a 3D scatterplot for the features mean radius, mean
texture, and mean concavity, which correspond to the columns 0, 1, and 6 of the model
matrix X. Figure 7.12 suggests that the malignant and benign breast masses could be well
separated using these three features.
skclass2.py
from skclass1 import X, y
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np
Bidx = np.where(y == 'B')
Midx= np.where(y == 'M')
278
Classification with Scikit-Learn
# plot features Radius (column 0), Texture (1), Concavity (6)
fig = plt.figure()
ax = fig.gca(projection = '3d')
ax.scatter(X[Bidx,0], X[Bidx,1], X[Bidx,6],
c='r', marker='^', label='Benign')
ax.scatter(X[Midx,0], X[Midx,1], X[Midx,6],
c='b', marker='o', label='Malignant')
ax.legend()
ax.set_xlabel('Mean Radius')
ax.set_ylabel('Mean Texture')
ax.set_zlabel('Mean Concavity')
plt.show()
Mean Radius
10
15
20
25
Mean Texture
10
15
20
25
30
35
40
Mean Concavity
0.0
0.1
0.2
0.3
0.4
Benign
Malignant
Figure 7.12: Scatterplot of three features of the benign and malignant breast masses.
The following code uses various classifiers to predict the category of breast masses
(benign or malignant). In this case the training set has 341 elements and the test set has 228
elements. For each classifier the percentage of correct predictions (that is, the accuracy) in
the test set is reported. We see that in this case quadratic discriminant analysis gives the
highest accuracy (0.956). Exercise 18 explores the question whether this metric is the most
appropriate for these data.
skclass3.py
from skclass1 import X_train , y_train , X_test , y_test
from sklearn.metrics import accuracy_score
import sklearn.discriminant_analysis as DA
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
names = ["Logit","NBayes

", "LDA", "QDA", "KNN", "SVM"]
Classification
279
classifiers = [LogisticRegression(C=1e5),
GaussianNB(),
DA.LinearDiscriminantAnalysis(),
DA.QuadraticDiscriminantAnalysis(),
KNeighborsClassifier(n_neighbors=5),
SVC(kernel='rbf', gamma = 1e-4)]
print('Name
Accuracy\n'+14*'-')
for name, clf in zip(names, classifiers):
clf.fit(X_train , y_train)
y_pred = clf.predict(X_test)
print('{:6}
{:3.3f}'.format(name, accuracy_score(y_test,y_pred)))
Name
Accuracy
--------------
Logit
0.943
NBayes
0.908
LDA
0.943
QDA
0.956
KNN
0.925
SVM
0.939
Further Reading
An excellent source for understanding various pattern recognition techniques is the book
[35] by Duda et al. Theoretical foundations of classification, including the Vapnik–
Chernovenkis dimension and the fundamental theorem of learning, are discussed in
[109, 121, 122]. A popular measure for characterizing the performance of a binary classi-
fier is the receiver operating characteristic (ROC) curve [38]. The naïve Bayes classific-
ation paradigm can be extended to handle explanatory variable dependency via graphical
models such as Bayesian networks and Markov random fields [46, 66, 69]. For a detailed
discussion on Bayesian decision theory, see [8].
Exercises
1. Let 0 ⩽w ⩽1. Show that the solution to the convex optimization problem
min
p1,...,pn
n
X
i=1
p2
i
subject to:
n−1
X
i−1
pi = w and
n
X
i=1
pi = 1,
(7.28)
is given by pi = w/(n −1), i = 1, . . . , n −1 and pn = 1 −w.
2. Derive the formulas (7.14) by minimizing the cross-entropy training loss:
−1
n
n
X
i=1
ln g(xi, yi | θ),
280
Exercises
where g(x, y | θ) is such that:
ln g(x, y | θ) = ln αy −1
2 ln |Σy| −1
2 (x −µy)⊤Σ−1
y (x −µy) −p
2 ln(2π).
3. Adapt the code in Example 7.2 to plot the estimated decision boundary instead of the
true one in Figure 7.3. Compare the true and estimated decision boundaries.
4. Recall from equation (7.16) that the decision boundaries of the multi-logit classifier are
linear, and that the pre-classifier can be written as a conditional pdf o

f the form:
g(y | W, b, x) =
exp(zy+1)
Pc
i=1 exp(zi),
y ∈{0, . . . , c −1},
where x⊤= [1,ex⊤] and z = Wex + b.
(a) Show that the linear discriminant pre-classifier in Section 7.4 can also be written as a
conditional pdf of the form (θ = {αy, Σy, µy}c−1
y=0):
g(y | θ, x) =
exp(zy+1)
Pc
i=1 exp(zi),
y ∈{0, . . . , c −1},
where x⊤= [1,ex⊤] and z = Wex + b. Find formulas for the corresponding b and W
in terms of the linear discriminant parameters {αy, µy, Σy}c−1
y=0, where Σy = Σ for all y.
(b) Explain which pre-classifier has smaller approximation error: the linear discriminant
or multi-logit one? Justify your answer by proving an inequality between the two
approximation errors.
5. Consider a binary classification problem where the response Y takes values in {−1, 1}.
Show that optimal prediction function for the hinge loss Loss(y,by) = (1−yby)+ := max{0, 1−
yby} is the same as the optimal prediction function g∗for the indicator loss:
g∗(x) =

1
if
P[Y = 1 | X = x] > 1/2,
−1
if
P[Y = 1 | X = x] < 1/2.
That is, show that
E (1 −Y h(X))+ ⩾E (1 −Y g∗(X))+
(7.29)
for all functions h.
6. In Example 4.12, we applied a principal component analysis (PCA) to the iris data,
☞159
but refrained from classifying the flowers based on their feature vectors x. Implement a
1-nearest neighbor algorithm, using a training set of 50 randomly chosen data pairs (x, y)
from the iris data set. How many of the remaining 100 flowers are correctly classified?
Now classify these entries with an off-the-shelf multi-logit classifier, e.g., such as can be
found in the sklearn and statsmodels packages.
7. Figure 7.13 displays two groups of data points, given in Table 7.8. The convex hulls
have also been plotted. It is possible to separate the two classes of points via a straight line.
Classification
281
In fact, many such lines are possible. SVM gives the best separation, in the sense that the
gap (margin) between the points is maximal.
-3
-2
-1
0
1
2
3
4
-2
-1
0
1
2
3
4
5
6
Figure 7.13: Separat

e the points by a straight line so that the separation between the two
groups is maximal.
Table 7.8: Data for Figure 7.13.
x1
x2
y
2.4524
5.5673
−1
1.2743
0.8265
1
0.8773
−0.5478
1
1.4837
3.0464
−1
0.0628
4.0415
−1
−2.4151
−0.9309
1
1.8152
3.9202
−1
1.8557
2.7262
−1
−0.4239
1.8349
1
1.9630
0.6942
1
x1
x2
y
0.5819
−1.0156
1
1.2065
3.2984
−1
2.6830
0.4216
1
−0.0734
1.3457
1
0.0787
0.6363
1
0.3816
5.2976
−1
0.3386
0.2882
1
−0.1493
−0.7095
1
1.5554
4.9880
−1
3.2031
4.4614
−1
(a) Identify from the figure the three support vectors.
(b) For a separating boundary (line) given by β0 + β⊤x = 0, show that the margin width
is 2/∥β∥.
(c) Show that the parameters β0 and β that solve the convex optimization problem (7.24)
provide the maximal width between the margins.
(d) Solve (7.24) using a penalty approach; see Section B.4. In particular, minimize the
☞415
282
Exercises
penalty function
S (β, β0) = ∥β∥2 −C
n
X
i=1
min
n
(β0 + β⊤xi) yi −1, 0
o
for some positive penalty constant C.
(e) Find the solution the dual optimization problem (7.21) by using sklearn’s SCV
method. Note that, as the two point sets are separable, the constraint λ ⩽1 may
be removed, and the value of γ can be set to 1.
8. In Example 7.6 we used the feature map ϕ(x) = [x1, x2, x2
1 + x2
2]⊤to classify the points.
An easier way is to map the points into R1 via the feature map ϕ(x) = ∥x∥or any monotone
function thereof. Translated back into R2 this yields a circular separating boundary. Find
the radius and center of this circle, using the fact that here the sorted norms for the two
groups are . . . , 0.4889, 0.5528, . . ..
9. Let Y ∈{0, 1} be a response variable and let h(x) be the regression function
h(x) := E[Y | X = x] = P[Y = 1 | X = x].
Recall that the Bayes classifier is g∗(x) = 1{h(x) > 1/2}. Let g : R →{0, 1} be any
other classifier function. Below, we denote all probabilities and expectations conditional
on X = x as Px[·] and Ex[·].
(a) Show that
Px[g(x) , Y] =
irreducible error
z           }|           {


Px[g∗(x) , Y] +|2h(x) −1| 1{g(x) , g∗(x)}.
Hence, deduce that for a learner gT constructed from a training set T , we have
E[Px[gT(x) , Y | T ]] = Px[g∗(x) , Y] + |2h(x) −1| P[gT(x) , g∗(x)],
where the first expectation and last probability operations are with respect to T .
(b) Using the previous result, deduce that for the unconditional error (that is, we no longer
condition on X = x), we have
P[g∗(X) , Y] ⩽P[gT(X) , Y].
(c) Show that, if gT := 1{hT(x) > 1/2} is a classifier function such that as n →∞
hT(x)
d
−→Z ∼N(µ(x), σ2(x))
for some mean and variance functions µ(x) and σ2(x), respectively, then
Px[gT(x) , g∗(x)] −→Φ
 sign(1 −2h(x))(2µ(x) −1)
2σ(x)
!
,
where Φ is the cdf of a standard normal random variable.
Classification
283
10. The purpose of this exercise is to derive the dual program (7.21) from the primal
program (7.20). The starting point is to introduce a vector of auxiliary variables ξ :=
[ξ1, . . . , ξn]⊤and write the primal program as
min
α,α0,ξ
n
X
i=1
ξi + γ
2α⊤Kα
subject to:
ξ ⩾0,
yi(α0 + {Kα}i) ⩾1 −ξi, i = 1, . . . , n.
(7.30)
(a) Apply the Lagrangian optimization theory from Section B.2.2 to obtain the Lag-
☞406
rangian function L({α0, α, ξ}, {λ, µ}), where µ and λ are the Lagrange multipliers cor-
responding to the first and second inequality constraints, respectively.
(b) Show that the Karush–Kuhn–Tucker (see Theorem B.2) conditions for optimizing L
☞407
are:
λ⊤y = 0
α = y ⊙λ/γ
0 ⩽λ ⩽1
(1 −λ) ⊙ξ = 0,
λi (yig(xi) −1 + ξi) = 0, i = 1, . . . , n
ξ ⩾0,
yig(xi) −1 + ξi ⩾0, i = 1, . . . , n.
(7.31)
Here ⊙stands for componentwise multiplication; e.g., y ⊙λ = [y1λ1, . . . , ynλn]⊤, and
we have abbreviated α0 + {Kα}i to g(xi), in view of (7.19). [Hint: one of the KKT
conditions is λ = 1 −µ; thus we can eliminate µ.]
(c) Using the KKT conditions (7.31), reduce the Lagrange dual function L∗(λ) :=
minα0,α,ξ L({α0, α, ξ}, {λ, 1 −λ}) to
L∗(λ) =
n
X
i=1
λi −1
2γ
n
X
i=1
n
X
j=1
λiλ jyiyj κ(xi, xj).
(7.32)
(d) As a consequence of (7.19) and (a)–(c), show th

at the optimal prediction function gτ
is given by
gτ(x) = α0 + 1
γ
n
X
i=1
yiλi κ(xi, x),
(7.33)
where λ is the solution to
max
λ
L∗(λ)
subject to:
λ⊤y = 0, 0 ⩽λ ⩽1,
(7.34)
and α0 = y j −1
γ
Pn
i=1 yiλi κ(xi, xj) for any j such that λj ∈(0, 1).
11. Consider SVM classification as illustrated in Figure 7.7. The goal of this exercise is to
classify the training points {(xi, yi)} based on the value of the multipliers {λi} in Exercise 10.
Let ξi be the auxiliary variable in Exercise 10, i = 1, . . . , n.
284
Exercises
(a) For λi ∈(0, 1) show that (xi, yi) lies exactly on the decision border.
(b) For λi = 1, show that (xi, yi) lies strictly inside the margins.
(c) Show that for λi = 0 the point (xi, yi) lies outside the margins and is correctly classi-
fied.
12. A well-known data set is the MNIST handwritten digit database, containing many
thousands of digitalized numbers (from 0 to 9), each described by a 28 × 28 matrix of gray
scales. A similar but much smaller data set is described in [63]. Here, each handwritten
digit is summarized by a 8 × 8 matrix with integer entries from 0 (white) to 15 (black).
Figure 7.14 shows the first 50 digitized images. The data set can be accessed with Python
using the sklearn package as follows.
from sklearn import datasets
digits = datasets.load_digits()
x_digits = digits.data
# explanatory variables
y_digits = digits.target
# responses
Figure 7.14: Classify the digitized images.
(a) Divide the data into a 75% training set and 25% test set.
(b) Compare the effectiveness of the K-nearest neighbors and naïve Bayes method to
classify the data.
(c) Assess which K to use in the K-nearest neighbors classification.
13. Download the winequality-red.csv data set from UCI’s wine-quality website.
The response here is the wine quality (from 0 to 10) as specified by a wine “expert”
and the explanatory variables are various characteristics such as acidity and sugar con-
tent. Use the SVC classifier of sklearn.svm with a linear kernel and penalty para-


meter C = 1 (see Remark 7.2) to fit the data. Use the method cross_val_score from
Classification
285
sklearn.model_selection to obtain a five-fold cross-validation score as an estimate of
the probability that the predicted class matches the expert’s class.
14. Consider the credit approval data set crx.data from UCI’s credit approval website.
The data set is concerned with credit card applications. The last column in the data set
indicates whether the application is approved (+) or not (−). With the view of preserving
data privacy, all 15 explanatory variables were anonymized. Note that some explanatory
variables are continuous and some are categorical.
(a) Load and prepare the data for analysis with sklearn. First, eliminate data
rows with missing values. Next, encode categorical explanatory variables using a
OneHotEncoder object from sklearn.preprocessing to create a model matrix X
with indicator variables for the categorical variables, as described in Section 5.3.5.
☞177
(b) The model matrix should contain 653 rows and 46 columns. The response variable
should be a 0/1 variable (reject/approve). We will consider several classification al-
gorithms and test their performance (using a zero-one loss) via ten-fold cross valida-
tion.
i. Write a function which takes 3 parameters: X, y, and a model, and returns the
ten-fold cross-validation estimate of the expected generalization risk.
ii. Consider the following sklearn classifiers: KNeighborsClassifier (k = 5),
LogisticRegression, and MPLClassifier (multilayer perceptron). Use the
function from (i) to identify the best performing classifier.
15. Consider a synthetic data set that was generated in the following fashion. The explan-
atory variable follows a standard normal distribution. The response label is 0 if the explan-
atory variable is between the 0.95 and 0.05 quantiles of the standard normal distribution,
and 1, otherwise. The data set was generated using the following code.
import numpy as np
import scipy.stats


# generate data
np.random.seed(12345)
N = 100
X = np.random.randn(N)
q = scipy.stats.norm.ppf(0.95)
y = np.zeros(N)
y[X>=q] = 1
y[X<=-q] = 1
X = X.reshape(-1,1)
Compare the K-nearest neighbors classifier with K = 5 and logistic regression classi-
fier. Without computation, which classifier is likely to be better for these data? Verify your
answer by coding both classifiers and printing the corresponding training 0–1 loss.
16. Consider the digits data set from Exercise 12. In this exercise, we would like to train
a binary classifier for the identification of digit 8.
(a) Divide the data such that the first 1000 rows are used as the training set and the rest
are used as the test set.
286
Exercises
(b) Train the LogisticRegression classifier from the sklearn.linear_model pack-
age.
(c) “Train” a naïve classifier that always returns 0. That is, the naïve classifier identifies
each instance as being not 8.
(d) Compare the zero-one test losses of the logistic regression and the naïve classifiers.
(e) Find the confusion matrix, the precision, and the recall of the logistic regression clas-
sifier.
(f) Find the fraction of eights that are correctly detected by the logistic regression clas-
sifier.
17. Repeat Exercise 16 with the original MNIST data set. Use the first 60,000 rows as the
train set and the remaining 10,000 rows as the test set. The original data set can be obtained
using the following code.
from sklearn.datasets import fetch_openml
X, y = fetch_openml('mnist_784', version=1, return_X_y=True)
18. For the breast cancer data in Section 7.8, investigate and discuss whether accuracy is
☞277
the relevant metric to use or if other metrics discussed in Section 7.2 are more appropriate.
☞253
CHAPTER8
DECISION TREES AND ENSEMBLE
METHODS
Statistical learning methods based on decision trees have gained tremendous pop-
ularity due to their simplicity, intuitive representation, and predictive accuracy. This
chapter gives an introduction to the construction and use of such 

trees. We also dis-
cuss two key ensemble methods, namely bootstrap aggregation and boosting, which
can further improve the efficiency of decision trees and other learning methods.
8.1
Introduction
Tree-based methods provide a simple, intuitive, and powerful mechanism for both regres-
sion and classification. The main idea is to divide a (potentially complicated) feature space
X into smaller regions and fit a simple prediction function to each region. For example,
in a regression setting, one could take the mean of the training responses associated with
the training features that fall in that specific region. In the classification setting, a com-
monly used prediction function takes the majority vote among the corresponding response
variables. We start with a simple classification example.
Example 8.1 (Decision Tree for Classification) The left panel of Figure 8.1 shows a
training set of 15 two-dimensional points (features) falling into two classes (red and blue).
How should the new feature vector (black point) be classified?
20
10
0
10
20
20
10
0
10
20
30
40
20
10
0
10
20
20
10
0
10
20
30
40
Figure 8.1: Left: training data and a new feature. Right: a partition of the feature space.
287
288
Introduction
It is not possible to linearly separate the training set, but we can partition the feature
space X = R2 into rectangular regions and assign a class (color) to each region, as shown
in the right panel of Figure 8.1. Points in these regions are classified accordingly as blue
or red. The partition thus defines a classifier (prediction function) g that assigns to each
feature vector x a class “red” or “blue”. For example, for x = [−15, 0]⊤(solid black point),
g(x) = “blue”, since it belongs to a blue region of the feature space.
Both the classification procedure and the partitioning of the feature space can be con-
veniently represented by a binary decision tree
decision tree
. This is a tree where each node v corres-
ponds to a region (subset) Rv of the feature space X 

— the root node corresponding to the
feature space itself.
x2 ≤12.0
x1 ≤−20.5
x1 ≤20.0
x1 ≤2.5
x1 ≤−5.0
True
True
False
True
True
True
False
False
False
False
Figure 8.2: The decision-
tree that corresponds to the
partition in Figure 8.1.
Each internal node v contains a logical condition that di-
vides Rv into two disjoint subregions. The leaf nodes (the ter-
minal nodes of the tree) are not subdivided, and their corres-
ponding regions form a partition of X, as they are disjoint and
their union is X. Associated with each leaf node w is also a
regional prediction function gw on Rw.
The partitioning of Figure 8.1 was obtained from
the decision tree shown in Figure 8.2. As an illustra-
tion of the decision procedure, consider again the input
x = [x1, x2]⊤= [−15, 0]⊤. The classification process starts
from the tree root, which contains the condition x2 ⩽12.0. As
the second component of x is 0, the root condition is satisfied
and we proceed to the left child, which contains the condition
x1 ⩽−20.5. The next step is similar. As −15 > −20.5, the
condition is not satisfied and we proceed to the right child.
Such an evaluation of logical conditions along the tree path
will eventually bring us to a leaf node and its associated re-
gion. In this case the process terminates in a leaf that corres-
ponds to the left blue region in the right-hand panel of Fig-
ure 8.1.
More generally, a binary tree T will partition the feature space X into as many regions
as there are leaf nodes. Denote the set of leaf nodes by W. The overall prediction function
g that corresponds to the tree can then be written as
g(x) =
X
w∈W
gw(x) 1{x ∈Rw},
(8.1)
where 1 denotes the indicator function. The representation (8.1) is very general and de-
pends on (1) how the regions {Rw} are constructed via the logical conditions in the decision
tree, as well as (2) how the regional prediction functions
regional
prediction
functions
of the leaf nodes are defined.
Simple logical conditions of the form xj ⩽ξ split a

 Euclidean feature space into rect-
angles aligned with the axes. For example, Figure 8.2 partitions the feature space into six
rectangles: two blue and four red rectangles.
In a classification setting, the regional prediction function gw corresponding to a leaf
node w takes values in the set of possible class labels. In most cases, as in Example 8.1, it
Decision Trees and Ensemble Methods
289
is taken to be constant on the corresponding region Rw. In a regression setting, gw is real-
valued and also usually takes only one value. That is, every feature vector in Rw leads to
the same predicted value. Of course, different regions will usually have different predicted
values.
Constructing a tree with a training set τ = {(xi, yi)}}n
i=1 amounts to minimizing the
training loss
ℓτ (g) = 1
n
n
X
i=1
Loss(yi, g(xi))
(8.2)
for some loss function; see Chapter 2. With g of the form (8.1), we can write
☞
19
ℓτ (g) = 1
n
n
X
i=1
Loss(yi, g(xi)) = 1
n
n
X
i=1
X
w∈W
1{xi ∈Rw} Loss(yi, g(xi))
(8.3)
=
X
w∈W
1
n
n
X
i=1
1{xi ∈Rw} Loss(yi, gw(xi))
|                                    {z                                    }
(∗)
,
(8.4)
where (∗) is the contribution by the regional prediction function gw to the overall training
loss. In the case where all {xi} are different, finding a decision tree T that gives a zero
squared-error or zero–one training loss is easy, see Exercise 1, but such an “overfitted” tree
will have poor predictive behavior, expressed in terms of the generalization risk. Instead
we consider a restricted class of decision trees and aim to minimize the training loss within
that class. It is common to use a top-down greedy approach, which can only achieve an
approximate minimization of the training loss.
8.2
Top-Down Construction of Decision Trees
Let τ = {(xi, yi)}n
i=1 be the training set. The key to constructing a binary decision tree T
is to specify a splitting rule
splitting rule
for each node v, which can be defined as a logical function
s : X →{False, True} or,

 equivalently, a binary function s : X →{0, 1}. For example,
in the decision tree of Figure 8.2 the root node has splitting rule x 7→1{x2 ⩽12.0}, in
correspondence with the logical condition {x2 ⩽12.0}. During the construction of the tree,
each node v is associated with a specific region Rv ⊆X and therefore also the training
subset {(x, y) ∈τ : x ∈Rv} ⊆τ. Using a splitting rule s, we can divide any subset σ of the
training set τ into two sets:
σT := {(x, y) ∈σ : s(x) = True} and σF := {(x, y) ∈σ : s(x) = False} .
(8.5)
Starting from an empty tree and the initial data set τ, a generic decision tree con-
struction takes the form of the recursive Algorithm 8.2.1. Here we use the notation
Tv for a subtree of T starting from node v. The final tree T is thus obtained via T =
Construct_Subtree(v0, τ), where v0 is the root of the tree.
290
Top-Down Construction of Decision Trees
Algorithm 8.2.1: Construct_Subtree
Input: A node v and a subset of the training data: σ ⊆τ.
Output: A (sub) decision tree Tv.
1 if termination criterion is met then
// v is a leaf node
2
Train a regional prediction function gv using the training data σ.
3 else
// split the node
4
Find the best splitting rule sv for node v.
5
Create successors vT and vF of v.
6
σT ←{(x, y) ∈σ : sv(x) = True}
7
σF ←{(x, y) ∈σ : sv(x) = False}
8
TvT ←Construct_Subtree (vT, σT)
// left branch
9
TvF ←Construct_Subtree (vF, σF)
// right branch
10 return Tv
The splitting rule sv divides the region Rv into two disjoint parts, say RvT and RvF. The
corresponding prediction functions, gT and gF, satisfy
gv(x) = gT(x) 1{x ∈RvT} + gF(x) 1{x ∈RvF},
x ∈Rv.
In order to implement the procedure described in Algorithm 8.2.1, we need to address
the construction of the regional prediction functions gv at the leaves (Line 2), the specific-
ation of the splitting rule (Line 4), and the termination criterion (Line 1). These important
aspects are detailed in the following Sections 8.2.1, 8.2.2, and 8.2.3, respectively.
8.2.1
Regional Predic

tion Functions
In general, there is no restriction on how to choose the prediction function gw for a leaf
node v = w in Line 2 of Algorithm 8.2.1. In principle we can train any model from the
data; e.g., via linear regression. However, in practice very simple prediction functions are
used. Below, we detail a popular choice for classification, as well as one for regression.
1. In the classification setting with class labels 0, . . . , c −1, the regional prediction
function gw for leaf node w is usually chosen to be constant and equal to the most
common class label of the training data in the associated region Rw (ties can be
broken randomly). More precisely, let nw be the number of feature vectors in region
Rw and let
pw
z = 1
nw
X
{(x,y)∈τ : x∈Rw}
1{y=z},
be the proportion of feature vectors in Rw that have class label z = 0, . . . , c −1. The
regional prediction function for node w is chosen to be the constant
gw(x) = argmax
z∈{0,...,c−1}
pw
z .
(8.6)
2. In the regression setting, gw is usually chosen as the mean response in the region;
that is,
gw(x) = yRw := 1
nw
X
{(x,y)∈τ : x∈Rw}
y,
(8.7)
Decision Trees and Ensemble Methods
291
where nw is again the number of feature vectors in Rw. It is not difficult to show that
gw(x) = yRw minimizes the squared-error loss with respect to all constant functions,
in the region Rw; see Exercise 2.
8.2.2
Splitting Rules
In Line 4 in Algorithm 8.2.1, we divide region Rv into two sets, using a splitting rule
(function) sv. Consequently, the data set σ associated with node v (that is, the subset of the
original data set τ whose feature vectors lie in Rv), is also split — into σT and σF. What is
the benefit of such a split in terms of a reduction in the training loss? If v were set to a leaf
node, its contribution to the training loss would be (see (8.4)):
1
n
n
X
i=1
1{(xi,yi)∈σ} Loss(yi, gv(xi)).
(8.8)
If v were to be split instead, its contribution to the overall training loss would be:
1
n
n
X
i=1
1{(xi,yi)∈σT} Loss(yi, gT(xi)) 

+ 1
n
n
X
i=1
1{(xi,yi)∈σF} Loss(yi, gF(xi)),
(8.9)
where gT and gF are the prediction functions belonging to the child nodes vT and vF. A
greedy heuristic is to pretend that the tree construction algorithm immediately terminates
after the split, in which case vT and vF are leaf nodes, and gT and gF are readily evaluated
— e.g., as in Section 8.2.1. Note that for any splitting rule the contribution (8.8) is always
greater than or equal to (8.9). It therefore makes sense to choose the splitting rule such that
(8.9) is minimized. Moreover, the termination criterion may involve comparing (8.9) with
(8.8). If their difference is too small it may not be worth further splitting the feature space.
As an example, suppose the feature space is X = Rp and we consider splitting rules of
the form
s(x) = 1{x j ⩽ξ},
(8.10)
for some 1 ⩽j ⩽p and ξ ∈R, where we identify 0 with False and 1 with True. Due to the
computational and interpretative simplicity, such binary splitting rules are implemented in
many software packages and are considered to be the de facto standard. As we have seen,
these rules divide up the feature space into rectangles, as in Figure 8.1. It is natural to ask
how j and ξ should be chosen so as to minimize (8.9). For a regression problem, using a
squared-error loss and a constant regional prediction function as in (8.7), the sum (8.9) is
given by
1
n
X
(x,y)∈τ:xj⩽ξ
 y −yT
2 + 1
n
X
(x,y)∈τ:xj>ξ
 y −yF
2 ,
(8.11)
where yT and yF are the average responses for the σT and σF data, respectively. Let {xj,k}m
k=1
denote the possible values of x j, j = 1, . . . , p within the training subset σ (with m ⩽n
elements). Note that, for a fixed j, (8.11) is a piecewise constant function of ξ, and that its
minimal value is attained at some value x j,k. As a consequence, to minimize (8.11) over
all j and ξ, it suffices to evaluate (8.11) for each of the m × p values xj,k and then take the
minimizing pair ( j, xj,k).
292
Top-Down Construction of Decision Trees
For a classificati

on problem, using the indicator loss and a constant regional prediction
function as in (8.6), the aim is to choose a splitting rule that minimizes
1
n
X
(x,y)∈σT
1{y , y∗
T} + 1
n
X
(x,y)∈σF
1{y , y∗
F},
(8.12)
where y∗
T = gT(x) is the most prevalent class (majority vote) in the data set σT and y∗
F
is the most prevalent class in σF. If the feature space is X = Rp and the splitting rules
are of the form (8.10), then the optimal splitting rule can be obtained in the same way as
described above for the regression case; the only difference is that (8.11) is replaced with
(8.12).
We can view the minimization of (8.12) as minimizing a weighted average of “impur-
ities” of nodes σT and σF. Namely, for an arbitrary training subset σ ⊆τ, if y∗is the most
prevalent label, then
1
|σ|
X
(x,y)∈σ
1{y , y∗} = 1 −1
|σ|
X
(x,y)∈σ
1{y = y∗} = 1 −py∗= 1 −
max
z∈{0,...,c−1} pz,
where pz is the proportion of data points in σ that have class label z, z = 0, . . . , c −1. The
quantity
1 −
max
z∈{0,...,c−1} pz
measures the diversity of the labels in σ and is called the misclassification impurity. Con-
misclassification
impurity
sequently, (8.12) is the weighted sum of the misclassification impurities of σT and σF,
with weights by |σT|/n and |σF|/n, respectively. Note that the misclassification impurity
only depends on the label proportions rather than on the individual responses. Instead of
using the misclassification impurity to decide if and how to split a data set σ, we can use
other impurity measures that only depend on the label proportions. Two popular choices
are the entropy impurity
entropy
impurity
:
−
c−1
X
z=0
pz log2(pz)
and the Gini impurity
Gini impurity
:
1
2
1 −
c−1
X
z=0
p2
z
.
All of these impurities are maximal when the label proportions are equal to 1/c. Typical
shapes of the above impurity measures are illustrated in Figure 8.3 for the two-label case,
with class probabilities p and 1 −p. We see here the similarity of the different impurity
measures. N

ote that impurities can be arbitrarily scaled, and so using ln(pz) = log2(pz) ln(2)
instead of log2(pz) above gives an equivalent entropy impurity.
8.2.3
Termination Criterion
When building a tree, one can define various types of termination conditions. For example,
we might stop when the number of data points in the tree node (the size of the input σ set
in Algorithm 8.2.1) is less than or equal to some predefined number. Or we might choose
Decision Trees and Ensemble Methods
293
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
p
impurity
cross-entropy
Gini index
misclassiﬁcation
Figure 8.3: Entropy, Gini, and misclassification impurities for binary classification, with
class frequencies p1 = p and p2 = 1−p. The entropy impurity was normalized (divided by
2), to ensure that all impurity measures attain the same maximum value of 1/2 at p = 1/2.
the maximal depth of the tree in advance. Another possibility is to stop when there is no
significant advantage, in terms of training loss, to split regions. Ultimately, the quality of a
tree is determined by its predictive performance (generalization risk) and the termination
condition should aim to strike a balance between minimizing the approximation error and
minimizing the statistical error, as discussed in Section 2.4.
☞
31
Example 8.2 (Fixed Tree Depth) To illustrate how the tree depth impacts on the gener-
alization risk, consider Figure 8.4, which shows the typical behavior of the cross-validation
loss as a function of the tree depth. Recall that the cross-validation loss is an estimate of the
expected generalization risk. Complicated (deep) trees tend to overfit the training data by
producing many divisions of the feature space. As we have seen, this overfitting problem is
typical of all learning methods; see Chapter 2 and in particular Example 2.1. To conclude,
☞
26
increasing the maximal depth does not necessarily result in better performance.
0
5
10
15
20
25
30
0.3
0.35
0.4
0.45
tree depth
loss
Figure 8.4: The ten-fold cross-va

lidation loss as a function of the maximal tree depth for a
classification problem. The optimal maximal tree depth is here 6.
294
Top-Down Construction of Decision Trees
To create Figure 8.4 we used1 the Python method make_blobs from the sklearn
module to produce a training set of size n = 5000 with ten-dimensional feature vectors
☞491
(thus, p = 10 and X = R10), each of which is classified into one of c = 3 classes. The full
code is given below.
TreeDepthCV.py
import numpy as np
from sklearn.datasets import make_blobs
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import zero_one_loss
import matplotlib.pyplot as plt
def ZeroOneScore(clf, X, y):
y_pred = clf.predict(X)
return zero_one_loss(y, y_pred)
# Construct the training set
X, y =
make_blobs(n_samples=5000, n_features=10, centers=3,
random_state=10, cluster_std=10)
# construct a decision tree classifier
clf = DecisionTreeClassifier(random_state=0)
# Cross-validation loss as a function of tree depth (1 to 30)
xdepthlist = []
cvlist = []
tree_depth = range(1,30)
for d in tree_depth:
xdepthlist.append(d)
clf.max_depth=d
cv = np.mean(cross_val_score(clf, X, y, cv=10, scoring=
ZeroOneScore))
cvlist.append(cv)
plt.xlabel('tree depth', fontsize=18, color='black')
plt.ylabel('loss', fontsize=18, color='black')
plt.plot(xdepthlist , cvlist,'-*' , linewidth=0.5)
The code above relies heavily on sklearn and hides the implementation details. To
show how decision trees are actually constructed using the previous theory, we proceed
with a very basic implementation.
8.2.4
Basic Implementation
In this section we implement a regression tree, step by step. To run the program, amalgam-
ate the code snippets below into one file, in the order presented. First, we import various
packages and define a function to generate the training and test data.
1The data used for Figure 8.1 was produced in a similar way.
Decision Trees and Ensemble Methods
295
BasicTree.py


import numpy as np
from sklearn.datasets import make_friedman1
from sklearn.model_selection import train_test_split
def makedata():
n_points = 500 # number of samples
X, y =
make_friedman1(n_samples=n_points , n_features=5,
noise=1.0, random_state=100)
return train_test_split(X, y, test_size=0.5, random_state=3)
The “main” method calls the makedata method, uses the training data to build a regres-
sion tree, and then predicts the responses of the test set and reports the mean squared-error
loss.
def main():
X_train , X_test, y_train , y_test = makedata()
maxdepth = 10 # maximum tree depth
# Create tree root at depth 0
treeRoot = TNode(0, X_train ,y_train)
# Build the regression tree with maximal depth equal to max_depth
Construct_Subtree(treeRoot , maxdepth)
# Predict
y_hat = np.zeros(len(X_test))
for i in range(len(X_test)):
y_hat[i] = Predict(X_test[i],treeRoot)
MSE = np.mean(np.power(y_hat - y_test ,2))
print("Basic tree: tree loss = ",
MSE)
The next step is to specify a tree node as a Python class. Each node has a number of
attributes, including the features and the response data (X and y) and the depth at which
the node is placed in the tree. The root node has depth 0. Each node w can calculate its
contribution to the squared-error training loss Pn
i=1 1{xi ∈Rw}(yi −gw(xi))2. Note that we
have omitted the constant 1/n term when training the tree, which simply scales the loss
(8.2).
class TNode:
def __init__(self, depth, X, y):
self.depth = depth
self.X = X
# matrix of features
self.y = y
# vector of response variables
# initialize optimal split parameters
self.j = None
self.xi = None
# initialize children to be None
self.left = None
self.right = None
296
Top-Down Construction of Decision Trees
# initialize the regional predictor
self.g = None
def CalculateLoss(self):
if(len(self.y)==0):
return 0
return np.sum(np.power(self.y - self.y.mean(),2))
The function below implements the training (tree-building) Algorithm 8.2.1.
def Construct_Subtree(node, max_depth):
i

f(node.depth == max_depth or len(node.y) == 1):
node.g
= node.y.mean()
else:
j, xi = CalculateOptimalSplit(node)
node.j = j
node.xi = xi
Xt, yt, Xf, yf = DataSplit(node.X, node.y, j, xi)
if(len(yt)>0):
node.left = TNode(node.depth+1,Xt,yt)
Construct_Subtree(node.left, max_depth)
if(len(yf)>0):
node.right = TNode(node.depth+1, Xf,yf)
Construct_Subtree(node.right, max_depth)
return node
This requires an implementation of the CalculateOptimalSplit function. To start,
we implement a function DataSplit that splits the data according to s(x) = 1{x j ⩽ξ}.
def DataSplit(X,y,j,xi):
ids = X[:,j]<=xi
Xt
= X[ids == True ,:]
Xf
= X[ids == False ,:]
yt
= y[ids == True]
yf
= y[ids == False]
return Xt, yt, Xf, yf
The CalculateOptimalSplit method runs through the possible splitting thresholds
ξ from the set {xj,k} and finds the optimal split.
def CalculateOptimalSplit(node):
X = node.X
y = node.y
best_var = 0
best_xi = X[0,best_var]
best_split_val = node.CalculateLoss()
Decision Trees and Ensemble Methods
297
m, n
= X.shape
for j in range(0,n):
for i in range(0,m):
xi = X[i,j]
Xt, yt, Xf, yf = DataSplit(X,y,j,xi)
tmpt = TNode(0, Xt, yt)
tmpf = TNode(0, Xf, yf)
loss_t = tmpt.CalculateLoss()
loss_f = tmpf.CalculateLoss()
curr_val =
loss_t + loss_f
if (curr_val < best_split_val):
best_split_val = curr_val
best_var = j
best_xi = xi
return best_var ,
best_xi
Finally, we implement the recursive method for prediction.
def Predict(X,node):
if(node.right == None and node.left != None):
return Predict(X,node.left)
if(node.right != None and node.left == None):
return Predict(X,node.right)
if(node.right == None and node.left == None):
return node.g
else:
if(X[node.j] <= node.xi):
return Predict(X,node.left)
else:
return Predict(X,node.right)
Running the main function defined above gives a similar2 result to what one would
achieve with the sklearn package, using the DecisionTreeRegressor method.
main()
# run the main program
# compare with sklearn
from sklearn.tree import DecisionTreeRegressor
X

_train , X_test, y_train , y_test = makedata() # use the same data
regTree = DecisionTreeRegressor(max_depth = 10, random_state=0)
regTree.fit(X_train ,y_train)
y_hat = regTree.predict(X_test)
MSE2 = np.mean(np.power(y_hat - y_test ,2))
print("DecisionTreeRegressor: tree loss = ",
MSE2)
Basic tree: tree loss =
9.067077996170276
DecisionTreeRegressor: tree loss =
10.197991295531748
2After establishing a best split ξ = x j,k, sklearn assigns the corresponding feature vector randomly to
one of the two child nodes, rather than to the True child.
298
Additional Considerations
8.3
Additional Considerations
8.3.1
Binary Versus Non-Binary Trees
While it is possible to split a tree node into more than two groups (multiway splits), it
generally produces inferior results compared to the simple binary split. The major reason
is that multiway splits can lead to too many nodes near the tree root that have only a
few data points, thus leaving insufficient data for later splits. As multiway splits can be
represented as several binary splits, the latter is preferred [55].
8.3.2
Data Preprocessing
Sometimes, it can be beneficial to preprocess the data prior to the tree construction. For
example, PCA can be used with a view to identify the most important dimensions, which
☞153
in turn will lead to simpler and possibly more informative splitting rules in the internal
nodes.
8.3.3
Alternative Splitting Rules
We restricted our attention to splitting rules of the type s(x) = 1{xj ⩽ξ}, where j ∈
{1, . . . , p} and ξ ∈R. These types of rules may not always result in a simple partition
of the feature space, as illustrated by the binary data in Figure 8.5. In this case, the feature
space could have been partitioned into just two regions, separated by a straight line.
Figure 8.5: The two groups of points can here be separated by a straight line. Instead, the
classification tree divides up the space into many rectangles, leading to an unnecessarily
complicated classification procedure.
In this 

case many classification methods discussed in Chapter 7, such as linear discrim-
inant analysis (Section 7.4), will work very well, whereas the classification tree is rather
☞259
elaborate, dividing the feature set into too many regions. An obvious remedy is to use
splitting rules of the form
s(x) = 1{a⊤x ⩽ξ}.
Decision Trees and Ensemble Methods
299
In some cases, such as the one just discussed, it may be useful to use a splitting rule
that involves several variables, as opposed to a single one. The decision regarding the split
type clearly depends on the problem domain. For example, for logical (binary) variables
our domain knowledge may indicate that a different behavior is expected when both xi and
x j (i , j) are True. In this case, we will naturally introduce a decision rule of the form:
s(x) = 1{xi = T and xj = T}.
8.3.4
Categorical Variables
When an explanatory variable is categorical with labels (levels) say {1, . . . , k}, the split-
ting rule is generally defined via a partition of the label set {1, . . . , k} into two subsets.
Specifically, let L and R be a partition of {1, . . . , k}. Then, the splitting rule is defined via
s(x) = 1{xj ∈L}.
For the general supervised learning case, finding the optimal partition in the sense of min-
imal loss requires one to consider 2k subsets of {1, . . . , k}. Consequently, finding a good
splitting rule for categorical variables can be challenging when the number of labels k is
large.
8.3.5
Missing Values
Missing data is present in many real-life problems. Generally, when working with incom-
plete feature vectors, where one or more values are missing, it is typical to either com-
pletely delete the feature vector from the data (which may distort the data) or to impute
(guess) its missing values from the available data; see e.g., [120]. Tree methods, however,
allow an elegant approach for handling missing data. Specifically, in the general case, the
missing data problem can be handled via surrogate splitting rules [20].


When dealing with categorical (factor) features, we can introduce an additional cat-
egory “missing” for the absent data.
The main idea of surrogate rules is as follows. First, we construct a decision (regression
or a classification) tree via Algorithm 8.2.1. During this construction process, the solution
of the optimization problem (8.9) is calculated only over the observations that are not
missing a particular variable. Suppose that a tree node v has a splitting rule s∗(x) = 1{x j∗⩽
ξ∗} for some 1 ⩽j∗⩽k and threshold ξ∗.
For the node v we can introduce a set of alternative splitting rules that resemble the
original splitting rule, sometimes called the primary splitting rule, using different variables
and thresholds. Namely, we look for a binary splitting rule s(x | j, ξ), j , j∗such that the
data split introduced by s will be similar to the original data split from s∗. The similarity is
generally measured via a binary misclassification loss, where the true classes of observa-
tions are determined by the primary splitting rule and the surrogate splitting rules serve as
classifiers. Consider, for example, the data in Table 8.1 and suppose that the primary split-
ting rule at node v is 1{Age ⩽25}. That is, the five data points are split such that the left
and the right child of v contains two and three data points, respectively. Next, the following
surrogate splitting rules can be considered:
300
Controlling the Tree Shape
1. 1{Salary ⩽1500}, and
2. 1{Height ⩽173}.
Table 8.1: Example data with three variables (Age, Height, and Salary).
Id
Age
Height
Salary
1
20
173
1000
2
25
168
1500
3
38
191
1700
4
49
170
1900
5
62
182
2000
The 1{Salary ⩽1500} surrogate rule completely mimics the primary rule, in the sense
that the data splits induced by these rules are identical. Namely, both rules partition the
data into two sets (by Id) {1, 2} and {3, 4, 5}. On the other hand, the 1{Height ⩽173} rule
is less similar to the primary rule, since it causes the different partition {1

, 2, 4} and {3, 5}.
It is up to the user to define the number of surrogate rules for each tree node. As soon as
these surrogate rules are available, we can use them to handle a new data point, even if the
main rule cannot be applied due to a missing value of the primary variable x j∗. Specifically,
if the observation is missing the primary split variable, we apply the first (best) surrogate
rule. If the first surrogate variable is also missing, we apply the second best surrogate rule,
and so on.
8.4
Controlling the Tree Shape
Eventually, we are interested in getting the right-size tree. Namely, a tree that shows good
generalization properties. It was already discussed in Section 8.2.3 (Figure 8.4) that shal-
low trees tend to underfit and deep trees tend to overfit the data. Basically, a shallow tree
does not produce a sufficient number of splits and a deep tree will produce many partitions
and thus many leaf nodes. If we grow the tree to a sufficient depth, each training sample
will occupy a separate leaf and we will observe a zero loss with respect to the training data.
The above phenomenon is illustrated in Figure 8.6, which presents the cross-validation loss
and the training loss as a function of the tree depth.
In order to overcome the under- and the overfitting problem, Breiman et al. [20] ex-
amined the possibility of stopping the tree from growing as soon as the decrease in loss
due to a split of node v, as expressed in the difference of (8.8) and (8.9), is smaller than
some predefined parameter δ ∈R. Under this setting, the tree construction process will
terminate when no leaf node can be split such that the contribution to the training loss after
this split is greater than δ.
The authors found that this approach was unsatisfactory. Specifically, it was noted that a
very small δ leads to an excessive amount of splitting and thus causes overfitting. Increasing
δ did not work either. The problem is that the nature of the proposed rule is one-step-look-
ahead.

 To see this, consider a tree node for which the best possible decrease in loss is
Decision Trees and Ensemble Methods
301
0
5
10
15
20
0
0.1
0.2
0.3
0.4
tree depth
loss
train
CV
Figure 8.6: The cross-validation and the training loss as a function of the tree depth for a
binary classification problem.
smaller than δ. According to the proposed procedure, this node will not be split further. This
may, however, be sub-optimal, because it could happen that one of the node’s descendants,
if split, could lead to a major decrease in loss.
To address these issues, a so-called pruning
tree pruning
routine can be employed. The idea is as
follows. We first grow a very deep tree and then prune (remove nodes) it upwards until we
reach the root node. Consequently, the pruning process causes the number of tree nodes
to decrease. While the tree is being pruned, the generalization risk gradually decreases up
to the point where it starts increasing again, at which point the pruning is stopped. This
decreasing/increasing behavior is due to the bias–variance tradeoff (2.22).
We next describe the details. To start with, let v and v′ be tree nodes. We say that v′ is
a descendant of v if there is a path down the tree, which leads from v to v′. If such a path
exists, we also say that v is an ancestor of v′. Consider the tree in Figure 8.7.
To formally define pruning, we will require the following Definition 8.1. An example
of pruning is demonstrated in Figure 8.8.
Definition 8.1: Branches and Pruning
1. A tree branch
tree branch
Tv of the tree T is a sub-tree of T rooted at node v ∈T.
2. The pruning of branch Tv from a tree T is performed via deletion of the entire
branch Tv from T except the branch’s root node v. The resulting pruned tree is
denoted by T −Tv.
3. A sub-tree T −Tv is called a pruned sub-tree of T. We indicate this with the
notation T −Tv ≺T or T ≻T −Tv.
A basic decision tree pruning procedure is summarized in Algorithm 8.4.1.
302
Controlling the Tree Shape
v1
v2
v4
v7
v5
v8


v9
v3
v6
v10
v11
Figure 8.7: The node v9 is a descendant of v2, and v2 is an ancestor of {v4, v5, v7, v8, v9}, but
v6 is not a descendant of v2.
v1
v2
v4
v7
v5
v8
v9
v3
v6
v10
v11
(a) T
v2
v4
v7
v5
v8
v9
(b) Tv2
v1
v2
v3
v6
v10
v11
(c) T −Tv2
Figure 8.8: The pruned tree T −Tv2 in (c) is the result of pruning the Tv2 branch in (b) from
the original tree T in (a).
Algorithm 8.4.1: Decision Tree Pruning
Input: Training set τ.
Output: Sequence of decision trees T0 ≻T1 ≻· · ·
1 Build a large decision tree T0 via Algorithm 8.2.1. [A possible termination
criterion for that algorithm is to have some small predetermined number of data
points at each terminal node of T0.]
2 T′ ←T0
3 k ←0
4 while T′ has more than one node do
5
k ←k + 1
6
Choose v ∈T′.
7
Prune the branch rooted at v from T′.
8
Tk ←T′ −Tv and T′ ←Tk.
9 return T0, T1, . . . , Tk
Decision Trees and Ensemble Methods
303
Let T0 be the initial (deep) tree and let Tk be the tree obtained after the k-th pruning
operation, for k = 1, . . . , K. As soon as the sequence of trees T0 ≻T1 ≻· · · ≻TK is avail-
able, one can choose the best tree of {Tk}K
k=1 according to the smallest generalization risk.
Specifically, we can split the data into training and validation sets. In this case, Algorithm
8.4.1 is executed using the training set and the generalization risks of {Tk}K
k=1 are estimated
via the validation set.
While Algorithm 8.4.1 and the corresponding best tree selection process look appeal-
ing, there is still an important question to consider; namely, how to choose the node v and
the corresponding branch Tv in Line 6 of the algorithm. In order to overcome this problem,
Breiman proposed a method called cost complexity pruning, which we discuss next.
8.4.1
Cost-Complexity Pruning
Let T ≺T0 be a tree obtained via pruning of a tree T0. Denote the set of leaf (terminal)
nodes of T by W. The number of leaves |W| is a measure for the complexity of the tree;
recall that |W| is the number of regions {Rw} in the partition of 

X. Corresponding to each
tree T is a prediction function g, as in (8.1). In cost-complexity pruning
cost-complexity
pruning
the objective is to
find a prediction function g (or, equivalently, tree T) that minimizes the training loss ℓτ(g)
while taking into account the complexity of the tree. The idea is to regularize the training
loss, similar to what was done in Chapter 6, by adding a penalty term for the complexity
of the tree. This leads to the following definition.
Definition 8.2: Cost-Complexity Measure
Let τ = {(xi, yi)}n
i=1 be a data set and γ ⩾0 be a real number. For a given tree T, the
cost-complexity measure
cost-complexity
measure
Cτ(γ, T) is defined as:
Cτ(γ, T) := 1
n
X
w∈W

n
X
i=1
1{xi ∈Rw} Loss(yi, gw(xi))
+ γ |W|
(8.13)
= ℓτ (g) + γ |W|,
where ℓτ (g) is the training loss (8.2).
Small values of γ result in a small penalty for the tree complexity |W|, and thus large
trees (that fit the entire training data well) will minimize the measure Cτ(γ, T). In particular,
for γ = 0, T = T0 will be the minimizer of Cτ(γ, T). On the other hand, large values of γ
will prefer smaller trees or, more precisely, trees with fewer leaves. For sufficiently large
γ, the solution T will collapse to a single (root) node.
It can be shown that, for every value of γ, there exists a smallest minimizing sub-tree
of T0 with respect to the cost-complexity measure. In practice, a suitable γ is selected via
observing the performance of the learner on the validation set or by cross-validation.
These advantages and the corresponding limitations are detailed next.
304
Controlling the Tree Shape
8.4.2
Advantages and Limitations of Decision Trees
We list a number of advantages and disadvantages of decision trees, as compared with
other supervised learning methods such as were discussed in Chapters 5, 6, and 7.
Advantages
1. The tree structure can handle both categorical and numerical features in a natural
and straightforward way. Specifically, there is no need to pre-proce

ss categorical
features, say via the introduction of dummy variables.
2. The final tree obtained after the training phase can be compactly stored for the pur-
pose of making predictions for new feature vectors. The prediction process only
involves a single tree traversal from the tree root to a leaf.
3. The hierarchical nature of decision trees allows for an efficient encoding of the fea-
ture’s conditional information. Specifically, after an internal split of a feature x j via
the standard splitting rule (8.10), Algorithm 8.2.1 will only consider such subsets of
data that were constructed based on this split, thus implicitly exploiting the corres-
ponding conditional information from the initial split of xj.
4. The tree structure can be easily understood and interpreted by domain experts with
little statistical knowledge, since it is essentially a logical decision flow diagram.
5. The sequential decision tree growth procedure in Algorithm 8.2.1, and in particular
the fact that the tree has been split using the most important features, provides an
implicit step-wise variable elimination procedure. In addition, the partition of the
variable space into smaller regions results in simpler prediction problems in these
regions.
6. Decision trees are invariant under monotone transformations of the data. To see this,
consider the (optimal) splitting rule s(x) = 1{x3 ⩽2}, where x3 is a positive feature.
Suppose that x3 is transformed to x′
3 = x2
3. Now, the optimal splitting rule will take
the form s(x) = 1{x′
3 ⩽4}.
7. In the classification setting, it is common to report not only the predicted value of a
feature vector, e.g., as in (8.6), but also the respective class probabilities. Decision
trees handle this task without any additional effort. Specifically, consider a new fea-
ture vector. During the estimation process, we will perform a tree traversal and the
point will end up in a certain leaf w. The probability of this feature vector lying in
class z can be estimated 

as the proportion of training points in w that are in class z.
8. As each training point is treated equally in the construction of a tree, the structure of
the tree will be relatively robust to outliers. In a way, trees exhibit a similar kind of
robustness as the sample median does for real-valued data.
Decision Trees and Ensemble Methods
305
Limitations
Despite the fact that the decision trees are extremely interpretable, the predictive accuracy
is generally inferior to other established statistical learning methods. In addition, decision
trees, and in particular very deep trees that were not subject to pruning, are heavily reliant
on their training set. A small change in the training set can result in a dramatic change of the
resulting decision tree. Their inferior predictive accuracy, however, is a direct consequence
of the bias–variance tradeoff. Specifically, a decision tree model generally exhibits a high
variance. To overcome the above limitations, several promising approaches such as bag-
ging, random forest, and boosting are introduced below.
The bagging approach was initially introduced in the context of an ensemble of
decision trees. However, both the bagging and the boosting methods can be applied
to improve the accuracy of general prediction functions.
8.5
Bootstrap Aggregation
The major idea of the bootstrap aggregation or bagging
bagging
method is to combine prediction
functions learned from multiple data sets, with a view to improving overall prediction
accuracy. Bagging is especially beneficial when dealing with predictors that tend to overfit
the data, such as in decision trees, where the (unpruned) tree structure is very sensitive to
small changes in the training set [37, 55].
To start with, consider an idealized setting for a regression tree, where we have access
to B iid copies3 T1, . . . , TB of a training set T . Then, we can train B separate regression
models (B different decision trees) using these sets, giving learners gT1, . . . , gTB, and

 take
their average:
gavg(x) = 1
B
B
X
b=1
gTb(x).
(8.14)
By the law of large numbers, as B →∞, the average prediction function converges to
☞445
the expected prediction function g† := EgT. The following result shows that using g† as
a prediction function (if it were known) would result in an expected squared-error gen-
eralization risk that is less than or equal to the expected generalization risk for a general
☞
24
prediction function gT. It thus suggests that taking an average of prediction functions may
lead to a better expected squared-error generalization risk.
Theorem 8.1: Expected Squared-Error Generalization Risk
Let T be a random training set and let X, Y be a random feature vector and response
that are independent of T . Then,
E

Y −gT(X)
2
⩾E

Y −g†(X)
2 .
3In this section Tk means the k-th training set, not a training set of size k.
306
Bootstrap Aggregation
Proof: We have
E
"
Y −gT(X)
2  X, Y
#
⩾

E[Y | X, Y] −E[gT(X) | X, Y]
2
=

Y −g†(X)
2
,
where the inequality follows from EU2 ⩾(EU)2 for any (conditional) expectation. Con-
sequently, by the tower property,
☞431
E

Y −gT(X)
2
= E
h
E
h Y −gT(X)2 | X, Y
ii
⩾E

Y −g†(X)
2
.
□
Unfortunately, multiple independent data sets are rarely available. But we can substi-
tute them by bootstrapped ones. Specifically, instead of the T1, . . . , TB sets, we can obtain
random training sets T ∗
1 , . . . , T ∗
B by resampling them from a single (fixed) training set τ,
☞76
similar to Algorithm 3.2.6, and use them to train B separate models. By model averaging
as in (8.14) we obtain the bootstrapped aggregated estimator or bagged estimator
bagged
estimator
of the
form:
gbag(x) = 1
B
B
X
b=1
gT ∗
b (x).
(8.15)
Algorithm 8.5.1: Bootstrap Aggregation Sampling
Input: Training set τ = {(xi, yi)}n
i=1 and resample size B.
Output: Bootstrapped data sets.
1 for b = 1 to B do
2
T ∗
b ←∅
3
for i = 1 to n do
4
Draw U ∼U(0, 1)
5
I ←⌈nU⌉
// select random index
6
T ∗
b ←T ∗
b ∪{(xI, yI)}.
7 return T ∗
b , b = 1,

 . . . , B.
Remark 8.1 (Bootstrap Aggregation for Classification Problems) Note that (8.15)
is suitable for handling regression problems. However, the bagging idea can be readily
extended to handle classification settings as well. For example, gbag can take the majority
vote among {gT ∗
b }, b = 1, . . . , B; that is, to accept the most frequent class among B predict-
ors.
While bagging can be applied for any statistical model (such as decision trees, neural
networks, linear regression, K-nearest neighbors, and so on), it is most effective for pre-
dictors that are sensitive to small changes in the training set. The reason becomes clear
when we decompose the expected generalization risk as
E ℓ(gT) = ℓ∗+ E (E[gT(X) | X] −g∗(X))2
|                           {z                           }
expected squared bias
+ E[Var[gT(X) | X]]
|                 {z                 }
expected variance
,
(8.16)
Decision Trees and Ensemble Methods
307
similar to (2.22). Compare this with the same decomposition for the average prediction
☞
35
function gbag in (8.14). As Egbag(x) = EgT(x), we see that any possible improvement in
the generalization risk must be due to the expected variance term. Averaging and bagging
are thus only useful for predictors with a large expected variance, relative to the other two
terms. Examples of such “unstable” predictors include decision trees, neural networks, and
subset selection in linear regression [22]. On the other hand, “stable” predictors are in-
sensitive to small data changes, an example being the K-nearest neighbors method. Note
that for independent training sets T1, . . . , TB a reduction of the variance by a factor B is
achieved: Var gbag(x) = B−1Var gT(x). Again, it depends on the squared bias and irredu-
cible loss how significant this reduction is for the generalization risk.
Remark 8.2 (Limitations of Bagging) It is important to remember that gbag is not ex-
actly equal to gavg, which in turn is not exactly g†. Specifically, gbag is constr

ucted from the
bootstrap approximation of the sampling pdf f. As a consequence, for stable predictors,
it can happen that gbag will perform worse than gT. In addition to the deterioration of the
bagging performance for stable procedures, it can also happen that gT has already achieved
a near optimal predictive accuracy given the available training data. In this case, bagging
will not introduce a significant improvement.
The bagging process provides an opportunity to estimate the generalization risk of
the bagged model without an additional test set. Specifically, recall that we obtain the
T ∗
1 , . . . , T ∗
B sets from a single training set τ by sampling via Algorithm 8.5.1, and use them
to train B separate models. It can be shown (see Exercise 8) that, for large sample sizes, on
average about a third (more precisely, a fraction e−1 ≈0.37) of the original sample points
are not included in bootstrapped set T ∗
b for 1 ⩽b ⩽B. Therefore, these samples can be
used for the loss estimation. These samples are called out-of-bag
out-of-bag
(OOB) observations.
Specifically, for each sample from the original data set, we calculate the OOB loss using
predictors that were trained without this particular sample. The estimation procedure is
summarized in Algorithm 8.5.2. Hastie et al. [55] observe that, under certain conditions, the
OOB loss is almost identical to the n-fold cross-validation loss. In addition, the OOB loss
can be used to determine the number of trees required. Specifically, we can train predictors
until the OOB loss stops changing. Namely, decision trees are added until the OOB loss
stabilizes.
Algorithm 8.5.2: Out-of-Bag Loss Estimation
Input: The original data set τ = {(x1, y1), . . . , (xn, yn)}, the bootstrapped data sets
{T ∗
1 , . . . , T ∗
B}, and the trained predictors
n
gT ∗
1 , . . . , gT ∗
B
o
.
Output: Out-of-bag loss for the averaged model.
1 for i = 1 to n do
2
Ci ←∅
// Indices of predictors not depending on (xi, yi)
3
for b = 1 to B do
4
if (xi, yi

) < T ∗
b then Ci ←Ci ∪{b}
5
Y′
i ←|Ci|−1 P
b∈Ci gT ∗
b (xi)
6
Li ←Loss

yi, Y′
i

7 LOOB ←1
n
Pn
i=1 Li
8 return LOOB.
308
Bootstrap Aggregation
Example 8.3 (Bagging for a Regression Tree) We next proceed with a basic bagging
example for a regression tree, in which we compare the decision tree estimator with the
corresponding bagged estimator. We use the R2 metric (coefficient of determination) for
comparison.
BaggingExample.py
import numpy as np
from sklearn.datasets import make_friedman1
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
np.random.seed(100)
# create regression problem
n_points = 1000 # points
x, y =
make_friedman1(n_samples=n_points , n_features=15,
noise=1.0, random_state=100)
# split to train/test set
x_train , x_test, y_train , y_test = \
train_test_split(x, y, test_size=0.33, random_state=100)
# training
regTree = DecisionTreeRegressor(random_state=100)
regTree.fit(x_train ,y_train)
# test
yhat = regTree.predict(x_test)
# Bagging construction
n_estimators=500
bag = np.empty((n_estimators), dtype=object)
bootstrap_ds_arr = np.empty((n_estimators), dtype=object)
for i in range(n_estimators):
# sample bootstrapped data set
ids = np.random.choice(range(0,len(x_train)),size=len(x_train),
replace=True)
x_boot = x_train[ids]
y_boot = y_train[ids]
bootstrap_ds_arr[i] = np.unique(ids)
bag[i] = DecisionTreeRegressor()
bag[i].fit(x_boot,y_boot)
# bagging prediction
yhatbag = np.zeros(len(y_test))
for i in range(n_estimators):
yhatbag = yhatbag + bag[i].predict(x_test)
yhatbag = yhatbag/n_estimators
# out of bag loss estimation
Decision Trees and Ensemble Methods
309
oob_pred_arr = np.zeros(len(x_train))
for i in range(len(x_train)):
x = x_train[i].reshape(1, -1)
C = []
for b in range(n_estimators):
if(np.isin(i, bootstrap_ds_arr[b])==False):
C.append(b)
for pred in
bag[C]:
oob_pred_arr[i] = oob_pred_arr[i] + (pred.predict(x)/len(C))
L_oob = r2_score(y_train , oob_pre

d_arr)
print("DecisionTreeRegressor R^2 score = ",r2_score(y_test, yhat),
"\nBagging R^2 score = ", r2_score(y_test, yhatbag),
"\nBagging OOB R^2 score = ",L_oob)
DecisionTreeRegressor R^2 score =
0.575438224929718
Bagging R^2 score =
0.7612121189201985
Bagging OOB R^2 score =
0.7758253149069059
The decision tree bagging improves the test-set R2 score by about 32% (from 0.575
to 0.761). Moreover, the OOB score (0.776) is very close to the true generalization risk
(0.761) of the bagged estimator.
The bagging procedure can be further enhanced by introducing random forests, which
is discussed next.
8.6
Random Forests
In Section 8.5, we discussed the intuition behind the prediction averaging procedure. Spe-
cifically, for some feature vector x let Zb = gTb(x), b = 1, 2, . . . , B be iid prediction val-
ues, obtained from independent training sets T1, . . . , TB. Suppose that Var Zb = σ2 for all
b = 1, . . . , B. Then the variance of the average prediction value ZB is equal to σ2/B. How-
ever, if bootstrapped data sets {T ∗
b } are used instead, the corresponding random variables
{Zb} will be correlated. In particular, Zb = gT ∗
b (x) for b = 1, . . . , B are identically distrib-
uted (but not independent) with some positive pairwise correlation ϱ. It then holds that (see
Exercise 9)
Var ZB = ϱ σ2 + σ2(1 −ϱ)
B
.
(8.17)
While the second term of (8.17) goes to zero as the number of observation B increases, the
first term remains constant.
This issue is particularly relevant for bagging with decision trees. For example, con-
sider a situation in which there exists a feature that provides a very good split of the data.
Such a feature will be selected and split for every {gT ∗
b }B
b=1 at the root level and we will
consequently end up with highly correlated predictions. In such a situation, prediction
averaging will not introduce the desired improvement in the performance of the bagged
predictor.
310
Random Forests
The major idea of random forests is to perform bagging in com

bination with a “decor-
relation” of the trees by including only a subset of features during the tree construction. For
each bootstrapped training set T ∗
b we build a decision tree using a randomly selected subset
of m ⩽p features for the splitting rules. This simple but powerful idea will decorrelate the
trees, since strong predictors will have a smaller chance to be considered at the root levels.
Consequentially, we can expect to improve the predictive performance of the bagged
estimator. The resulting predictor (random forest) construction is summarized in Algorithm
8.6.1.
Algorithm 8.6.1: Random Forest Construction
Input: Training set τ = {(xi, yi)}n
i=1, the number of trees in the forest B, and the
number m ⩽p of features to be included, where p is the total number of
features in x.
Output: Ensemble of trees.
1 Generate bootstrapped training sets {T ∗
1 , . . . , T ∗
B} via Algorithm 8.5.1.
2 for b = 1 to B do
3
Train a decision tree gT ∗
b via Algorithm 8.2.1, where each split is performed
using m randomly selected features out of p.
4 return {gT ∗
b }B
b=1.
For regression problems, the output of Algorithm 8.6.1 is combined to yield the random
forest prediction function:
gRF(x) = 1
B
B
X
b=1
gT ∗
b (x).
In the classification setting, similar to Remark 8.1, we take instead the majority vote from
the {gT ∗
b }.
Example 8.4 (Random Forest for a Regression Tree) We continue with the basic
bagging Example 8.3 for a regression tree, in which we compared the decision tree es-
timator with the corresponding bagged estimator. Here, however, we use the random forest
with B = 500 trees and a subset size m = 8. It can be seen that the random forest’s R2 score
is outperforming that of the bagged estimator.
BaggingExampleRF.py
from sklearn.datasets import make_friedman1
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
# create regression problem
n_points = 1000 # points
x, y =
make_

friedman1(n_samples=n_points , n_features=15,
noise=1.0, random_state=100)
# split to train/test set
x_train , x_test, y_train , y_test = \
train_test_split(x, y, test_size=0.33, random_state=100)
Decision Trees and Ensemble Methods
311
rf = RandomForestRegressor(n_estimators=500, oob_score = True,
max_features=8,random_state=100)
rf.fit(x_train ,y_train)
yhatrf = rf.predict(x_test)
print("RF R^2 score = ", r2_score(y_test, yhatrf),
"\nRF OOB R^2 score = ", rf.oob_score_)
RF R^2 score =
0.8106589580845707
RF OOB R^2 score =
0.8260541058404149
Remark 8.3 (The Optimal Number of Subset Features m) The default values for m
are ⌊p/3⌋and
j√p
k
for regression and classification setting, respectively. However, the
standard practice is to treat m as a hyperparameter that requires tuning, depending on the
specific problem at hand [55].
Note that the procedure of bagging decision trees is a special case of a random forest
construction (see Exercise 11). Consequently, the OOB loss is readily available for random
forests.
While the advantage of bagging in the sense of enhanced accuracy is clear, we should
also consider its negative aspects and, in particular, the loss of interpretability. Specifically
a random forest consists of many trees, thus making the prediction process both hard to
visualize and interpret. For example, given a random forest, it is not easy to determine a
subset of features that are essential for accurate prediction.
The feature importance measure intends to address this issue. The idea is as follows.
Each internal node of a decision tree induces a certain decrease in the training loss; see
(8.9). Let us denote this decrease in the training loss by ∆Loss(v), where v is not a leaf node
of T. In addition, recall that for splitting rules of the type 1{xj ⩽ξ} (1 ⩽j ⩽p), each node
v is associated with a feature x j that determines the split. Using the above definitions, we
can define the feature importance
feature
importance
of x j as
IT(x j) =
X
v internal ∈T
∆

Loss(v) 1{x j is associated with v},
1 ⩽j ⩽p.
(8.18)
While (8.18) is defined for a single tree, it can be readily extended to random forests.
Specifically, the feature importance in that case will be averaged over all trees of the forest;
that is, for a forest consisting of B trees {T1, . . . , TB}, the feature importance measure is:
IRF(x j) = 1
B
B
X
b=1
ITb(xj),
1 ⩽j ⩽p.
(8.19)
Example 8.5 (Feature Importance) We consider a classification problem with 15 fea-
tures. The data is specifically designed to contain only 5 informative features out of 15.
In the code below, we apply the random forest procedure and calculate the corresponding
feature importance measures, which are summarized in Figure 8.9.
312
Random Forests
VarImportance.py
import numpy as np
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt, pylab
n_points = 1000 # create regression data with 1000 data points
x, y =
make_classification(n_samples=n_points , n_features=15,
n_informative=5, n_redundant=0, n_repeated=0, random_state=100,
shuffle=False)
rf = RandomForestClassifier(n_estimators=200, max_features="log2")
rf.fit(x,y)
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]
for f in range(15):
print("Feature %d (%f)" % (indices[f]+1, importances[indices[f
]]))
std = np.std([rf.feature_importances_ for tree in rf.estimators_],
axis=0)
f = plt.figure()
plt.bar(range(x.shape[1]), importances[indices],
color="b", yerr=std[indices], align="center")
plt.xticks(range(x.shape[1]), indices+1)
plt.xlim([-1, x.shape[1]])
pylab.xlabel("feature index")
pylab.ylabel("importance")
plt.show()
5 1 2 4 3 7 11 13 6 9 15 8 14 10 12
0
0.1
0.2
feature index
importance
Figure 8.9: Importance measure for the 15-feature data set with only 5 informative features
x1, x2, x3, x4, and x5.
Decision Trees and Ensemble Methods
313
Clearly, it is hard to visualize and understand the prediction process based on 200 trees.
How

ever, Figure 8.9 shows that the features x1, x2, x3, x4, and x5 were correctly identified
as being important.
8.7
Boosting
Boosting is a powerful idea that aims to improve the accuracy of any learning algorithm,
especially when involving weak learners
weak learners
— simple prediction functions that exhibit per-
formance slightly better than random guessing. Shallow decision trees typically yield weak
learners.
Originally, boosting was developed for binary classification tasks, but it can be readily
extended to handle general classification and regression problems. The boosting approach
has some similarity with the bagging method in the sense that boosting uses an ensemble of
prediction functions. Despite this similarity, there exists a fundamental difference between
these methods. Specifically, while bagging involves the fitting of prediction functions to
bootstrapped data, the predicting functions in boosting are learned sequentially. That is,
each learner uses information from previous learners.
The idea is to start with a simple model (weak learner) g0 for the data τ = {(xi, yi)}n
i=1
and then to improve or “boost” this learner to a learner g1 := g0 + h1. Here, the function h1
is found by minimizing the training loss for g0 + h1 over all functions h in some class of
functions H. For example, H could be the set of prediction functions that can be obtained
via a decision tree of maximal depth 2. Given a loss function Loss, the function h1 is thus
obtained as the solution to the optimization problem
h1 = argmin
h∈H
1
n
n
X
i=1
Loss (yi, g0(xi) + h (xi)).
(8.20)
This process can be repeated for g1 to obtain g2 = g1 + h2, and so on, yielding the boosted
prediction function
gB(x) = g0(x) +
B
X
b=1
hb(x).
(8.21)
Instead of using the updating step gb = gb−1 + hb, one prefers to use the smooth updating
step gb = gb−1 + γ hb, for some suitably chosen step-size parameter γ. As we shall see
shortly, this helps reduce overfitting.
Boosting can be used for regression and clas

sification problems. We start with a simple
regression setting, using the squared-error loss; thus, Loss(y,by) = (y −by)2. In this case, it
is common to start with g0(x) = n−1 Pn
i=1 yi, and each hb for b = 1, . . . , B is chosen as a
learner for the data set τb of residuals corresponding to gb−1. That is, τb :=
n
xi, e(b)
i
on
i=1,
with
e(b)
i
:= yi −gb−1(xi).
(8.22)
This leads to the following boosting procedure for regression with squared-error loss.
314
Boosting
Algorithm 8.7.1: Regression Boosting with Squared-Error Loss
Input: Training set τ = {(xi, yi)}n
i=1, the number of boosting rounds B, and a
shrinkage step-size parameter γ.
Output: Boosted prediction function.
1 Set g0(x) ←n−1 Pn
i=1 yi.
2 for b = 1 to B do
3
Set e(b)
i
←yi −gb−1(xi) for i = 1, . . . , n, and let τb ←
n
xi, e(b)
i
on
i=1.
4
Fit a prediction function hb on the training data τb.
5
Set gb(x) ←gb−1(x) + γ hb(x).
6 return gB.
The step-size parameter γ
step-size
parameter γ
introduced in Algorithm 8.7.1 controls the speed of the
fitting process. Specifically, for small values of γ, boosting takes smaller steps to-
wards the training loss minimization. The step-size γ is of great practical import-
ance, since it helps the boosting algorithm to avoid overfitting. This phenomenon is
demonstrated in Figure 8.10.
−2
−1
0
1
2
−50
0
50
train data
g1000, γ = 1
−2
−1
0
1
2
−50
0
50
train data
g1000, γ = 0.005
Figure 8.10: The left and the right panels show the fitted boosting regression model g1000
with γ = 1.0 and γ = 0.005, respectively. Note the overfitting on the left.
A very basic implementation of Algorithm 8.7.1 which reproduces Figure 8.10 is
provided below.
RegressionBoosting.py
import numpy as np
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_regression
import matplotlib.pyplot as plt
Decision Trees and Ensemble Methods
315
def TrainBoost(alpha,BoostingRounds ,x,y):
g_0 = np.mean(y)
residuals
= y-alpha

*g_0
# list of basic regressor
g_boost = []
for i in range(BoostingRounds):
h_i = DecisionTreeRegressor(max_depth=1)
h_i.fit(x,residuals)
residuals = residuals
- alpha*h_i.predict(x)
g_boost.append(h_i)
return g_0, g_boost
def Predict(g_0, g_boost ,alpha, x):
yhat = alpha*g_0*np.ones(len(x))
for j in range(len(g_boost)):
yhat = yhat+alpha*g_boost[j].predict(x)
return yhat
np.random.seed(1)
sz = 30
# create data set
x,y = make_regression(n_samples=sz, n_features=1, n_informative=1,
noise=10.0)
# boosting algorithm
BoostingRounds = 1000
alphas = [1, 0.005]
for alpha in alphas:
g_0, g_boost = TrainBoost(alpha,BoostingRounds ,x,y)
yhat = Predict(g_0, g_boost , alpha, x)
# plot
tmpX =
np.reshape(np.linspace(-2.5,2,1000) ,(1000,1))
yhatX = Predict(g_0, g_boost , alpha, tmpX)
f = plt.figure()
plt.plot(x,y,'*')
plt.plot(tmpX,yhatX)
plt.show()
The parameter γ can be viewed as a step size made in the direction of the negative
gradient of the squared-error training loss. To see this, note that the negative gradient
−∂Loss (yi, z)
∂z
z=gb−1(xi)
= −∂(yi −z)2
∂z
z=gb−1(xi)
= 2(yi −gb−1(xi))
is two times the residual e(b)
i
given in (8.22) that is used in Algorithm 8.7.1 to fit the pre-
diction function hb.
316
Boosting
In fact, one of the major advances in the theory of boosting was the recognition that
one can use a similar gradient descent method for any differentiable loss function. The
resulting algorithm is called gradient boosting
gradient
boosting
. The general gradient boosting algorithm is
summarized in Algorithm 8.7.2. The main idea is to mimic a gradient descent algorithm
in the following sense. At each stage of the boosting procedure, we calculate a negative
☞412
gradient on n training points x1, . . . , xn (Lines 3–4). Then, we fit a simple model (such as
a shallow decision tree) to approximate the gradient (Line 5) for any feature x. Finally,
similar to the gradient descent method, we make a γ-sized step in the direction of the
negative gradient (Line 6).


Algorithm 8.7.2: Gradient Boosting
Input: Training set τ = {(xi, yi)}n
i=1, the number of boosting rounds B, a
differentiable loss function Loss(y,by), and a gradient step-size parameter γ.
Output: Gradient boosted prediction function.
1 Set g0(x) ←0.
2 for b = 1 to B do
3
for i = 1 to n do
4
Evaluate the negative gradient of the loss at (xi, yi) via
r(b)
i
←−∂Loss (yi, z)
∂z
z=gb−1(xi)
i = 1, . . . , n.
5
Approximate the negative gradient by solving
hb = argmin
h∈H
1
n
n
X
i=0

r(b)
i
−h (xi)
2.
(8.23)
6
Set gb(x) ←gb−1(x) + γ hb(x).
7 return gB
Example 8.6 (Gradient Boosting for a Regression Tree) Let us continue with the ba-
sic bagging and random forest examples for a regression tree (Examples 8.3 and 8.4), where
we compared the standard decision tree estimator with the corresponding bagging and ran-
dom forest estimators. Now, we use the gradient boosting estimator from Algorithm 8.7.2,
as implemented in sklearn. We use γ = 0.1 and perform B = 100 boosting rounds. As
a prediction function hb for b = 1, . . . , B we use small regression trees of depth at most
3. Note that such individual trees do not usually give good performance; that is, they are
weak prediction functions. We can see that the resulting boosting prediction function gives
the R2 score equal to 0.899, which is better than R2 scores of simple decision tree (0.5754),
the bagged tree (0.761), and the random forest (0.8106).
GradientBoostingRegression.py
import numpy as np
from sklearn.datasets import make_friedman1
from sklearn.tree import DecisionTreeRegressor
Decision Trees and Ensemble Methods
317
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
# create regression problem
n_points = 1000 # points
x, y =
make_friedman1(n_samples=n_points , n_features=15,
noise=1.0, random_state=100)
# split to train/test set
x_train , x_test, y_train , y_test = \
train_test_split(x, y, test_size=0.33, random_state=100)
# boosting sklearn
from sklearn.ensemble impor

t GradientBoostingRegressor
breg = GradientBoostingRegressor(learning_rate=0.1,
n_estimators=100, max_depth =3, random_state=100)
breg.fit(x_train ,y_train)
yhat = breg.predict(x_test)
print("Gradient Boosting R^2 score = ",r2_score(y_test, yhat))
Gradient Boosting R^2 score =
0.8993055635639531
We proceed with the classification setting and consider the original boosting algorithm:
AdaBoost
AdaBoost
. The inventors of the AdaBoost method considered a binary classification prob-
lem, where the response variable belongs to the {−1, 1} set. The idea of AdaBoost is similar
to the one presented in the regression setting, that is, AdaBoost fits a sequence of prediction
functions g0, g1 = g0 + h1, g2 = g0 + h1 + h2, . . . with final prediction function
gB(x) = g0(x) +
B
X
b=1
hb(x),
(8.24)
where each function hb is of the form hb(x) = αb cb(x), with αb ∈R+ and where cb is a
proper (but weak) classifier in some class C. Thus, cb(x) ∈{−1, 1}. Exactly as in (8.20), we
solve at each boosting iteration the optimization problem
(αb, cb) = argmin
α⩾0, c∈C
1
n
n
X
i=1
Loss (yi, gb−1(xi) + α c(xi)).
(8.25)
However, in this case the loss function is defined as Loss(y,by) = e−yby. The algorithm starts
with a simple model g0 := 0 and for each successive iteration b = 1, . . . , B solves (8.25).
Thus,
(αb, cb) = argmin
α⩾0, c∈C
n
X
i=1
e−yi gb−1(xi)
|    {z    }
w(b)
i
e−yi α c(xi) = argmin
α⩾0, c∈C
n
X
i=1
w(b)
i
e−yiα c(xi),
where w(b)
i
:= exp{−yi gb−1(xi)} does not depend on α or c. It follows that
(αb, cb) = argmin
α⩾0, c∈C
e−α
n
X
i=1
w(b)
i 1{c(xi) = yi} + eα
n
X
i=1
w(b)
i 1{c(xi) , yi}
= argmin
α⩾0, c∈C
(eα −e−α) ℓ(b)
τ (c) + e−α,
(8.26)
318
Boosting
where
ℓ(b)
τ (c) :=
Pn
i=1 w(b)
i
1{c(xi) , yi}
Pn
i=1 w(b)
i
can be interpreted as the weighted zero–one training loss at iteration b.
For any α ⩾0, the program (8.26) is minimized by a classifier c ∈C that minimizes
this weighted training loss; that is,
cb(x) = argmin
c∈C
ℓ(b)
τ .
(8.27)
Substituting (8.27) into (8.26) and solv

ing for the optimal α gives
αb = 1
2 ln
 1 −ℓ(b)
τ (cb)
ℓ(b)
τ (cb)
!
.
(8.28)
This gives the AdaBoost algorithm, summarized below.
Algorithm 8.7.3: AdaBoost
Input: Training set τ = {(xi, yi)}n
i=1, and the number of boosting rounds B.
Output: AdaBoost prediction function.
1 Set g0(x) ←0.
2 for i = 1 to n do
3
w(1)
i
←1/n
4 for b = 1 to B do
5
Fit a classifier cb on the training set τ by solving
cb = argmin
c∈C
ℓ(b)
τ (c) = argmin
c∈C
Pn
i=1 w(b)
i
1{c(xi) , yi}
Pn
i=1 w(b)
i
.
6
Set αb ←1
2 ln
 1 −ℓ(b)
τ (cb)
ℓ(b)
τ (cb)
!
.
// Update weights
7
for i = 1 to n do
8
w(b+1)
i
←w(b)
i
exp{−yi αb cb(xi)}.
9 return gB(x) := PB
b=1 αb cb(x).
Algorithm 8.7.3 is quite intuitive. At the first step (b = 1), AdaBoost assigns an equal
weight w(1)
i
= 1/n to each training sample (xi, yi) in the set τ = {(xi, yi)}n
i=1. Note that, in
this case, the weighted zero–one training loss is equal to the regular zero–one training loss.
At each successive step b > 1, the weights of observations that were incorrectly classified
by the previous boosting prediction function gb are increased, and the weights of correctly
classified observations are decreased. Due to the use of the weighted zero–one loss, the set
of incorrectly classified training samples will receive an extra weight and thus have a better
chance of being classified correctly by the next classifier cb+1. As soon as the AdaBoost
algorithm finds the prediction function gB, the final classification is delivered via
sign

B
X
b=1
αb cb(x)
.
Decision Trees and Ensemble Methods
319
The step-size parameter αb found by the AdaBoost algorithm in Line 6 can be
viewed as an optimal step-size in the sense of training loss minimization. How-
ever, similar to the regression setting, one can slow down the AdaBoost algorithm
by setting αb to be a fixed (small) value αb = γ. As usual, when the latter is done in
practice, it is tackling the problem of overfitting.
We consider an implementation of Algorithm 8.7.3 for a binary classi

fication problem.
Specifically, during all boosting rounds, we use simple decision trees of depth 1 (also called
decision tree stumps
stumps
) as weak learners. The exponential and zero–one training losses as a
function of the number of boosting rounds are presented in Figure 8.11.
AdaBoost.py
from sklearn.datasets import make_blobs
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import zero_one_loss
import numpy as np
def ExponentialLoss(y,yhat):
n = len(y)
loss = 0
for i in range(n):
loss = loss+np.exp(-y[i]*yhat[i])
loss = loss/n
return loss
# create binary classification problem
np.random.seed(100)
n_points = 100 # points
x, y =
make_blobs(n_samples=n_points , n_features=5,
centers=2,
cluster_std=20.0, random_state=100)
y[y==0]=-1
# AdaBoost implementation
BoostingRounds = 1000
n = len(x)
W = 1/n*np.ones(n)
Learner = []
alpha_b_arr = []
for i in range(BoostingRounds):
clf = DecisionTreeClassifier(max_depth=1)
clf.fit(x,y, sample_weight=W)
Learner.append(clf)
train_pred = clf.predict(x)
err_b = 0
320
Boosting
for i in range(n):
if(train_pred[i]!=y[i]):
err_b = err_b+W[i]
err_b = err_b/np.sum(W)
alpha_b = 0.5*np.log((1-err_b)/err_b)
alpha_b_arr.append(alpha_b)
for i in range(n):
W[i] = W[i]*np.exp(-y[i]*alpha_b*train_pred[i])
yhat_boost = np.zeros(len(y))
for j in range(BoostingRounds):
yhat_boost = yhat_boost+alpha_b_arr[j]*Learner[j].predict(x)
yhat = np.zeros(n)
yhat[yhat_boost >=0] = 1
yhat[yhat_boost <0] = -1
print("AdaBoost Classifier exponential loss = ", ExponentialLoss(y,
yhat_boost))
print("AdaBoost Classifier zero--one loss = ",zero_one_loss(y,yhat))
AdaBoost Classifier exponential loss =
0.004224013663777142
AdaBoost Classifier zero--one loss =
0.0
200
400
600
800
1,000
0
0.2
0.4
0.6
0.8
1
B
loss
exponential loss
zero-one loss
Figure 8.11: Exponential and zero–one training loss as a function of the number of boosting
rounds B for a binary classification problem.
Decision Trees an

d Ensemble Methods
321
Further Reading
Breiman’s book on decision trees, [20], serves as a great starting point. Some additional
advances can be found in [62, 96]. From the computational point of view, there exists
an efficient recursive procedure for tree pruning; see Chapters 3 and 10 in [20]. Several
advantages and disadvantages of using decision trees are debated in [37, 55]. A detailed
discussion on bagging and random forests can be found in [21] and [23], respectively.
Freund and Schapire [44] provide the first boosting algorithm, the AdaBoost. While Ad-
aBoost was developed in the context of the computational complexity of learning, it was
later discovered by Friedman [45] that AdaBoost is a special case of an additive model.
In addition, it was shown that for any differentiable loss function, there exists an efficient
boosting procedure which mimics the gradient descent algorithm. The foundation of the
resulting gradient boosting method is detailed in [45]. Python packages that implement
gradient boosting include XGBoost and LightGBM.
Exercises
1. Show that any training set τ = {(x, yi), i = 1, . . . , n} can be fitted via a tree with zero
training loss.
2. Suppose during the construction of a decision tree we wish to specify a constant re-
gional prediction function gw on the region Rw, based on the training data in Rw, say
{(x1, y1), . . . , (xk, yk)}. Show that gw(x) := k−1 Pk
i=1 yi minimizes the squared-error loss.
3. Using the program from Section 8.2.4, write a basic implementation of a decision tree
for a binary classification problem. Implement the misclassification, Gini index, and en-
tropy impurity criteria to split nodes. Compare the results.
4. Suppose in the decision tree of Example 8.1, there are 3 blue and 2 red data points in
a certain tree region. Calculate the misclassification impurity, the Gini impurity, and the
entropy impurity. Repeat these calculations for 2 blue and 3 red data points.
5. Consider the procedure of finding the best sp

litting rule for a categorical variable with
k labels from Section 8.3.4. Show that one needs to consider 2k subsets of {1, . . . , k} to find
the optimal partition of labels.
6. Reproduce Figure 8.6 using the following classification data.
from sklearn.datasets import make_blobs
X, y =
make_blobs(n_samples=5000, n_features=10, centers=3,
random_state=10, cluster_std=10)
7. Prove (8.13); that is, show that
X
w∈W

n
X
i=1
1{xi ∈Rw} Loss(yi, gw(xi))
= n ℓτ (g) .
322
Exercises
8. Suppose τ is a training set with n elements and τ∗, also of size n, is obtained from τ
by bootstrapping; that is, resampling with replacement. Show that for large n, τ∗does not
contain a fraction of about e−1 ≈0.37 of the points from τ.
9. Prove Equation (8.17).
10. Consider the following training/test split of the data. Construct a random forest re-
gressor and identify the optimal subset size m in the sense of R2 score (see Remark 8.3).
import numpy as np
from sklearn.datasets import make_friedman1
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
# create regression problem
n_points = 1000 # points
x, y =
make_friedman1(n_samples=n_points , n_features=15,
noise=1.0, random_state=100)
# split to train/test set
x_train , x_test, y_train , y_test = \
train_test_split(x, y, test_size=0.33, random_state=100)
11. Explain why bagging decision trees are a special case of random forests.
12. Show that (8.28) holds.
13. Consider the following classification data and module imports:
from sklearn.datasets import make_blobs
from sklearn.metrics import zero_one_loss
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import GradientBoostingClassifier
X_train , y_train =
make_blobs(n_samples=5000, n_features=10,
centers=3, random_state=10, cluster_std=5)
Using the gradient boosting algorithm with B = 100 rounds, plot the training los

s as a
function of γ, for γ = 0.1, 0.3, 0.5, 0.7, 1. What is your conclusion regarding the relation
between B and γ?
CHAPTER9
DEEP LEARNING
In this chapter, we show how one can construct a rich class of approximating func-
tions called neural networks. The learners belonging to the neural-network class of
functions have attractive properties that have made them ubiquitous in modern machine
learning applications — their training is computationally feasible and their complexity
is easy to control and fine-tune.
9.1
Introduction
In Chapter 2 we described the basic supervised learning task; namely, we wish to predict a
random output Y from a random input X, using a prediction function g : x 7→y that belongs
to a suitably chosen class of approximating functions G. More generally, we may wish to
predict a vector-valued output y using a prediction function g : x 7→y from class G.
In this chapter y denotes the vector-valued output for a given input x. This differs
from our previous use (e.g., in Table 2.1), where y denotes a vector of scalar outputs.
In the machine learning context, the class G is sometimes referred to as the hypothesis
space or the universe of possible models, and the representational capacity of a hypothesis
representational
capacity
space G is simply its complexity.
Suppose that we have a class of functions GL, indexed by a parameter L that controls
the complexity of the class, so that GL ⊂GL+1 ⊂GL+2 ⊂· · · . In selecting a suitable
class of functions, we have to be mindful of the approximation–estimation tradeoff. On the
☞
31
one hand, the class GL must be complex (rich) enough to accurately represent the optimal
unknown prediction function g∗, which may require a very large L. On the other hand, the
learners in the class GL must be simple enough to train with small estimation error and
with minimal demands on computer memory, which may necessitate a small L.
In balancing these competing objectives, it helps if the more complex class GL+1 is
easily cons

tructed from an already existing and simpler GL. The simpler class of functions
GL may itself be constructed by modifying an even simpler class GL−1, and so on.
A class of functions that permits such a natural hierarchical construction is the class of
neural networks. Conceptually, a neural network with L layers is a nonlinear parametric
neural
networks
regression model whose representational capacity can easily be controlled by L.
☞188
323
324
Introduction
Alternatively, in (9.3) we will define the output of a neural network as the repeated
composition of linear and (componentwise) nonlinear functions. As we shall see, this rep-
resentation of the output will provide a flexible class of nonlinear functions that can be
easily differentiated. As a result, the training of learners via gradient optimization methods
involves mostly standard matrix operations that can be performed very efficiently.
☞412
Historically, neural networks were originally intended to mimic the workings of the
human brain, with the network nodes modeling neurons and the network links modeling
the axons connecting neurons. For this reason, rather than using the terminology of the
regression models in Chapter 5, we prefer to use a nomenclature inspired by the apparent
resemblance of neural networks to structures in the human brain.
We note, however, that the attempts at building efficient machine learning algorithms by
mimicking the functioning of the human brain have been as unsuccessful as the attempts
at building flying aircraft by mimicking the flapping of birds’ wings. Instead, many ef-
fective machine algorithms have been inspired by age-old mathematical ideas for function
approximation. One such idea is the following fundamental result (see [119] for a proof).
Theorem 9.1: Kolmogorov (1957)
Every continuous function g∗: [0, 1]p 7→R with p ⩾2 can be written as
g∗(x) =
2p+1
X
j=1
h j

p
X
i=1
hi j(xi)
,
where {hj, hij} is a set of univariate continuous functions that depend on g

∗.
This result tells us that any continuous high-dimensional map can be represented as
the function composition of much simpler (one-dimensional) maps. The composition of
the maps needed to compute the output g∗(x) for a given input x ∈Rp are depicted in
Figure 9.1, showing a directed graph or neural network with three layers, denoted as l =
0, 1, 2.
Deep Learning
325
hi j
xi
xp
z j
hj→a j
z1
h1→a1
zq
hq→aq
g∗(x)
x1
hpq
h11
Figure 9.1: Every continuous function g∗: [0, 1]p 7→R can be represented by a neural net-
work with one hidden layer (l = 1), an input layer (l = 0), and an output layer (l = 2).
In particular, each of the p components of the input x is represented as a node in the
input layer (l = 0). In the hidden layer (l = 1) there are q := 2p + 1 nodes, each of which
hidden layer
is associated with a pair of variables (z, a) with values
zj :=
p
X
i=1
hi j(xi)
and
aj := hj(zj).
A link between nodes (zj, aj) and xi with weight hi j signifies that the value of zj depends
on the value of xi via the function hi j. Finally, the output layer (l = 2) represents the value
g∗(x) = Pq
j=1 aj. Note that the arrows on the graph remind us that the sequence of the
computations is executed from left to right, or from the input layer l = 0 through to the
output layer l = 2.
In practice, we do not know the collection of functions {hj, hi j}, because they depend
on the unknown g∗. In the unlikely event that g∗is linear, then all of the (2p + 1)(p + 1)
one-dimensional functions will be linear as well. However, in general, we should expect
that each of the functions in {hj, hi j} is nonlinear.
Unfortunately, Theorem 9.1 only asserts the existence of {hj, hi j}, and does not tell us
how to construct these nonlinear functions. One way out of this predicament is to replace
these (2p + 1)(p + 1) unknown functions with a much larger number of known nonlinear
functions called activation functions.1 For example, a logistic activation function is
activation
functions
S (z) = (1 + exp(−z

))−1.
We then hope that such a network, built from a sufficiently large number of activation
functions, will have similar representational capacity as the neural network in Figure 9.1
with (2p + 1)(p + 1) functions.
In general, we wish to use the simplest activation functions that will allow us to build
a learner with large representational capacity and low training cost. The logistic function
1Activation functions derive their name from models of a neuron’s response when exposed to chemical
or electric stimuli.
326
Feed-Forward Neural Networks
is merely one possible choice for an activation function from among infinite possibilit-
ies. Figure 9.2 shows a small selection of activation functions with different regularity or
smoothness properties.
Heaviside or unit step
rectified linear unit (ReLU)
logistic
-2
0
2
0
0.5
1
-2
0
2
0
1
2
3
-2
0
2
0
0.5
1
1{z ⩾0}
z × 1{z ⩾0}
(1 + exp(−z))−1
Figure 9.2: Some common activation functions S (z) with their defining formulas and plots.
The logistic function is an example of a sigmoid (that is, an S-shaped) function. Some
books define the logistic function as 2S (z) −1 (in terms of our definition).
In addition to choosing the type and number of activation functions in a neural network,
we can improve its representational capacity in another important way: introduce more
hidden layers. In the next section we explore this possibility in detail.
9.2
Feed-Forward Neural Networks
In a neural network with L+1 layers, the zero or input layer (l = 0) encodes the input feature
vector x, and the last or output layer (l = L) encodes the (multivalued) output function g(x).
The remaining layers are called hidden layers. Each layer has a number of nodes, say pl
nodes for layer l = 0, . . . , L. In this notation, p0 is the dimension of the input feature vector
x and, for example, pL = 1 signifies that g(x) is a scalar output. All nodes in the hidden
layers (l = 1, . . . , L −1) are associated with a pair of variables (z, a), which we gather
in

to pl-dimensional column vectors zl and al. In the so-called feed-forward
feed-forward
networks, the
variables in any layer l are simple functions of the variables in the preceding layer l −1. In
particular, zl and al−1 are related via the linear relation zl = Wl al−1 + bl, for some weight
matrix Wl and bias vector bl.
weight matrix
bias vector
Within any hidden layer l = 1, . . . , L −1, the components of the vectors zl and al
are related via al = Sl(zl), where Sl : Rpl 7→Rpl is a nonlinear multivalued function. All of
these multivalued functions are typically of the form
Sl(z) = [S (z1), . . . , S (zdim(z))]⊤,
l = 1, . . . , L −1,
(9.1)
where S is an activation function common to all hidden layers. The function SL : RpL−1 7→
RpL in the output layer is more general and its specification depends, for example, on
whether the network is used for classification or for the prediction of a continuous output
Y. A four-layer (L = 3) network is illustrated in Figure 9.3.
Deep Learning
327
Input layer
Hidden layers
Output layer
bias
weight
b1, j
z3,m
S3
w1, ji
z2,k
S→a2,k
b2,k
b3,m
z1, j
S→a1, j
w2,k j
w3,mk
g(x)
xi
Figure 9.3: A neural network with L = 3: the l = 0 layer is the input layer, followed by two
hidden layers, and the output layer. Hidden layers may have different numbers of nodes.
The output of this neural network is determined by the input vector x, (nonlinear)
functions {Sl}, as well as weight matrices Wl = [wl,i j] and bias vectors bl = [bl,j] for
l = 1, 2, 3.
Here, the (i, j)-th element of the weight matrix Wl = [wl,i j] is the weight that con-
nects the j-th node in the (l −1)-st layer with the i-th node in the l-th layer.
The name given to L (the number of layers without the input layer) is the network depth
network depth
and maxl pl is called the network width. While we mostly study networks that have an equal
network width
number of nodes in the hidden layers (p1 = · · · = pL−1), in general there can be different
numbers of nodes in each hidden layer.
Th

e output g(x) of a multiple-layer neural network is obtained from the input x via the
following sequence of computations:
x
|{z}
a0
→W1 a0 + b1
|       {z       }
z1
→S1(z1)
|{z}
a1
→W2 a1 + b2
|       {z       }
z2
→S2(z2)
|{z}
a2
→· · ·
→WL aL−1 + bL
|          {z          }
zL
→SL(zL)
|{z}
aL
= g(x).
(9.2)
Denoting the function z 7→Wl z + bl by Ml, the output g(x) can thus be written as the
function composition
g(x) = SL ◦ML ◦· · · ◦S2 ◦M2 ◦S1 ◦M1(x).
(9.3)
The algorithm for computing the output g(x) for an input x is summarized next. Note
that we leave open the possibility that the activation functions {Sl} have different definitions
328
Feed-Forward Neural Networks
for each layer. In some cases, Sl may even depend on some or all of the already computed
z1, z2, . . . and a1, a2, . . ..
Algorithm 9.2.1: Feed-Forward Propagation for a Neural Network
input: Feature vector x; weights {wl,i j}, biases {bl,i} for each layer l = 1, . . . , L.
output: The value of the prediction function g(x).
1 a0 ←x
// the zero or input layer
2 for l = 1 to L do
3
Compute the hidden variable zl,i for each node i in layer l:
zl ←Wl al−1 + bl
4
Compute the activation function al,i for each node i in layer l:
al ←Sl(zl)
5 return g(x) ←aL
// the output layer
Deep Learning
329
Example 9.1 (Nonlinear Multi-Output Regression) Given the input x ∈Rp0 and an
activation function S : R 7→R, the output g(x) := [g1(x), . . . , gp2(x)]⊤of a nonlinear multi-
output regression model can be computed via a neural network with:
☞214
z1 = W1 x + b1,
where W1 ∈Rp1×p0, b1 ∈Rp1,
a1,k = S (z1,k),
k = 1, . . . , p1,
g(x) = W2 a1 + b2,
where W2 ∈Rp2×p1, b2 ∈Rp2,
which is a neural network with one hidden layer and output function S2(z) = z. In the
special case where p1 = p2 = 1, b2 = 0, W2 = 1, and we collect all parameters into the
vector θ⊤= [b1, W1] ∈Rp0+1, the neural network can be interpreted as a generalized linear
model with E[Y | X = x] = h([1, x⊤] θ) for some activation function h.
☞204
Example 9.2 (Mul

ti-Logit Classification) Suppose that, for a classification problem,
an input x has to be classified into one of c classes, labeled 0, . . . , c−1. We can perform the
classification via a neural network with one hidden layer, with p1 = c nodes. In particular,
we have
z1 = W1 x + b1,
a1 = S1(z1),
where S1 is the softmax function:
softmax
softmax : z 7→
exp(z)
P
k exp(zk).
For the output, we take g(x) = [g1(x), . . . , gc(x)]⊤= a1, which can then be used as a
pre-classifier of x. The actual classifier of x into one of the categories 0, 1, . . . , c−1 is then
☞252
argmax
k ∈{0,...,c−1}
gk+1(x).
This is equivalent to the multi-logit classifier in Section 7.5. Note, however, that there we
☞266
used a slightly different notation, with ex instead of x and we have a reference class; see
Exercise 13.
In practical implementations, the softmax function can cause numerical over- and
under-flow errors when either one of the exp(zk) happens to be extremely large or
P
k exp(zk) happens to be very small. In such cases we can exploit the invariance
property (Exercise 1):
softmax(z) = softmax(z + c × 1)
for any constant c.
Using this property, we can compute softmax(z) with greater numerical stability via
softmax(z −maxk{zk} × 1).
When neural networks are used for classification into c classes and the number of out-
put nodes is c −1, then the gi(x) may be viewed as nonlinear discriminant functions.
☞260
330
Feed-Forward Neural Networks
Example 9.3 (Density Estimation) Estimating the density f of some random feature
X ∈R is the prototypical unsupervised learning task, which we tackled in Section 4.5.2 us-
ing Gaussian mixture models. We can view a Gaussian mixture model with p1 components
☞137
and a common scale parameter σ > 0 as a neural network with two hidden layers, similar
to the one on Figure 9.3. In particular, if the activation function in the first hidden layer,
S1, is of the form (9.1) with S (z) := exp(−z2/(2σ2))/
√
2πσ2, then the density value g(x) is
computed via:
z1 = 

W1 x + b1,
a1 = S1(z1),
z2 = W2 a1 + b2,
a2 = S2(z2),
g(x) = a⊤
1 a2,
where W1 = 1 is a p1×1 column vector of ones, W2 = O is a p1×p1 matrix of zeros, and S2
is the softmax function. We identify the column vector b1 with the p1 location parameters,
[µ1, . . . , µp1]⊤of the Gaussian mixture and b2 ∈Rp1 with the p1 weights of the mixture.
Note the unusual activation function of the output layer — it requires the value of a1 from
the first hidden layer and a2 from the second hidden layer.
There are a number of key design characteristics of a feed-forward network. First, we
need to choose the activation function(s). Second, we need to choose the loss function for
the training of the network. As we shall explain in the next section, the most common
choices are the ReLU activation function and the cross-entropy loss. Crucially, we need
to carefully construct the network architecture — the number of connections among the
network
architecture
nodes in different layers and the overall number of layers of the network.
For example, if the connections from one layer to the next are pruned (called sparse
connectivity) and the links share the same weight values {wl,i j} (called parameter sharing)
for all {(i, j) : |i −j| = 0, 1, . . .}, then the weight matrices will be sparse and Toeplitz.
☞379
Intuitively, the parameter sharing and sparse connectivity can speed up the training of
the network, because there are fewer parameters to learn, and the Toeplitz structure permits
quick computation of the matrix-vector products in Algorithm 9.2.1. An important example
of such a network is the convolution neural network (CNN), in which some or all of the
convolution
neural
network
network layers encode the linear operation of convolution:
Wl al−1 = wl ∗al−1,
where [x ∗y]i := P
k xkyi−k+1. As discussed in Example A.10, a convolution matrix is a
☞380
special type of sparse Toeplitz matrix, and its action on a vector of learning parameters can
be evaluated quickly via the fast Fourier transfo

rm.
☞394
CNNs are particularly suited to image processing problems, because their convolution
layers closely mimic the neurological properties of the visual cortex. In particular, the
cortex partitions the visual field into many small regions and assigns a group of neurons to
every such region. Moreover, some of these groups of neurons respond only to the presence
of particular features (for example, edges).
This neurological property is naturally modeled via convolution layers in the neural
network. Specifically, suppose that the input image is given by an m1 ×m2 matrix of pixels.
Now, define a k × k matrix (sometimes called a kernel, where k is generally taken to be 3
or 5). Then, the convolution layer output can be calculated using the discrete convolution
Deep Learning
331
of all possible k × k input matrix regions and the kernel matrix; (see Example A.10). In
particular, by noting that there are (m1 −k+1)×(m2 −k+1) possible regions in the original
image, we conclude that the convolution layer output size is (m1 −k + 1) × (m2 −k + 1).
In practice, we frequently define several kernel matrices, giving an output layer of size
(m1 −k + 1) × (m2 −k + 1) × (the number of kernels). Figure 9.4 shows a 5 × 5 input image
and a 2 × 2 kernel with a 4 × 4 output matrix. An example of using a CNN for image
classification is given in Section 9.5.2.
Figure 9.4: An example 5×5 input image and a 2×2 kernel. The kernel is applied to every
2 × 2 region of the original image.
9.3
Back-Propagation
The training of neural networks is a major challenge that requires both ingenuity and much
experimentation. The algorithms for training neural networks with great depth are collect-
ively referred to as deep learning methods. One of the simplest and most effective methods
deep learning
for training is via steepest descent and its variations.
☞412
Steepest descent requires computation of the gradient with respect to all bias vectors
and weight matrices. Given the potentially large number of 

parameters (weight and bias
terms) in a neural network, we need to find an efficient method to calculate this gradient.
To illustrate the nature of the gradient computations, let θ = {Wl, bl} be a column vec-
tor of length dim(θ) = PL
l=1(pl−1pl + pl) that collects all the weight parameters (number-
ing PL
l=1 pl−1pl) and bias parameters (numbering PL
l=1 pl) of a multiple-layer network with
training loss:
ℓτ(g(· | θ)) := 1
n
n
X
i=1
Loss(yi, g(xi | θ)).
Writing Ci(θ) := Loss(yi, g(xi | θ)) for short (using C for cost), we have
ℓτ(g(· | θ)) = 1
n
n
X
i=1
Ci(θ),
(9.4)
so that obtaining the gradient of ℓτ requires computation of ∂Ci/∂θ for every i. For ac-
tivation functions of the form (9.1), define Dl as the diagonal matrix with the vector of
derivatives
S′(z) := [S ′(zl,1), . . . , S ′(zl,pl)]⊤
down its main diagonal; that is,
Dl := diag(S ′(zl,1), . . . , S ′(zl,pl)),
l = 1, . . . , L −1.
332
Back-Propagation
The following theorem provides us with the formulas needed to compute the gradient of a
typical Ci(θ).
Theorem 9.2: Gradient of Training Loss
For a given (input, output) pair (x, y), let g(x | θ) be the output of Algorithm 9.2.1,
and let C(θ) = Loss(y, g(x | θ)) be an almost-everywhere differentiable loss func-
tion. Suppose {zl, al}L
l=1 are the vectors obtained during the feed-forward propagation
(a0 = x, aL = g(x | θ)). Then, we have for l = 1, . . . , L:
∂C
∂Wl
= δl a⊤
l−1
and
∂C
∂bl
= δl,
where δl := ∂C/∂zl is computed recursively for l = L, . . . , 2:
δl−1 = Dl−1W⊤
l δl
with
δL = ∂SL
∂zL
∂C
∂g.
(9.5)
Proof: The scalar value C is obtained from the transitions (9.2), followed by the mapping
g(x | θ) 7→Loss(y, g(x | θ)). Using the chain rule (see Appendix B.1.2), we have
☞400
δL = ∂C
∂zL
= ∂g(x)
∂zL
∂C
∂g(x) = ∂SL
∂zL
∂C
∂g.
Recall that the vector/vector derivative of a linear mapping z 7→Wz is given by W⊤; see
(B.5). It follows that, since zl = Wl al−1 + bl and al = S(zl), the chain rule gives
☞399
∂zl
∂zl−1
= ∂al−1
∂zl−1
∂zl
∂al−1
= Dl−1W⊤
l .
Hence, the

 recursive formula (9.5):
δl−1 = ∂C
∂zl−1
= ∂zl
∂zl−1
∂C
∂zl
= Dl−1W⊤
l δl,
l = L, . . . , 3, 2.
Using the {δl}, we can now compute the derivatives with respect to the weight matrices
and the biases. In particular, applying the “scalar/matrix” differentiation rule (B.10) to
zl = Wl al−1 + bl gives:
∂C
∂Wl
= ∂C
∂zl
∂zl
∂Wl
= δl a⊤
l−1,
l = 1, . . . , L
and
∂C
∂bl
= ∂zl
∂bl
∂C
∂zl
= δl,
l = 1, . . . , L.
□
From the theorem we can see that for each pair (x, y) in the training set, we can compute the
gradient ∂C/∂θ in a sequential manner, by computing δL, . . . , δ1. This procedure is called
back-propagation
back-
propagation
. Since back-propagation mostly involves simple matrix multiplication, it
Deep Learning
333
can be efficiently implemented using dedicated computing hardware such as graphical pro-
cessor units (GPUs) and other parallel computing architecture. Note also that many matrix
computations that run in quadratic time can be replaced with linear-time componentwise
multiplication. Specifically, multiplication of a vector with a diagonal matrix is equivalent
to componentwise multiplication:
A
|{z}
diag(a)
b = a ⊙b.
Consequently, we can write δl−1 = Dl−1W⊤
l δl as: δl−1 = S′(zl−1) ⊙W⊤
l δl, l = L, . . . , 3, 2.
We now summarize the back-propagation algorithm for the computation of a typical
∂C/∂θ. In the following algorithm, Lines 1 to 5 are the feed-forward part of the algorithm,
and Lines 7 to 10 are the back-propagation part of the algorithm.
Algorithm 9.3.1: Computing the Gradient of a Typical C(θ)
input: Training example (x, y), weight matrices and bias vectors {Wl, bl}L
l=1 =: θ,
activation functions {Sl}L
l=1.
output: The derivatives with respect to all weight matrices and bias vectors.
1 a0 ←x
2 for l = 1, . . . , L do
// feed-forward
3
zl ←Wl al−1 + bl
4
al ←Sl(zl)
5 δL ←∂SL
∂zL
∂C
∂g
6 z0 ←0
// arbitrary assignment needed to finish the loop
7 for l = L, . . . , 1 do
// back-propagation
8
∂C
∂bl ←δl
9
∂C
∂Wl ←δl a⊤
l−1
10
δl−1 ←S′(zl−1) ⊙W⊤
l δl
11 re

turn
∂C
∂Wl and ∂C
∂bl for all l = 1, . . . , L and the value g(x) ←aL (if needed)
Note that for the gradient of C(θ) to exist at every point, we need the activation func-
tions to be differentiable everywhere. This is the case, for example, for the logistic activa-
tion function in Figure 9.2. It is not the case for the ReLU function, which is differentiable
everywhere, except at z = 0. However, in practice, the kink of the ReLU function at z = 0
is unlikely to trip the back-propagation algorithm, because rounding errors and the finite-
precision computer arithmetic make it extremely unlikely that we will need to evaluate the
ReLU at precisely z = 0. This is the reason why in Theorem 9.2 we merely required that
C(θ) is almost-everywhere differentiable.
In spite of its kink at the origin, the ReLU has an important advantage over the logistic
function. While the derivative of the logistic function decays exponentially fast to zero as
we move away from the origin, a phenomenon referred to as saturation, the derivative of
saturation
the ReLU function is always unity for positive z. Thus, for large positive z, the derivative of
the logistic function does not carry any useful information, but the derivative of the ReLU
can help guide a gradient optimization algorithm. The situation for the Heaviside function
in Figure 9.2 is even worse, because its derivative is completely noninformative for any
z , 0. In this respect, the lack of saturation of the ReLU function for z > 0 makes it a
desirable activation function for training a network via back-propagation.
334
Back-Propagation
Finally, note that to obtain the gradient ∂ℓτ/∂θ of the training loss, we simply need to
loop Algorithm 9.3.1 over all the n training examples, as follows.
Algorithm 9.3.2: Computing the Gradient of the Training Loss
input: Training set τ = {(xi, yi)}n
i=1, weight matrices and bias vectors
{Wl, bl}L
l=1 =: θ, activation functions {Sl}L
l=1.
output: The gradient of the training loss.
1 for i = 1, . 

. . , n do
// loop over all training examples
2
Run Algorithm 9.3.1 with input (xi, yi) to compute
n ∂Ci
∂Wl, ∂Ci
∂bl
oL
l=1
3 return
∂C
∂Wl = 1
n
Pn
i=1
∂Ci
∂Wl and ∂C
∂bl = 1
n
Pn
i=1
∂Ci
∂bl for all l = 1, . . . , L
Example 9.4 (Squared-Error and Cross-Entropy Loss)
The back-propagation Al-
gorithm 9.3.1 requires a formula for δL in line 5. In particular, to execute line 5 we need to
specify both a loss function and an SL that defines the output layer: g(x | θ) = aL = SL(zL).
For instance, in the multi-logit classification of inputs x into pL categories labeled
☞266
0, 1, . . . , (pL −1), the output layer is defined via the softmax function:
SL : zL 7→
exp(zL)
PpL
k=1 exp(zL,k).
In other words, g(x | θ) is a probability vector such that its (y+1)-st component gy+1(x | θ) =
g(y | θ, x) is the estimate or prediction of the true conditional probability f(y | x). Combin-
ing the softmax output with the cross-entropy loss, as was done in (7.17), yields:
☞267
Loss( f(y | x), g(y | θ, x)) = −ln g(y | θ, x)
= −ln gy+1(x | θ)
= −zy+1 + ln PpL
k=1 exp(zk).
Hence, we obtain the vector δL with components (k = 1, . . . , pL)
δL,k =
∂
∂zk

−zy+1 + ln PpL
k=1 exp(zk)

= gk(x | θ) −1{y = k −1}.
Note that we can remove a node from the final layer of the multi-logit network, be-
cause g1(x | θ) (which corresponds to the y = 0 class) can be eliminated, using the fact
that g1(x | θ) = 1 −PpL
k=2 gk(x | θ). For a numerical comparison, see Exercise 13.
As another example, in nonlinear multi-output regression (see Example 9.1), the out-
put function SL is typically of the form (9.1), so that ∂SL/∂z = diag(S ′
L(z1), . . . , S ′
L(zpL)).
Combining the output g(x | θ) = SL(zL) with the squared-error loss yields:
Loss(y, g(x | θ)) = ∥y −g(x | θ)∥2 =
pL
X
j=1
(y j −gj(x | θ))2.
Hence, line 5 in Algorithm 9.3.1 simplifies to:
δL = ∂SL
∂z
∂C
∂g = S′
L(zL) ⊙2(g(x | θ) −y).
Deep Learning
335
9.4
Methods for Training
Neural networks have been studied for a long time, yet it is only recently t

hat there have
been sufficient computational resources to train them effectively. The training of neural
networks requires minimization of a training loss, ℓτ(g(· | θ)) = 1
n
Pn
i=1 Ci(θ), which is typ-
ically a difficult high-dimensional optimization problem with multiple local minima. We
next consider a number of simple training methods.
In this section, the vectors δt and 1t use the notation of Section B.3.2 and should not
be confused with the derivative δ and the prediction function g, respectively.
9.4.1
Steepest Descent
If we can compute the gradient of ℓτ(g(· | θ)) via back-propagation, then we can apply the
steepest descent algorithm, which reads as follows. Starting from a guess θ1, we iterate the
☞412
following step until convergence:
θt+1 = θt −αt ut,
t = 1, 2, . . . ,
(9.6)
where ut := ∂ℓτ
∂θ (θt) and αt is the learning rate
learning rate
.
Observe that, rather than operating directly on the weights and biases, we operate in-
stead on θ := {Wl, bl}L
l=1 — a column vector of length PL
l=1(pl−1pl + pl) that stores all the
weight and bias parameters. The advantage of organizing the computations in this way is
that we can easily compute the learning rate αt; for example, via the Barzilai–Borwein
formula in (B.26).
☞413
Algorithm 9.4.1: Training via Steepest Descent
input: Training set τ = {(xi, yi)}n
i=1, initial weight matrices and bias vectors
{Wl, bl}L
l=1 =: θ1, activation functions {Sl}L
l=1.
output: The parameters of the trained learner.
1 t ←1, δ ←0.1 × 1, ut−1 ←0, α ←0.1
// initialization
2 while stopping condition is not met do
3
compute the gradient ut = ∂ℓτ
∂θ (θt) using Algorithm 9.3.2
4
1 ←ut −ut−1
5
if δ⊤1 > 0 then
// check if Hessian is positive-definite
6
α ←δ⊤1∥1∥2
// Barzilai-Borwein
7
else
8
α ←2 × α
// failing positivity, do something heuristic
9
δ ←−α ut
10
θt+1 ←θt + δ
11
t ←t + 1
12 return θt as the minimizer of the training loss
Typically, we initialize the algorithm with small random values for θ1, while being
careful to avoid satur

ating the activation function. For example, in the case of the ReLU
336
Methods for Training
activation function, we will use small positive values to ensure that its derivative is not
zero. A zero derivative of the activation function prevents the propagation of information
useful for computing a good search direction.
Recall that computation of the gradient of the training loss via Algorithm 9.3.2 requires
averaging over all training examples. When the size n of the training set τn is too large,
computation of the gradient ∂ℓτn/∂θ via Algorithm 9.3.2 may be too costly. In such cases,
we may employ the stochastic gradient descent
stochastic
gradient
descent
algorithm. In this algorithm, we view the
training loss as an expectation that can be approximated via Monte Carlo sampling. In
particular, if K is a random variable with distribution P[K = k] = 1/n for k = 1, . . . , n, then
we can write
ℓτ(g(· | θ)) = 1
n
n
X
k=1
Loss(yk, g(xk | θ)) = E Loss(yK, g(xK | θ)).
We can thus approximate ℓτ(g(· | θ)) via a Monte Carlo estimator using N iid copies of K:
bℓτ(g(· | θ)) := 1
N
N
X
i=1
Loss(yKi, g(xKi | θ)).
The iid Monte Carlo sample K1, . . . , KN is called a minibatch
minibatch
(see also Exercise 3). Typic-
ally, n ≫N so that the probability of observing ties in a minibatch of size N is negligible.
Finally, note that if the learning rate of the stochastic gradient descent algorithm sat-
isfies the conditions in (3.30), then the stochastic gradient descent algorithm is simply a
☞107
version of the stochastic approximation Algorithm 3.4.5.
9.4.2
Levenberg–Marquardt Method
Since a neural network with squared-error loss is a special type of nonlinear regression
model, it is possible to train it using classical nonlinear least-squares minimization meth-
ods, such as the Levenberg–Marquardt algorithm.
☞415
For simplicity of notation, suppose that the output of the net for an input x is a scalar
g(x). For a given input parameter θ of dimension d = dim(θ), the Levenberg–Marqua

rdt
Algorithm B.3.3 requires computation of the following vector of outputs:
g(τ | θ) := [g(x1 | θ), . . . , g(xn | θ)]⊤,
as well as the n × d matrix of Jacobi, G, of g at θ. To compute these quantities, we can
again use the back-propagation Algorithm 9.3.1, as follows.
Algorithm 9.4.2: Output for Training via Levenberg–Marquardt
input: Training set τ = {(xi, yi)}n
i=1, parameter θ.
output: Vector g(τ | θ) and matrix of Jacobi G for use in Algorithm B.3.3.
1 for i = 1, . . . , n do
// loop over all training examples
2
Run Algorithm 9.3.1 with input (xi, yi) (using ∂C
∂g = 1 in line 5) to compute
g(xi | θ) and ∂g(xi | θ)
∂θ
.
3 g(τ | θ) ←[g(x1 | θ), . . . , g(xn | θ)]⊤
4 G ←
h∂g(x1 | θ)
∂θ
, · · · , ∂g(xn | θ)
∂θ
i⊤
5 return g(τ | θ) and G
Deep Learning
337
The Levenberg–Marquardt algorithm is not suitable for networks with a large number
of parameters, because the cost of the matrix computations becomes prohibitive. For in-
stance, obtaining the Levenberg–Marquardt search direction in (B.28) usually incurs an
O(d3) cost. In addition, the Levenberg–Marquardt algorithm is applicable only when we
wish to train the network using the squared-error loss. Both of these shortcomings are
mitigated to an extent with the quasi-Newton or adaptive gradient methods described next.
9.4.3
Limited-Memory BFGS Method
All the methods discussed so far have been first-order optimization methods, that is, meth-
ods that only use the gradient vector ut := ∂ℓτ
∂θ (θt) at the current (and/or immediate past) can-
didate solution θt. In trying to design a more efficient second-order optimization method,
we may be tempted to use Newton’s method with a search direction:
☞410
−H−1
t ut,
where Ht is the d × d matrix of second-order partial derivatives of ℓτ(g(· | θ)) at θt.
There are two problems with this approach. First, while the computation of ut via Al-
gorithm 9.3.2 typically costs O(d), the computation of Ht costs O(d2). Second, even if we
have somehow computed Ht very fast, computing the 

search direction H−1
t ut still incurs an
O(d3) cost. Both of these considerations make Newton’s method impractical for large d.
Instead, a practical alternative is to use a quasi-Newton method, in which we directly
quasi-newton
method
aim to approximate H−1
t
via a matrix Ct that satisfies the secant condition:
☞411
Ct 1t = δt,
where δt := θt −θt−1 and 1t := ut −ut−1.
An ingenious formula that generates a suitable sequence of approximating matrices
{Ct} (each satisfying the secant condition) is the BFGS updating formula (B.23), which
can be written as the recursion (see Exercise 9):
Ct =

I −υt 1tδ⊤
t
⊤Ct−1

I −υt 1tδ⊤
t

+ υt δtδ⊤
t ,
υt := (1⊤
t δt)−1.
(9.7)
This formula allows us to update Ct−1 to Ct and then compute Ct ut in O(d2) time. While
this quasi-Newton approach is better than the O(d3) cost of Newton’s method, it may be
still too costly in large-scale applications.
Instead, an approximate or limited memory BFGS
limited memory
bfgs
updating can be achieved in O(d)
time. The idea is to store a few of the most recent pairs {δt, 1t} in order to evaluate its action
on a vector ut without explicitly constructing and storing Ct in computer memory. This is
possible, because updating C0 to C1 in (9.7) requires only the pair δ1, 11, and similarly
computing Ct from C0 only requires the history of the updates δ1, 11 . . . , δt, 1t, which can
be shown as follows.
Define the matrices At, . . . , A0 via the backward recursion ( j = 1, . . . , t):
At := I,
Aj−1 :=

I −υ j 1jδ⊤
j

Aj,
and observe that all matrix vector products: Aj u =: qj, for j = 0, . . . , t can be computed
efficiently via the backward recursion starting with qt = u:
τj := δ⊤
j q j,
qj−1 = q j −υ jτj 1j,
j = t, t −1, . . . , 1.
(9.8)
338
Methods for Training
In addition to {q j}, we will make use of the vectors {rj} defined via the recursion:
r0 := C0 q0,
rj = rj−1 + υj

τj −1⊤
j rj−1

δ j,
j = 1, . . . , t.
(9.9)
At the final iteration t, the BFGS updating formula (9.7) can be rewritten in t

he form:
Ct = A⊤
t−1Ct−1At−1 + υt δtδ⊤
t .
By iterating the recursion (9.7) backwards to C0, we can write:
Ct = A⊤
0 C0A0 +
tX
j=1
υj A⊤
j δ jδ⊤
j Aj,
that is, we can express Ct in terms of the initial C0 and the entire history of all BFGS values
{δ j, 1j}, as claimed. Further, with the {qj, rj} computed via (9.8) and (9.9), we can write:
Ct u = A⊤
0 C0 q0 +
tX
j=1
υ j

δ⊤
j qj

A⊤
j δ j
= A⊤
0 r0 + υ1τ1A⊤
1 δ1 +
tX
j=2
υjτ jA⊤
j δ j
= A⊤
1
 I −υ1δ11⊤
1
 r0 + υ1τ1δ1
 +
tX
j=2
υjτjA⊤
j δ j.
Hence, from the definition of the {rj} in (9.9), we obtain
Ctu = A⊤
1 r1 +
tX
j=2
υjτjA⊤
j δ j
= A⊤
2 r2 +
tX
j=3
υjτjA⊤
j δ j
= · · · = A⊤
t rt + 0 = rt.
Given C0 and the history of all recent BFGS values {δ j, 1j}h
j=1, the computation of the quasi-
Newton search direction d = −Ch u can be accomplished via the recursions (9.8) and (9.9)
as summarized in Algorithm 9.4.3.
Note that if C0 is a diagonal matrix, say the identity matrix, then C0 q is cheap to
compute and the cost of running Algorithm 9.4.3 is O(h d). Thus, for a fixed length of the
BFGS history, the cost of the limited-memory BFGS updating grows linearly in d, making
it a viable optimization algorithm in large-scale applications.
Deep Learning
339
Algorithm 9.4.3: Limited-Memory BFGS Update
input: BFGS history list {δ j, 1j}h
j=1, initial C0, and input u.
output: d = −Ch u, where Ct =  I −υt δt1⊤
t
 Ct−1

I −υt 1tδ⊤
t

+ υt δtδ⊤
t .
1 q ←u
2 for i = h, h −1, . . . , 1 do
// backward recursion to compute A0 u
3
υi ←

δ⊤
i 1i
−1
4
τi ←δ⊤
i q
5
q ←q −υiτi 1i
6 q ←C0 q
// compute C0(A0 u)
7 for i = 1, . . . , h do
// compute recursion (9.9)
8
q ←q + υi(τi −1⊤
i q) δi
9 return d ←−q, the value of −Ch u
In summary, a quasi-Newton algorithm with limited-memory BFGS updating reads as
follows.
Algorithm 9.4.4: Quasi-Newton Minimization with Limited-Memory BFGS
input: Training set τ = {(xi, yi)}n
i=1, initial weight matrices and bias vectors
{Wl, bl}L
l=1 =: θ1, activation functions {Sl}L
l=1, and history parameter h.


output: The parameters of the trained learner.
1 t ←1, δ ←0.1 × 1, ut−1 ←0
// initialization
2 while stopping condition is not met do
3
Compute ℓvalue = ℓτ(g(· | θt)) and ut = ∂ℓτ
∂θ (θt) via Algorithm 9.3.2.
4
1 ←ut −ut−1
5
Add (δ, 1) to the BFGS history as the newest BFGS pair.
6
if the number of pairs in the BFGS history is greater than h then
7
remove the oldest pair from the BFGS history
8
Compute d via Algorithm 9.4.3 using the BFGS history, C0 = I, and ut.
9
α ←1
10
while ℓτ(g(· | θt + α d)) ⩾ℓvalue + 10−4α d⊤ut do
11
α ←α/1.5
// line-search along quasi-Newton direction
12
δ ←α d
13
θt+1 ←θt + δ
14
t ←t + 1
15 return θt as the minimizer of the training loss
9.4.4
Adaptive Gradient Methods
Recall that the limited-memory BFGS method in the previous section determines a search
direction using the recent history of previously computed gradients {ut} and input paramet-
ers {θt}. This is because the BFGS pairs {δt, 1t} can be easily constructed from the identities:
δt = θt −θt−1 and 1t = ut −ut−1. In other words, using only past gradient computations and
with little extra computation, it is possible to infer some of the second-order information
340
Methods for Training
contained in the Hessian matrix of ℓτ(θ). In addition to the BFGS method, there are other
ways in which we can exploit the history of past gradient computations.
One approach is to use the normal approximation method, in which the Hessian of ℓτ
☞414
at θt is approximated via
bHt = γ I + 1
h
tX
i=t−h+1
uiu⊤
i ,
(9.10)
where ut−h+1, . . . , ut are the h most recently computed gradients and γ is a tuning parameter
(for example, γ = 1/h). The search direction is then given by
−bH−1
t ut,
which can be computed quickly in O(h2 d) time either using the QR decomposition (Exer-
cises 5 and 6), or the Sherman–Morrison Algorithm A.6.1. This approach requires that we
☞373
store the last h gradient vectors in memory.
Another approach that completely bypasses the need to invert a Hessian approximation
is the Adap

tive Gradient or AdaGrad
AdaGrad
method, in which we only store the diagonal of bHt
and use the search direction:
−diag(bHt)−1/2ut.
We can avoid storing any of the gradient history by instead using the slightly different
search direction2
−ut
.p
vt + γ × 1,
where the vector vt is updated recursively via
vt =
 
1 −1
h
!
vt−1 + 1
h ut ⊙ut.
With this updating of vt, the difference between the vector vt + γ × 1 and the diagonal of
the Hessian bHt will be negligible.
A more sophisticated version of AdaGrad is the adaptive moment estimation or Adam
Adam
method, in which we not only average the vectors {vt}, but also average the gradient vectors
{ut}, as follows.
Algorithm 9.4.5: Updating of Search Direction at Iteration t via Adam
input: ut, but−1, vt−1, θt, and parameters (α, hv, hu), equal to, e.g., (10−3, 103, 10).
output: but, vt, θt+1.
1 but ←

1 −1
hu

but−1 + 1
hu ut
2 vt ←

1 −1
hv

vt−1 + 1
hv ut ⊙ut
3 u∗
t ←but
. 
1 −(1 −h−1
u )t
4 v∗
t ←vt
. 
1 −(1 −h−1
v )t
5 θt+1 ←θt −α u∗
t
. p
v∗
t + 10−8 × 1

6 return but, vt, θt+1
2Here we divide two vectors componentwise.
Deep Learning
341
Yet another computationally cheap approach is the momentum method, in which the
momentum
method
steepest descent iteration (9.6) is modified to
θt+1 = θt −αt ut + γ δt,
where δt = θt −θt−1 and γ is a tuning parameter. This strategy frequently performs better
than the “vanilla” steepest descent method, because the search direction is less likely to
change abruptly.
Numerical experience suggests that the vanilla steepest-descent Algorithm 9.4.1 and
the Levenberg–Marquardt Algorithm B.3.3 are effective for networks with shallow archi-
tectures, but not for networks with deep architectures. In comparison, the stochastic gradi-
ent descent method, the limited-memory BFGS Algorithm 9.4.4, or any of the adaptive
gradient methods in this section, can frequently handle networks with many hidden lay-
ers (provided that any tuning parameters and initialization values are carefully chose

n via
experimentation).
9.5
Examples in Python
In this section we provide two numerical examples in Python. In the first example, we
train a neural network with the stochastic gradient descent method using the polynomial
regression data from Example 2.1, and without using any specialized Python packages.
☞
26
In the second example, we consider a realistic application of a neural network to image
recognition and classification. Here we use the specialized open-source Python package
Pytorch.
9.5.1
Simple Polynomial Regression
Consider again the polynomial regression data set depicted in Figure 2.4. We use a network
with architecture
[p0, p1, p2, p3] = [1, 20, 20, 1].
In other words, we have two hidden layers with 20 neurons, resulting in a learner with a
total of dim(θ) = 481 parameters. To implement such a neural network, we first import the
numpy and the matplotlib packages, then read the regression problem data and define
the feed-forward neural network layers.
NeuralNetPurePython.py
import numpy as np
import matplotlib.pyplot as plt
#%%
# import data
data = np.genfromtxt('polyreg.csv',delimiter=',')
X = data[:,0].reshape(-1,1)
y = data[:,1].reshape(-1,1)
# Network setup
p = [X.shape[1],20,20,1] # size of layers
342
Examples in Python
L = len(p)-1
# number of layers
Next, the initialize method generates random initial weight matrices and bias vec-
tors {Wl, bl}L
l=1. Specifically, all parameters are initialized with values distributed according
to the standard normal distribution.
def initialize(p, w_sig = 1):
W, b = [[]]*len(p), [[]]*len(p)
for l in range(1,len(p)):
W[l]= w_sig * np.random.randn(p[l], p[l-1])
b[l]= w_sig * np.random.randn(p[l], 1)
return W,b
W,b = initialize(p) # initialize weight matrices and bias vectors
The following code implements the ReLU activation function from Figure 9.2 and the
squared error loss. Note that these functions return both the function values and the corres-
ponding gradients.
def RELU(z,l):
# RELU activation function: value 

and derivative
if l == L: return z, np.ones_like(z)
else:
val = np.maximum(0,z) # RELU function element -wise
J = np.array(z>0, dtype = float) # derivative of RELU
element -wise
return val, J
def loss_fn(y,g):
return (g - y)**2, 2 * (g - y)
S = RELU
Next, we implement the feed-forward and backward-propagation Algorithm 9.3.1.
Here, we have implemented Algorithm 9.3.2 inside the backward-propagation loop.
def feedforward(x,W,b):
a, z, gr_S = [0]*(L+1), [0]*(L+1), [0]*(L+1)
a[0] = x.reshape(-1,1)
for l in range(1,L+1):
z[l] = W[l] @ a[l-1] + b[l] # affine transformation
a[l], gr_S[l] = S(z[l],l) # activation function
return a, z, gr_S
def backward(W,b,X,y):
n =len(y)
delta = [0]*(L+1)
dC_db, dC_dW = [0]*(L+1), [0]*(L+1)
loss=0
Deep Learning
343
for i in range(n): # loop over training examples
a, z, gr_S = feedforward(X[i,:].T, W, b)
cost, gr_C = loss_fn(y[i], a[L]) # cost i and gradient wrt g
loss += cost/n
delta[L] = gr_S[L] @ gr_C
for l in range(L,0,-1): # l = L,...,1
dCi_dbl = delta[l]
dCi_dWl = delta[l] @
a[l-1].T
# ---- sum up over samples ----
dC_db[l] = dC_db[l] + dCi_dbl/n
dC_dW[l] = dC_dW[l] + dCi_dWl/n
# -----------------------------
delta[l-1] =
gr_S[l-1] * W[l].T @ delta[l]
return dC_dW, dC_db, loss
As explained in Section 9.4, it is sometimes more convenient to collect all the weight
matrices and bias vectors {Wl, bl}L
l=1 into a single vector θ. Consequently, we code two
functions that map the weight matrices and the bias vectors into a single parameter vector,
and vice versa.
def list2vec(W,b):
# converts list of weight matrices and bias vectors into
# one column vector
b_stack = np.vstack([b[i] for i in range(1,len(b))] )
W_stack = np.vstack(W[i].flatten().reshape(-1,1) for i in range
(1,len(W)))
vec = np.vstack([b_stack , W_stack])
return vec
#%%
def vec2list(vec, p):
# converts vector to weight matrices and bias vectors
W, b = [[]]*len(p),[[]]*len(p)
p_count = 0
for l in range(1,len(p)): # construct bias vectors
b[l] = vec[p_count:(p_count+p[l])].res

hape(-1,1)
p_count = p_count + p[l]
for l in range(1,len(p)): # construct weight matrices
W[l] = vec[p_count:(p_count + p[l]*p[l-1])].reshape(p[l], p[
l-1])
p_count = p_count + (p[l]*p[l-1])
return W, b
Finally, we run the stochastic gradient descent for 104 iterations using a minibatch of
size 20 and a constant learning rate of αt = 0.005.
344
Examples in Python
batch_size = 20
lr = 0.005
beta = list2vec(W,b)
loss_arr = []
n = len(X)
num_epochs = 10000
print("epoch | batch loss")
print("----------------------------")
for epoch in range(1,num_epochs+1):
batch_idx = np.random.choice(n,batch_size)
batch_X = X[batch_idx].reshape(-1,1)
batch_y=y[batch_idx].reshape(-1,1)
dC_dW, dC_db, loss = backward(W,b,batch_X ,batch_y)
d_beta = list2vec(dC_dW,dC_db)
loss_arr.append(loss.flatten()[0])
if(epoch==1 or np.mod(epoch ,1000)==0):
print(epoch,": ",loss.flatten()[0])
beta = beta - lr*d_beta
W,b = vec2list(beta,p)
# calculate the loss of the entire training set
dC_dW, dC_db, loss = backward(W,b,X,y)
print("entire training set loss = ",loss.flatten()[0])
xx = np.arange(0,1,0.01)
y_preds = np.zeros_like(xx)
for i in range(len(xx)):
a, _, _ = feedforward(xx[i],W,b)
y_preds[i],
= a[L]
plt.plot(X,y, 'r.', markersize = 4,label = 'y')
plt.plot(np.array(xx), y_preds , 'b',label = 'fit')
plt.legend()
plt.xlabel('x')
plt.ylabel('y')
plt.show()
plt.plot(np.array(loss_arr), 'b')
plt.xlabel('iteration')
plt.ylabel('Training Loss')
plt.show()
epoch | batch loss
----------------------------
1 :
158.6779278688539
1000 :
54.52430507401445
2000 :
38.346572088604965
3000 :
31.02036319180713
4000 :
22.91114276931535
5000 :
27.75810262906341
6000 :
22.296907007032928
7000 :
17.337367420038046
8000 :
19.233689945334195
9000 :
39.54261478969857
10000 :
14.754724387604416
entire training set loss =
28.904957963612727
Deep Learning
345
The left panel of Figure 9.5 shows a trained neural network with a training loss of
approximately 28.9. As seen from the right panel of Figure 9.5, the algorithm initial

ly
makes rapid progress until it settles down into a stationary regime after 400 iterations.
0.00
0.25
0.50
0.75
1.00
input u
0
20
40
output y
ﬁt
y
0
500
1000
1500
2000
iteration
0
100
200
300
Batch Loss
Figure 9.5: Left panel: The fitted neural network with training loss of ℓτ(gτ) ≈28.9. Right
panel: The evolution of the estimated loss, bℓτ(gτ(· | θ)), over the steepest-descent iterations.
9.5.2
Image Classification
In this section, we will use the package Pytorch, which is an open-source machine learn-
ing library for Python. Pytorch can easily exploit any graphics processing unit (GPU)
for accelerated computation. As an example, we consider the Fashion-MNIST data set
from https://www.kaggle.com/zalando-research/fashionmnist. The Fashion-
MNIST data set contains 28 × 28 gray-scale images of clothing. Our task is to classify
each image according to its label. Specifically, the labels are: T-Shirt, Trouser, Pullover,
Dress, Coat, Sandal, Shirt, Sneaker, Bag, and Ankle Boot. Figure 9.6 depicts a typical
ankle boot in the left panel and a typical dress in the right panel. To start with, we import
the required libraries and load the Fashion-MNIST data set.
Figure 9.6: Left: an ankle boot. Right: a dress.
346
Examples in Python
ImageClassificationPytorch.py
import torch
import torch.nn as nn
from torch.autograd import Variable
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset , DataLoader
from PIL import Image
import torch.nn.functional as F
#################################################################
# data loader class
#################################################################
class LoadData(Dataset):
def __init__(self, fName, transform=None):
data = pd.read_csv(fName)
self.X = np.array(data.iloc[:, 1:], dtype=np.uint8).reshape
(-1, 1, 28, 28)
self.y = np.array(data.iloc[:, 0])
def __len__(self):
return len(self.X)
def __getitem__(self, idx):
img = self.X[idx]
lbl = self.y[idx]
return (img, lbl)
# l

oad the image data
train_ds = LoadData('fashionmnist/fashion -mnist_train.csv')
test_ds
= LoadData('fashionmnist/fashion -mnist_test.csv')
# set labels dictionary
labels = {0 : 'T-Shirt', 1 : 'Trouser', 2 : 'Pullover',
3 : 'Dress', 4 : 'Coat', 5 : 'Sandal', 6 : 'Shirt',
7 : 'Sneaker', 8 : 'Bag', 9 : 'Ankle Boot'}
Since an image input data is generally memory intensive, it is important to partition
the data set into (mini-)batches. The code below defines a batch size of 100 images and
initializes the Pytorch data loader objects. These objects will be used for efficient iteration
over the data set.
# load the data in batches
batch_size = 100
train_loader = torch.utils.data.DataLoader(dataset=train_ds ,
batch_size=batch_size ,
shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_ds ,
batch_size=batch_size ,
shuffle=True)
Next, to define the network architecture in Pytorch all we need to do is define an
Deep Learning
347
instance of the torch.nn.Module class. Choosing a network architecture with good gen-
eralization properties can be a difficult task. Here, we use a network with two convolution
layers (defined in the cnn_layer block), a 3×3 kernel, and three hidden layers (defined in
the flat_layer block). Since there are ten possible output labels, the output layer has ten
nodes. More specifically, the first and the second convolution layers have 16 and 32 output
channels. Combining this with the definition of the 3 × 3 kernel, we conclude that the size
of the first flat hidden layer should be:

second convolution layer
z                      }|                      {
(28 −3 + 1)
|        {z        }
first convolution layer
−3 + 1

2
× 32 = 18432,
where the multiplication by 32 follows from the fact that the second convolution layer has
32 output channels. Having said that, the flat_fts variable determines the number of
output layers of the convolution block. This number is used to define the size of the first
hidden layer of the fl

at_layer block. The rest of the hidden layers have 100 neurons and
we use the ReLU activation function for all layers. Finally, note that the forward method
in the CNN class implements the forward pass.
# define the network
class CNN(nn.Module):
def __init__(self):
super(CNN, self).__init__()
self.cnn_layer = nn.Sequential(
nn.Conv2d(1, 16, kernel_size=3, stride=(1,1)),
nn.ReLU(),
nn.Conv2d(16, 32, kernel_size=3, stride=(1,1)),
nn.ReLU(),
)
self.flat_fts = (((28-3+1) -3+1)**2)*32
self.flat_layer = nn.Sequential(
nn.Linear(self.flat_fts , 100),
nn.ReLU(),
nn.Linear(100, 100),
nn.ReLU(),
nn.Linear(100, 100),
nn.ReLU(),
nn.Linear(100, 10))
def forward(self, x):
out = self.cnn_layer(x)
out = out.view(-1, self.flat_fts)
out = self.flat_layer(out)
return out
Next, we specify how the network will be trained. We choose the device type, namely,
the central processing unit (CPU) or the GPU (if available), the number of training itera-
tions (epochs), and the learning rate. Then, we create an instance of the proposed convolu-
348
Examples in Python
tion network and send it to the predefined device (CPU or GPU). Note how easily one can
switch between the CPU or the GPU without major changes to the code.
In addition to the specifications above, we need to choose an appropriate loss function
and training algorithm. Here, we use the cross-entropy loss and the Adam adaptive gradi-
☞267
ent Algorithm 9.4.5. Once these parameters are set, the learning proceeds to evaluate the
gradient of the loss function via the back-propagation algorithm.
# learning parameters
num_epochs = 50
learning_rate = 0.001
#device = torch.device ('cpu') # use this to run on CPU
device = torch.device ('cuda') # use this to run on GPU
#instance of the Conv Net
cnn = CNN()
cnn.to(device=device)
#loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)
# the learning loop
losses = []
for epoch in range(1,num_epochs+1):
for i, (images, labels)

 in enumerate(train_loader):
images = Variable(images.float()).to(device=device)
labels = Variable(labels).to(device=device)
optimizer.zero_grad()
outputs = cnn(images)
loss = criterion(outputs , labels)
loss.backward()
optimizer.step()
losses.append(loss.item())
if(epoch==1 or epoch % 10 == 0):
print ("Epoch : ", epoch, ", Training Loss: ",
loss.item())
# evaluate on the test set
cnn.eval()
correct = 0
total = 0
for images, labels in test_loader:
images = Variable(images.float()).to(device=device)
outputs = cnn(images)
_, predicted = torch.max(outputs.data, 1)
total += labels.size(0)
correct += (predicted.cpu() == labels).sum()
print("Test Accuracy of the model on the 10,000 training test images
: ", (100 * correct.item() / total),"%")
# plot
Deep Learning
349
plt.rc('text', usetex=True)
plt.rc('font', family='serif',size=20)
plt.tight_layout()
plt.plot(np.array(losses)[10:len(losses)])
plt.xlabel(r'{iteration}',fontsize=20)
plt.ylabel(r'{Batch Loss}',fontsize=20)
plt.subplots_adjust(top=0.8)
plt.show()
Epoch :
1 , Training Loss:
0.412550151348114
Epoch :
10 , Training Loss:
0.05452106520533562
Epoch :
20 , Training Loss:
0.07233225554227829
Epoch :
30 , Training Loss:
0.01696968264877796
Epoch :
40 , Training Loss:
0.0008199119474738836
Epoch :
50 , Training Loss:
0.006860652007162571
Test Accuracy of the model on the 10,000 training test images: 91.02 %
Finally, we evaluate the network performance using the test data set. A typical mini-
batch loss as a function of iteration is shown in Figure 9.7 and the proposed neural network
achieves about 91% accuracy on the test set.
0
200
400
600
800
1000
iteration
0.2
0.4
0.6
0.8
1.0
Batch Loss
Figure 9.7: The batch loss history.
Further Reading
A popular book written by some of the pioneers of deep learning is [53]. For an excellent
and gentle introduction to the intuition behind neural networks, we recommend [94]. A
summary of many effective gradient descent methods for training of deep networks is given
in [105]. An ea

rly resource on the limited-memory BFGS method is [81], and a more recent
resource includes [13], which makes recommendations on the best choice for the length of
the BFGS history (that is, the value of the parameter h).
350
Exercises
Exercises
1. Show that the softmax function
softmax : z 7→
exp(z)
P
k exp(zk)
satisfies the invariance property:
softmax(z) = softmax(z + c × 1),
for any constant c.
2. Projection pursuit
projection
pursuit
is a network with one hidden layer that can be written as:
g(x) = S (ω⊤x),
where S is a univariate smoothing cubic spline. If we use squared-error loss with τn =
☞235
{yi, xi}n
i=1, we need to minimize the training loss:
1
n
n
X
i=1
 yi −S (ω⊤xi)2
with respect to ω and all cubic smoothing splines. This training of the network is typically
tackled iteratively in a manner similar to the EM algorithm. In particular, we iterate (t =
☞139
1, 2, . . .) the following steps until convergence.
(a) Given the missing data ωt, compute the spline S t by training a cubic smoothing spline
on {yi, ω⊤
t xi}. The smoothing coefficient of the spline may be determined as part of
this step.
(b) Given the spline function S t, compute the next projection vector ωt+1 via iterative
reweighted least squares:
iterative
reweighted
least squares
ωt+1 = argmin
β
(et −Xβ)⊤Σt (et −Xβ),
(9.11)
where
et,i := ω⊤
t xi + yi −S t(ω⊤
t xi)
S ′
t(ω⊤
t xi)
,
i = 1, . . . , n
is the adjusted response, and Σ1/2
t
= diag(S ′
t(ω⊤
t x1), . . . , S ′
t(ω⊤
t xn)) is a diagonal mat-
rix.
Apply Taylor’s Theorem B.1 to the function S t and derive the iterative reweighted
☞400
least-squares optimization program (9.11).
3. Suppose that in the stochastic gradient descent method we wish to repeatedly draw
☞336
minibatches of size N from τn, where we assume that N × m = n for some large integer m.
Instead of repeatedly resampling from τn, an alternative is to reshuffle τn via a random per-
mutation Π and then advance sequentially through the reshuffled training set to construct
☞115
m 

non-overlapping minibatches. A single traversal of such a reshuffled training set is called
an epoch
epoch
. The following pseudo-code describes the procedure.
Deep Learning
351
Algorithm 9.5.1: Stochastic Gradient Descent with Reshuffling
input: Training set τn = {(xi, yi)}n
i=1, initial weight matrices and bias vectors
{Wl, bl}L
l=1 →θ1, activation functions {Sl}L
l=1, learning rates {α1, α2, . . .}.
output: The parameters of the trained learner.
1 t ←1 and epoch ←0
2 while stopping condition is not met do
3
Draw U1, . . . , Un
iid∼U(0, 1).
4
Let Π be the permutation of {1, . . . , n} that satisfies UΠ1 < · · · < UΠn.
5
(xi, yi) ←(xΠi, yΠi) for i = 1, . . . , n
// reshuffle τn
6
for j = 1, . . . , m do
7
bℓτ ←1
N
PjN
i=( j−1)N+1 Loss(yi, g(xi | θ))
8
θt+1 ←θt −αt
∂bℓτ
∂θ (θt)
9
t ←t + 1
10
epoch ←epoch + 1
// number of reshuffles or epochs
11 return θt as the minimizer of the training loss
Write Python code that implements the stochastic gradient descent with data reshuffling,
and use it to train the neural net in Section 9.5.1.
☞341
4. Denote the pdf of the N(0, Σ) distribution by φΣ(·), and let
D(µ0, Σ0 | µ1, Σ1) =
Z
Rd φΣ0(x −µ0) ln φΣ0(x −µ0)
φΣ1(x −µ1) dx
be the Kullback–Leibler divergence between the densities of the N(µ0, Σ0) and N(µ1, Σ1)
☞
42
distributions on Rd. Show that
2D(µ0, Σ0 | µ1, Σ1) = tr(Σ−1
1 Σ0) −ln |Σ−1
1 Σ0| + (µ1 −µ0)⊤Σ−1
1 (µ1 −µ0) −d.
Hence, deduce the formula in (B.22).
5. Suppose that we wish to compute the inverse and log-determinant of the matrix
In + UU⊤,
where U is an n × h matrix with h ≪n. Show that
(In + UU⊤)−1 = In −QnQ⊤
n ,
where Qn contains the first n rows of the (n + h) × h matrix Q in the QR factorization of
☞375
the (n + h) × h matrix:
"U
Ih
#
= QR.
In addition, show that ln |In + UU⊤| = Ph
i=1 ln r2
ii, where {rii} are the diagonal elements of
the h × h matrix R.
352
Exercises
6. Suppose that
U = [u0, u1, . . . , uh−1],
where all u ∈Rn are column vectors and we have computed (In + UU⊤)−1 via the QR
factorization method in

 Exercise 5. If the columns of matrix U are updated to
[u1, . . . , uh−1, uh],
show that the inverse (In + UU⊤)−1 can be updated in O(h n) time (rather than computed
from scratch in O(h2 n) time). Deduce that the computing cost of updating the Hessian
approximation (9.10) is the same as that for the limited-memory BFGS Algorithm 9.4.3.
In your solution you may use the following facts from [29]. Suppose we are given the
Q and R factors in the QR factorization of a matrix A ∈Rn×h. If a row/column is added to
matrix A, then the Q and R factors need not be recomputed from scratch (in O(h2 n) time),
but can be updated efficiently in O(h n) time. Similarly, if a row/column is removed from
matrix A, then the Q and R factors can be updated in O(h2) time.
7. Suppose that U ∈Rn×h has its k-th column v replaced with w, giving the updated eU.
(a) If e ∈Rh denotes the unit-length vector such that ek = ∥e∥= 1 and
r± :=
√
2
2 U⊤(w −v) +
√
2 ∥w −v∥2
4
e ±
√
2
2 e,
show that
eU⊤eU = U⊤U + r+r⊤
+ −r−r⊤
−.
[Hint: You may find Exercise 16 in Chapter 6 useful.]
☞247
(b) Let B := (Ih + U⊤U)−1. Use the Woodbury identity (A.15) to show that
☞371
(In + eUeU⊤)−1 = In −eU

B−1 + r+r⊤
+ −r−r⊤
−
−1 eU⊤.
(c) Suppose that we have stored B in computer memory. Use Algorithm 6.8.1 and parts
(a) and (b) to write pseudo-code that updates (In+UU⊤)−1 to (In+eUeU⊤)−1 in O((n+h)h)
computing time.
8. Equation (9.7) gives the rank-two BFGS update of the inverse Hessian Ct−1 to Ct. In-
stead of using a two-rank update, we can consider a one-rank update, in which Ct−1 is
updated to Ct by the general rank-one formula:
Ct = Ct−1 + υt rtr⊤
t .
Find values for the scalar υt and vector rt, such that Ct satisfies the secant condition Ct1t =
δt.
9. Show that the BFGS formula (B.23) can be written as:
C ←

I −υ1δ⊤⊤C

I −υ1δ⊤
+ υδδ⊤,
where υ := (1⊤δ)−1.
Deep Learning
353
10. Show that the BFGS formula (B.23) is the solution to the constrained optimization
problem:
CBFGS =
argmin
A subject to A1 = δ, A = A⊤D(0, C

 | 0, A),
where D is the Kullback–Leibler discrepancy defined in (B.22). On the other hand, show
that the DFP formula (B.24) is the solution to the constrained optimization problem:
CDFP =
argmin
A subject to A1 = δ, A = A⊤D(0, A | 0, C).
11. Consider again the logistic regression model in Exercise 5.18, which used iterative
☞213
reweighted least squares for training the learner. Repeat all the computations, but this
time using the limited-memory BFGS Algorithm 9.4.4. Which training algorithm converges
faster to the optimal solution?
12. Download the seeds_dataset.txt data set from the book’s GitHub site, which con-
tains 210 independent examples. The categorical output (response) here is the type of wheat
grain: Kama, Rosa, and Canadian (encoded as 1, 2, and 3), so that c = 3. The seven con-
tinuous features (explanatory variables) are measurements of the geometrical properties of
the grain (area, perimeter, compactness, length, width, asymmetry coefficient, and length
of kernel groove). Thus, x ∈R7 (which does not include the constant feature 1) and the
multi-logit pre-classifier in Example 9.2 can be written as g(x) = softmax(Wx + b), where
☞329
W ∈R3×7 and b ∈R3. Implement and train this pre-classifier on the first n = 105 examples
of the seeds data set using, for example, Algorithm 9.4.1. Use the remaining n′ = 105
examples in the data set to estimate the generalization risk of the learner using the cross-
entropy loss. [Hint: Use the cross-entropy loss formulas from Example 9.4.]
13. In Exercise 12 above, we train the multi-logit classifier using a weight matrix W ∈R3×7
and bias vector b ∈R3. Repeat the training of the multi-logit model, but this time keeping z1
as an arbitrary constant (say z1 = 0), and thus setting c = 0 to be a “reference” class. This
has the effect of removing a node from the output layer of the network, giving a weight
matrix W ∈R2×7 and bias vector b ∈R2 of smaller dimensions than in (7.16).
☞267
14. Consider again Example 9.4, where we 

used a softmax output function SL in con-
☞334
junction with the cross-entropy loss: C(θ) = −ln gy+1(x | θ). Find formulas for ∂C
∂g and ∂SL
∂zL .
Hence, verify that:
∂SL
∂zL
∂C
∂g = g(x | θ) −ey+1,
where ei is the unit length vector with an entry of 1 in the i-th position.
15. Derive the formula (B.25) for a diagonal Hessian update in a quasi-Newton method
☞412
for minimization. In other words, given a current minimizer xt of f(x), a diagonal matrix
C of approximating the Hessian of f, and a gradient vector u = ∇f(xt), find the solution
to the constrained optimization program:
min
A D(xt, C | xt −Au, A)
subject to: A1 ⩾δ, A is diagonal,
where D is the Kullback–Leibler distance defined in (B.22) (see Exercise 4).
354
Exercises
16. Consider again the Python implementation of the polynomial regression in Sec-
tion 9.5.1, where the stochastic gradient descent was used for training.
Using the polynomial regression data set, implement and run the following four altern-
ative training methods:
(a) the steepest-descent Algorithm 9.4.1;
(b) the Levenberg–Marquardt Algorithm B.3.3, in conjunction with Algorithm 9.4.2 for
☞415
computing the matrix of Jacobi;
(c) the limited-memory BFGS Algorithm 9.4.4;
(d) the Adam Algorithm 9.4.5, which uses past gradient values to determine the next
search direction.
For each training algorithm, using trial and error, tune any algorithmic parameters so that
the network training is as fast as possible. Comment on the relative advantages and disad-
vantages of each training/optimization method. For example, comment on which optimiz-
ation method makes rapid initial progress, but gets trapped in a suboptimal solution, and
which method is slower, but more consistent in finding good optima.
17. Consider again the Pytorch code in Section 9.5.2. Repeat all the computations, but
this time using the momentum method for training of the network. Comment on which
method is preferable: the momentum or the Adam method?
APPENDIXA
LINEAR ALGEBRA AND FUNCTI

ONAL
ANALYSIS
The purpose of this appendix is to review some important topics in linear algebra
and functional analysis. We assume that the reader has some familiarity with matrix
and vector operations, including matrix multiplication and the computation of determ-
inants.
A.1
Vector Spaces, Bases, and Matrices
Linear algebra is the study of vector spaces and linear mappings. Vectors are, by defini-
tion, elements of some vector space
vector space
V and satisfy the usual rules of addition and scalar
multiplication, e.g.,
if x ∈V and y ∈V, then αx + βy ∈V for all α, β ∈R (or C).
We will be dealing mostly with vectors in the Euclidean vector space Rn for some n. That
is, we view the points of Rn as objects that can be added up and multiplied with a scalar,
e.g., (x1, x2) + (y1, y2) = (x1 + y1, x2 + y2) for points in R2. Sometimes it is convenient to
work with the complex vector space Cn instead of Rn; see also Section A.3.
Vectors v1, . . . , vk are called linearly independent
linearly
independent
if none of them can be expressed as
a linear combination of the others; that is, if α1v1 + · · · + αnvn = 0, then it must hold that
αi = 0 for all i = 1, . . . , n.
Definition A.1: Basis of a Vector Space
A set of vectors B = {v1, . . . , vn} is called a basis
basis
of the vector space V if every
vector x ∈V can be written as a unique linear combination of the vectors in B:
x = α1 v1 + · · · + αn vn.
The (possibly infinite) number n is called the dimension
dimension
of V.
355
356
Vector Spaces, Bases, and Matrices
Using a basis B of V, we can thus represent each vector x ∈V as a row or column of
numbers
[α1, . . . , αn]
or

α1
...
αn
.
(A.1)
Typically, vectors in Rn are represented via the standard basis
standard basis
, consisting of unit
vectors (points) e1 = (1, 0, . . . , 0), . . . , en = (0, 0, . . . , 0, 1). As a consequence, any point
(x1, . . . , xn) ∈Rn can be represented, using the standard basis, as a row or column vec-
tor of the form (A.1) 

above, with αi = xi, i = 1, . . . , n. We will also write [x1, x2, . . . , xn]⊤,
for the corresponding column vector, where ⊤denotes the transpose
transpose
.
To avoid confusion, we will use the convention from now on that a generic vector x
is always represented via the standard basis as a column vector. The corresponding
row vector is denoted by x⊤.
A matrix can be viewed as an array of m rows and n columns that defines a linear
matrix
transformation from Rn to Rm (or for complex matrices, from Cn to Cm). The matrix is said
linear
transformation
to be square if m = n. If a1, a2, . . . , an are the columns of A, that is, A = [a1, a2, . . . , an],
and if x = [x1, . . . , xn]⊤, then Ax = x1 a1 + · · · + xn an. In particular, the standard basis
vector ek is mapped to the vector ak, k = 1, . . . , n. We sometimes use the notation A = [aij],
to denote a matrix whose (i, j)-th element is ai j. When we wish to emphasize that a matrix
A is real-valued with m rows and n columns, we write A ∈Rm×n. The rank
rank
of a matrix is the
number of linearly independent rows or, equivalently, the number of linearly independent
columns.
Example A.1 (Linear Transformation) Take the matrix
A =
" 1
1
−0.5
−2
#
.
It transforms the two basis vectors [1, 0]⊤and [0, 1]⊤, shown in red and blue in the left panel
of Figure A.1, to the vectors [1, −0.5]⊤and [1, −2]⊤, shown on the right panel. Similarly,
the points on the unit circle are transformed to an ellipse.
-1
-0.5
0
0.5
1
x
-1
-0.5
0
0.5
1
y
-2
0
2
x
-3
-2
-1
0
1
2
3
y
Figure A.1: A linear transformation of the unit circle.
Linear Algebra and Functional Analysis
357
Suppose A = [a1, . . . , an], where the A = {ai} form a basis of Rn. Take any vector x =
[x1, . . . , xn]⊤
E with respect to the standard basis E (we write subscript E to stress this). Then
the representation of this vector with respect to A is simply
y = A−1x,
where A−1 is the inverse
inverse
of A; that is, the matrix such that AA−1 = A−1A = In, where
In is the n-dimensional i

dentity matrix. To see this, note that A−1ai gives the i-th unit
vector representation, for i = 1, . . . , n, and recall that each vector in Rn is a unique linear
combination of these basis vectors.
Example A.2 (Basis Representation) Consider the matrix
A =
"1
2
3
4
#
with inverse
A−1 =
" −2
1
3/2
−1/2
#
.
(A.2)
The vector x = [1, 1]⊤
E in the standard basis has representation y = A−1x = [−1, 1]⊤
A in the
basis consisting of the columns of A. Namely,
Ay = −
"1
3
#
+
"2
4
#
=
"1
1
#
.
The transpose
transpose
of a matrix A = [ai j] is the matrix A⊤= [aji]; that is, the (i, j)-th element
of A⊤is the ( j, i)-th element of A. The trace
trace
of a square matrix is the sum of its diagonal
elements. A useful result is the following cyclic property.
Theorem A.1: Cyclic Property
The trace is invariant under cyclic permutations: tr(ABC) = tr(BCA) = tr(CAB).
Proof: It suffices to show that tr(DE) is equal to tr(ED) for any m × n matrix D = [dij]
and n × m matrix E = [eij]. The diagonal elements of DE are Pn
j=1 di j e ji, i = 1, . . . , m and
the diagonal elements of ED are Pm
i=1 e ji di j, j = 1, . . . , n. They sum up to the same number
Pm
i=1
Pn
j=1 dij e ji.
□
A square matrix has an inverse if and only if its columns (or rows) are linearly in-
dependent. This is the same as the matrix being of full rank; that is, its rank is equal to
the number of columns. An equivalent statement is that its determinant is not zero. The
determinant
determinant
of an n × n matrix A = [ai,j] is defined as
det(A) :=
X
π
(−1)ζ(π)
n
Y
i=1
aπi,i,
(A.3)
where the sum is over all permutations π = (π1, . . . , πn) of (1, . . . , n), and ζ(π) is the num-
ber of pairs (i, j) for which i < j and πi > π j. For example, ζ(2, 3, 4, 1) = 3 for the pairs
(1, 4), (2, 4), (3, 4). The determinant of a diagonal matrix
diagonal matrix
— a matrix with only zero ele-
ments off the diagonal — is simply the product of its diagonal elements.
358
Vector Spaces, Bases, and Matrices
Geometrically, the determinant of a 

square matrix A = [a1, . . . , an] is the (signed)
volume of the parallelepiped (n-dimensional parallelogram) defined by the columns
a1, . . . , an; that is, the set of points x = Pn
i=1 αi ai, where 0 ⩽αi ⩽1, i = 1, . . . , n.
The easiest way to compute a determinant of a general matrix is to apply simple op-
erations to the matrix that potentially reduce its complexity (as in the number of non-zero
elements, for example), while retaining its determinant:
• Adding a multiple of one column (or row) to another, does not change the determin-
ant.
• Multiplying a column (or row) with a number multiplies the determinant by the same
number.
• Swapping two rows changes the sign of the determinant.
By applying these rules repeatedly one can reduce any matrix to a diagonal matrix.
It follows then that the determinant of the original matrix is equal to the product of the
diagonal elements of the resulting diagonal matrix multiplied by a known constant.
Example A.3 (Determinant and Volume) Figure A.2 illustrates how the determinant
of a matrix can be viewed as a signed volume, which can be computed by repeatedly apply-
ing the first rule above. Here, we wish to compute the area of red parallelogram determined
by the matrix A given in (A.2). In particular, the corner points of the parallelogram corres-
pond to the vectors [0, 0]⊤, [1, 3]⊤, [2, 4]⊤, and [3, 7]⊤.
0
0.5
1
1.5
2
2.5
3
-2
0
2
4
6
8
Figure A.2: The volume of the red parallelogram can be obtained by a number of shear
operations that do not change the volume.
Adding −2 times the first column of A to the second column gives the matrix
B =
"1
0
3
−2
#
,
Linear Algebra and Functional Analysis
359
corresponding to the blue parallelogram. The linear operation that transforms the red to the
blue parallelogram can be thought of as a succession of two linear transformations. The
first is to transform the coordinates of points on the red parallelogram (in standard basis)
to the basis formed by the columns of A. Second, relativ

e to this new basis, we apply the
matrix B above. Note that the input of this matrix is with respect to the new basis, whereas
the output is with respect to the standard basis. The matrix for the combined operation is
now
BA−1 =
"1
0
3
−2
# " −2
1
3/2
−1/2
#
=
"−2
1
−9
4
#
,
which maps [1, 3]⊤to [1, 3]⊤(does not change) and [2, 4]⊤to [0, −2]⊤. We say that we
apply a shear
shear
in the direction [1, 3]⊤. The significance of such an operation is that a shear
does not alter the volume of the parallelogram. The second (blue) parallelogram has an
easier form, because one of the sides is parallel to the y-axis. By applying another shear,
in the direction [0, −2]⊤, we can obtain a simple (green) rectangle, whose volume is 2. In
matrix terms, we add 3/2 times the second column of B to the first column of B, to obtain
the matrix
C =
"1
0
0
−2
#
,
which is a diagonal matrix, whose determinant is −2, corresponding to the volume 2 of all
the parallelograms.
Theorem A.2 summarizes a number of useful matrix rules for the concepts that we have
discussed so far. We leave the proofs, which typically involves “writing out” the equations,
as an exercise for the reader; see also [116].
Theorem A.2: Useful Matrix Rules
1. (AB)⊤= B⊤A⊤
2. (AB)−1 = B−1A−1
3. (A−1)⊤= (A⊤)−1 =: A−⊤
4. det(AB) = det(A) det(B)
5. x⊤Ax = tr  Axx⊤
6. det(A) = Q
i aii if A = [ai j] is triangular
Next, consider an n × p matrix A for which the matrix inverse fails to exist. That is, A
is either non-square (n , p) or its determinant is 0. Instead of the inverse, we can use its
so-called pseudo-inverse, which always exists.
360
Inner Product
Definition A.2: Moore–Penrose Pseudo-Inverse
The Moore–Penrose pseudo-inverse
Moore–Penrose
pseudo-inverse
of a real matrix A ∈Rn×p is defined as the
unique matrix A+ ∈Rp×n that satisfies the conditions:
1. AA+A = A
2. A+AA+ = A+
3. (AA+)⊤= AA+
4. (A+A)⊤= A+A
We can write A+ explicitly in terms of A when A has a full column or row rank. For
example, we always have
A⊤AA+ = A⊤(AA+

)⊤= ((AA+)A)⊤= (A)⊤= A⊤.
(A.4)
If A has a full column rank p, then (A⊤A)−1 exists, so that from (A.4) it follows that
A+ = (A⊤A)−1A⊤. This is referred to as the left pseudo-inverse
left
pseudo-inverse
, as A+A = Ip. Similarly, if
A has a full row rank n, that is, (AA⊤)−1 exists, then it follows from
A+AA⊤= (A+A)⊤A⊤= (A(A+A))⊤= A⊤
that A+ = A⊤(AA⊤)−1. This is the right pseudo-inverse
right
pseudo-inverse
, as AA+ = In. Finally, if A is of full
rank and square, then A+ = A−1.
A.2
Inner Product
The (Euclidean) inner product
inner product
of two real vectors x = [x1, . . . , xn]⊤and y = [y1, . . . , yn]⊤
is defined as the number
⟨x, y⟩=
n
X
i=1
xi yi = x⊤y.
Here x⊤y is the matrix multiplication of the (1 × n) matrix x⊤and the (n × 1) matrix y.
The inner product induces a geometry on the linear space Rn, allowing for the definition of
length, angle, and so on. The inner product satisfies the following properties:
1. ⟨αx + βy, z⟩= α ⟨x, z⟩+ β ⟨y, z⟩;
2. ⟨x, y⟩= ⟨y, x⟩;
3. ⟨x, x⟩⩾0;
4. ⟨x, x⟩= 0 if and only if x = 0.
Vectors x and y are called perpendicular (or orthogonal
orthogonal
) if ⟨x, y⟩= 0. The Euclidean
norm
Euclidean norm
(or length) of a vector x is defined as
||x|| =
q
x2
1 + · · · + x2
n =
p
⟨x, x⟩.
Linear Algebra and Functional Analysis
361
If x and y are perpendicular, then Pythagoras’ theorem
Pythagoras’
theorem
holds:
||x + y||2 = ⟨x + y, x + y⟩= ⟨x, x⟩+ 2 ⟨x, y⟩+ ⟨y, y⟩= ||x||2 + ||y||2.
(A.5)
A basis {v1, . . . , vn} of Rn in which all the vectors are pairwise perpendicular and have
norm 1 is called an orthonormal
orthonormal
(short for orthogonal and normalized) basis. For example,
the standard basis is orthonormal.
Theorem A.3: Orthonormal Basis Representation
If {v1, . . . , vn} is an orthonormal basis of Rn, then any vector x ∈Rn can be expressed
as
x = ⟨x, v1⟩v1 + · · · + ⟨x, vn⟩vn.
(A.6)
Proof: Observe that, because the {vi} form a basis, there exist unique α1, . . . , αn such that
x = α1v1 + · · · + αnvn. By the linearity of the inner product and 

the orthonormality of the
{vi} it follows that ⟨x, vj⟩= ⟨P
i αivi, v j⟩= α j.
□
An n × n matrix V whose columns form an orthonormal basis is called an orthogonal
matrix
orthogonal
matrix
.1 Note that for an orthogonal matrix V = [v1, . . . , vn], we have
V⊤V =

v⊤
1
v⊤
2...
v⊤
n

[v1, v2, . . . , vn] =

v⊤
1 v1
v⊤
1 v2
. . .
v⊤
1 vn
...
...
...
...
v⊤
n v1
v⊤
n v2
. . .
v⊤
n vn
= In.
Hence, V−1 = V⊤. Note also that an orthogonal transformation is length preserving
length
preserving
; that
is, Vx has the same length as x. This follows from
||Vx||2 = ⟨Vx, Vx⟩= x⊤V⊤Vx = x⊤x = ||x||2.
A.3
Complex Vectors and Matrices
Instead of the vector space Rn of n-dimensional real vectors, it is sometimes useful to
consider the vector space Cn of n-dimensional complex vectors. In this case the adjoint
adjoint
or conjugate transpose operation (∗) replaces the transpose operation (⊤). This involves
the usual transposition of the matrix or vector with the additional step that any complex
number z = x + i y is replaced by its complex conjugate z = x −i y. For example, if
x =
"a1 + i b1
a2 + i b2
#
and
A =
"a11 + i b11
a12 + i b12
a21 + i b21
a22 + i b22
#
,
then
x∗= [a1 −i b1, a2 −i b2]
and
A∗=
"a11 −i b11
a21 −i b21
a12 −i b12
a22 −i b22
#
.
1The qualifier “orthogonal” for such matrices has been fixed by history. A better term would have been
“orthonormal”.
362
Orthogonal Projections
The (Euclidean) inner product of x and y (viewed as column vectors) is now defined as
⟨x, y⟩= y∗x =
n
X
i=1
xi yi,
which is no longer symmetric: ⟨x, y⟩= ⟨y, x⟩. Note that this generalizes the real-valued
inner product. The determinant of a complex matrix A is defined exactly as in (A.3). As a
consequence, det(A∗) = det(A).
A complex matrix is said to be Hermitian or self-adjoint if A∗= A, and unitary if
Hermitian
unitary
A∗A = I (that is, if A∗= A−1). For real matrices “Hermitian” is the same as “symmetric”,
and “unitary” is the same as “orthogonal”.


A.4
Orthogonal Projections
Let {u1, . . . , uk} be a set of linearly independent vectors in Rn. The set
V = Span {u1, . . . , uk} = {α1u1 + · · · + αkuk, α1, . . . , αk ∈R},
is called the linear subspace spanned by {u1, . . . , uk}. The orthogonal complement of V,
linear subspace
orthogonal
complement
denoted by V⊥, is the set of all vectors w that are orthogonal to V, in the sense that
⟨w, v⟩= 0 for all v ∈V. The matrix P such that Px = x, for all x ∈V, and Px = 0, for all
x ∈V⊥is called the orthogonal projection matrix
orthogonal
projection
matrix
onto V. Suppose that U = [u1, . . . , uk]
has full rank, in which case U⊤U is an invertible matrix. The orthogonal projection matrix
P onto V = Span {u1, . . . , uk} is then given by
P = U(U⊤U)−1U⊤.
Namely, since PU = U, the matrix P projects any vector in V onto itself. Moreover, P
projects any vector in V⊥onto the zero vector. Using the pseudo-inverse, it is possible to
specify the projection matrix also for the case where U is not of full rank, leading to the
following theorem.
Theorem A.4: Orthogonal Projection
Let U = [u1, . . . , uk]. Then, the orthogonal projection matrix P onto V = Span{u1,
. . . , uk} is given by
P = U U+,
(A.7)
where U+ is the (right) pseudo-inverse of U.
Proof: By Property 1 of Definition A.2 we have PU = UU+U = U, so that P projects any
vector in V onto itself. Moreover, P projects any vector in V⊥onto the zero vector.
□
Note that in the special case where u1, . . . , uk above form an orthonormal basis of V,
then the projection onto V is very simple to describe, namely we have
Px = UU⊤x =
k
X
i=1
⟨x, ui⟩ui.
(A.8)
Linear Algebra and Functional Analysis
363
For any point x ∈Rn, the point in V that is closest to x is its orthogonal projection Px,
as the following theorem shows.
Theorem A.5: Orthogonal Projection and Minimal Distance
Let {u1, . . . , uk} be an orthonormal basis of subspace V and let P be the orthogonal
projection matrix onto V. The solution to the minimization program
min
y∈V ∥x

 −y∥2
is y = Px. That is, Px ∈V is closest to x.
Proof: We can write each point y ∈V as y = Pk
i=1 αi ui. Consequently,
∥x −y∥2 =

x −
k
X
i=1
αi ui, x −
k
X
i=1
αi ui

= ∥x∥2 −2
k
X
i=1
αi ⟨x, ui⟩+
k
X
i=1
α2
i .
Minimizing this with respect to the {αi} gives αi = ⟨x, ui⟩, i = 1, . . . , k. In view of (A.8),
the optimal y is thus Px.
□
A.5
Eigenvalues and Eigenvectors
Let A be an n × n matrix. If Av = λv for some number λ and non-zero vector v, then λ is
called an eigenvalue of A with eigenvector v.
eigenvalue
eigenvector
If (λ, v) is an (eigenvalue, eigenvector) pair, the matrix λI −A maps any multiple of v
to the zero vector. Consequently, the columns of λI −A are linearly dependent, and hence
its determinant is 0. This provides a way to identify the eigenvalues, namely as the r ⩽n
different roots λ1, . . . , λr of the characteristic polynomial
characteristic
polynomial
det(λI −A) = (λ −λ1)α1 · · · (λ −λr)αr,
where α1 + · · · + αr = n. The integer αi is called the algebraic multiplicity of λi. The
algebraic
multiplicity
eigenvectors that correspond to an eigenvalue λi lie in the kernel or null space of the matrix
null space
λiI −A; that is, the linear space of vectors v such that (λiI −A)v = 0. This space is called
the eigenspace of λi. Its dimension, di ∈{1, . . . , n}, is called the geometric multiplicity of
geometric
multiplicity
λi. It always holds that di ⩽αi. If P
i di = n, then we can construct a basis for Rn consisting
of eigenvectors, as illustrated next.
Example A.4 (Linear Transformation (cont.)) We revisit the linear transformation in
Figure A.1, where
A =
" 1
1
−1/2
−2
#
.
The characteristic polynomial is (λ −1)(λ + 2) + 1/2, with roots λ1 = −1/2 −
√
7/2 ≈
−1.8229 and λ2 = −1/2 +
√
7/2 ≈0.8229. The corresponding unit eigenvectors are v1 ≈
[0.3339, −0.9426]⊤and v2 ≈[0.9847, −0.1744]⊤. The eigenspace corresponding to λ1 is
364
Eigenvalues and Eigenvectors
V1 = Span {v1} = {β v1 : β ∈R} and the eigenspace corresponding to λ2 is V2 = Span {v2}.
The alg

ebraic and geometric multiplicities are 1 in this case. Any pair of vectors taken
from V1 and V2 forms a basis for R2. Figure A.3 shows how v1 and v2 are transformed to
Av1 ∈V1 and Av2 ∈V2, respectively.
-2
0
2
x
-3
-2
-1
0
1
2
3
y
Figure A.3: The dashed arrows are the unit eigenvectors v1 (blue) and v2 (red) of matrix A.
Their transformed values Av1 and Av2 are indicated by solid arrows.
A matrix for which the algebraic and geometric multiplicities of all its eigenvalues
are the same is called semi-simple. This is equivalent to the matrix being diagonalizable,
semi-simple
diagonalizable
meaning that there is a matrix V and a diagonal matrix D such that
A = VDV−1.
To see that this so-called eigen-decomposition
eigen-
decomposition
holds, suppose A is a semi-simple matrix
with eigenvalues
λ1, . . . , λ1
|     {z     }
d1
, · · · , λr, . . . , λr
|     {z     }
dr
.
Let D be the diagonal matrix whose diagonal elements are the eigenvalues of A, and let V
be a matrix whose columns are linearly independent eigenvectors corresponding to these
eigenvalues. Then, for each (eigenvalue, eigenvector) pair (λ, v), we have Av = λv. Hence,
in matrix notation, we have AV = VD, and so A = VDV−1.
A.5.1
Left- and Right-Eigenvectors
The eigenvector as defined in the previous section is called a right-eigenvector, as it lies on
the right of A in the equation Av = λv.
If A is a complex matrix with an eigenvalue λ, then the eigenvalue’s complex conjugate
λ is an eigenvalue of A∗. To see this, define B := λI −A and B∗:= λI −A∗. Since λ is
an eigenvalue, we have det(B) = 0. Applying the identity det(B) = det(B∗), we see that
Linear Algebra and Functional Analysis
365
therefore det(B∗) = 0, and hence that λ is an eigenvalue of A∗. Let w be an eigenvector
corresponding to λ. Then, A∗w = λw or, equivalently,
w∗A = λw∗.
For this reason, we call w∗the left-eigenvector
left-
eigenvector
of A for eigenvalue λ. If v is a (right-) ei-
genvector of A, then its adjoint v∗is usually not a left-eigenve

ctor, unless A∗A = AA∗
(such matrices are called normal;
normal matrix
a real symmetric matrix is normal). However, the im-
portant property holds that left- and right-eigenvectors belonging to different eigenvalues
are orthogonal. Namely, if w∗is a left-eigenvalue of λ1 and v a right-eigenvalue of λ2 , λ1,
then
λ1w∗v = w∗Av = λ2w∗v,
which can only be true if w∗v = 0.
Theorem A.6: Schur Triangulation
For any complex matrix A, there exists a unitary matrix U such that T = U−1AU is
upper triangular.
Proof: The proof is by induction on the dimension n of the matrix. Clearly, the statement
is true for n = 1, as A is simply a complex number and we can take U equal to 1. Suppose
that the result is true for dimension n. We wish to show that it also holds for dimension
n + 1. Any matrix A always has at least one eigenvalue λ with eigenvector v, normalized
to have length 1. Let U be any unitary matrix whose first column is v. Such a matrix can
always be constructed2. As U is unitary, the first row of U−1 is v∗, and U−1AU is of the form
" v∗
∗
#
A
h
v
∗
i
|   {z   }
U
=
" λ
∗
0
B
#
,
for some matrix B. By the induction hypothesis, there exists a unitary matrix W and an
upper triangular matrix T such that W−1BW = T. Now, define
V :=
" 1
0⊤
0
W
#
.
Then,
V−1 
U−1AU

V =
" 1
0⊤
0
W−1
# " λ
∗
0
B
# " 1
0⊤
0
W
#
=
" λ
∗
0
W−1BW
#
=
" λ
∗
0
T
#
,
which is upper triangular of dimension n + 1. Since UV is unitary, this completes the
induction, and hence the result is true for all n.
□
The theorem above can be used to prove an important property of Hermitian matrices,
i.e., matrices for which A∗= A.
2After specifying v we can complete the rest of the unitary matrix via the Gram–Schmidt procedure, for
example; see Section A.6.4.
366
Eigenvalues and Eigenvectors
Theorem A.7: Eigenvalues of a Hermitian Matrix
Any n × n Hermitian matrix has real eigenvalues. The corresponding matrix of nor-
malized eigenvectors is a unitary matrix.
Proof: Let A be a Hermitian matrix. By Theorem A.6 ther

e exists a unitary matrix U such
that U−1AU = T, where T is upper triangular. It follows that the adjoint (U−1AU)∗= T∗
is lower triangular. However, (U−1AU)∗= U−1AU, since A∗= A and U∗= U−1. Hence,
T and T∗must be the same, which can only be the case if T is a real diagonal matrix D.
Since AU = DU, the diagonal elements are exactly the eigenvalues and the corresponding
eigenvectors are the columns of U.
□
In particular, the eigenvalues of a real symmetric matrix are real. We can now repeat
the proof of Theorem A.6 with real eigenvalues and eigenvectors, so that there exists an
orthogonal matrix Q such that Q−1AQ = Q⊤AQ = D. The eigenvectors can be chosen as
the columns of Q, which form an orthonormal basis. This proves the following theorem.
Theorem A.8: Real Symmetric Matrices are Orthogonally Diagonizable
Any real symmetric matrix A can be written as
A = QDQ⊤,
where D is the diagonal matrix of (real) eigenvalues and Q is an orthogonal matrix
whose columns are eigenvectors of A.
Example A.5 (Real Symmetric Matrices and Ellipses) As we have seen, linear trans-
formations map circles into ellipses. We can use the above theory for real symmetric
matrices to identify the principal axes. Consider, for example, the transformation with mat-
rix A = [1, 1; −1/2, −2] in (A.1). A point x on the unit circle is mapped to a point y = Ax.
Since for such points ∥x∥2 = x⊤x = 1, we have that y satisfies y⊤(A−1)⊤A−1y = 1, which
gives the equation for the ellipse
17 y2
1
9
+ 20 y1y2
9
+ 8 y2
2
9
= 1.
Let Q be the orthogonal matrix of eigenvectors of the symmetric matrix (A−1)⊤A−1 =
(AA⊤)−1, so Q⊤(AA⊤)−1Q = D for some diagonal matrix D. Taking the inverse on both
sides of the previous equation, we have Q⊤AA⊤Q = D−1, which shows that Q is also the
matrix of eigenvectors of AA⊤. These eigenvectors point precisely in the direction of the
principal axes, as shown in Figure A.4. It turns out, see Section A.6.5, that the square roots
of the eigenvalues of AA⊤, here approximately 2.4221 and 

0.6193, correspond to the sizes
of the principal axes of the ellipse, as illustrated in Figure A.4.
Linear Algebra and Functional Analysis
367
-1
0
1
-2
-1
0
1
2
Figure A.4: The eigenvectors and eigenvalues of AA⊤determine the principal axes of the
ellipse.
The following definition generalizes the notion of positivity of a real variable to that of a
(Hermitian) matrix, providing a crucial concept for multivariate differentiation and optim-
ization; see Appendix B.
☞397
Definition A.3: Positive (Semi)Definite Matrix
A Hermitian matrix A is called positive semidefinite
positive
semidefinite
(we write A ⪰0) if ⟨Ax, x⟩⩾0
for all x. It is called positive definite (we write A ≻0) if ⟨Ax, x⟩> 0 for all x , 0.
The positive (semi)definiteness of a matrix can be directly related to the positivity of
its eigenvalues, as follows:
Theorem A.9: Eigenvalues of a Positive Semidefinite Matrix
All eigenvalues of a positive semidefinite matrix are non-negative and all eigenval-
ues of a positive definite matrix are strictly positive.
Proof: Let A be a positive semidefinite matrix. By Theorem A.7, the eigenvalues of A are
all real. Suppose λ is an eigenvalue with eigenvector v. As A is positive semidefinite, we
have
0 ⩽⟨Av, v⟩= λ ⟨v, v⟩= λ ∥v∥2,
which can only be true if λ ⩾0. Similarly, for a positive definite matrix, λ must be strictly
greater than 0.
□
Corollary A.1 Any real positive semidefinite matrix A can be written as
A = BB⊤
for some real matrix B. Conversely, for any real matrix B, the matrix BB⊤is positive
semidefinite.
368
Matrix Decompositions
Proof: The matrix A is both Hermitian (by definition) and real (by assumption) and hence
it is symmetric. By Theorem A.8, we can write A = QDQ⊤, where D is the diagonal
matrix of (real) eigenvalues of A. By Theorem A.9 all eigenvalues are non-negative, and
thus their square root is real-valued. Now, define B = Q
√
D, where
√
D is defined as the
diagonal matrix whose diagonal elements are the square roots of the eigenvalues of A.
Then

, BB⊤= Q
√
D (
√
D)⊤Q⊤= QDQ⊤= A. The converse statement follows from the
fact that x⊤BB⊤x = ∥B⊤x∥2 ⩾0 for all x.
□
A.6
Matrix Decompositions
Matrix decompositions are frequently used in linear algebra to simplify proofs, avoid nu-
merical instability, and to speed up computations. We mention three important matrix de-
compositions: (P)LU, QR, and SVD.
A.6.1
(P)LU Decomposition
Every invertible matrix A can be written as the product of three matrices:
A = PLU,
(A.9)
where L is a lower triangular matrix, U an upper triangular matrix, and P a permutation
matrix
permutation
matrix
. A permutation matrix is a square matrix with a single 1 in each row and column,
and zeros otherwise. The matrix product PB simply permutes the rows of a matrix B and,
likewise, BP permutes its columns. A decomposition of the form (A.9) is called a PLU
decomposition
PLU
decomposition
. As a permutation matrix is orthogonal, its transpose is equal to its inverse,
and so we can write (A.9) as
P⊤A = LU.
The decomposition is not unique, and in many cases P can be taken to be the identity
matrix, in which case we speak of the LU decomposition of A, also called the LR for
left–right (triangular) decomposition.
A PLU decomposition of an invertible n × n matrix A0 can be obtained recursively as
follows. The first step is to swap the rows of A0 such that the element in the first column and
first row of the pivoted matrix is as large as possible in absolute value. Write the resulting
matrix as
eP0A0 =
"a1
b⊤
1
c1
D1
#
,
where eP0 is the permutation matrix that swaps the first and k-th row, where k is the row
that contains the largest element in the first column. Next, add the matrix −c1[1, b⊤
1 /a1] to
the last n −1 rows of eP0A0, to obtain the matrix
"a1
b⊤
1
0
D1 −c1b⊤
1 /a1
#
=:
"a1
b⊤
1
0
A1
#
.
In effect, we add some multiple of the first row to each of the remaining rows in order to
obtain zeros in the first column, except for the first element.
We now apply the same procedure to A1 as we did to 

A0 and then to subsequent smaller
matrices A2, . . . , An−1:
Linear Algebra and Functional Analysis
369
1. Swap the first row with the row having the maximal absolute value element in the
first column.
2. Make every other element in the first column equal to 0 by adding appropriate mul-
tiples of the first row to the other rows.
Suppose that At has a PLU decomposition PtLtUt. Then it is easy to check that
eP⊤
t−1
"1
0⊤
0
Pt
#
|        {z        }
Pt−1
"
1
0⊤
P⊤
t ct/at
Lt
#
|           {z           }
Lt−1
"at
b⊤
t
0
Ut
#
|   {z   }
Ut−1
(A.10)
is a PLU decomposition of At−1. Since the PLU decomposition for the scalar An−1 is trivial,
by working backwards we obtain a PLU decomposition P0L0U0 of A.
Example A.6 (PLU Decomposition) Take
A =

0
−1
7
3
2
0
1
1
1
.
Our goal is to modify A via Steps 1 and 2 above so as to obtain an upper triangular matrix
with maximal elements on the diagonal. We first swap the first and second row. Next, we
add −1/3 times the first row to the third row and 1/3 times the second row to the third row:

0
−1
7
3
2
0
1
1
1
−→

3
2
0
0
−1
7
1
1
1
−→

3
2
0
0
−1
7
0
1/3
1
−→

3
2
0
0
−1
7
0
0
10/3
.
The final matrix is U0, and in the process we have applied the permutation matrices
eP0 =

0
1
0
1
0
0
0
0
1
,
eP1 =
"1
0
0
1
#
.
Using the recursion (A.10) we can now recover P0 and L0. Namely, at the final iteration
we have P2 = 1, L2 = 1, and U2 = 10/3. And subsequently,
P1 =
"1
0
0
1
#
,
L1 =
" 1
0
−1/3
1
#
,
P0 =

0
1
0
1
0
0
0
0
1
,
L0 =

1
0
0
0
1
0
1/3
−1/3
1
,
observing that a1 = 3, c1 = [0, 1]⊤, a2 = −1, and c2 = 1/3.
PLU decompositions can be used to solve large systems of linear equations of the form
Ax = b efficiently, especially when such an equation has to be solved for many different b.
This is done by first decomposing A into PLU, and then solving two triangular systems:
1. Ly 

= P⊤b.
2. Ux = y.
370
Matrix Decompositions
The first equation can be solved efficiently via forward substitution, and the second via
forward
substitution
backward substitution, as illustrated in the following example.
backward
substitution
Example A.7 (Solving Linear Equations with an LU Decomposition) Let A = PLU
be the same as in Example A.6. We wish to solve Ax = [1, 2, 3]⊤. First, solving

1
0
0
0
1
0
1/3
−1/3
1


y1
y2
y3
=

2
1
3

gives, y1 = 2, y2 = 1 and y3 = 3 −2/3 + 1/3 = 8/3, by forward substitution. Next,

3
2
0
0
−1
7
0
0
10/3


x1
x2
x3
=

2
1
8/3

gives x3 = 4/5, x2 = −1 + 28/5 = 23/5, and x1 = 2(1 −23/5)/3 = −12/5, so x =
[−12, 23, 4]⊤/5.
A.6.2
Woodbury Identity
LU (or more generally PLU) decompositions can also be applied to block matrices. A
starting point is the following LU decomposition for a general 2 × 2 matrix:
"a
b
c
d
#
=
"a
0
c
d −bc/a
# "1
b/a
0
1
#
,
which holds as long as a , 0; this can be seen by simply writing out the matrix product.
The block matrix generalization for matrices A ∈Rn×n, B ∈Rn×k, C ∈Rk×n, D ∈Rk×k is
Σ :=
"A
B
C
D
#
=
"A
On×k
C
D −CA−1B
# " In
A−1B
Ok×n
Ik
#
,
(A.11)
provided that A is invertible (again, write out the block matrix product). Here, we use the
notation Op×q to denote the p × q matrix of zeros. We can further rewrite this as:
Σ =
" In
On×k
CA−1
Ik
# " A
On×k
Ok×n
D −CA−1B
# " In
A−1B
Ok×n
Ik
#
.
Thus, inverting both sides, we obtain
Σ−1 =
" In
A−1B
Ok×n
Ik
#−1 " A
On×k
Ok×n
D −CA−1B
#−1 " In
On×k
CA−1
Ik
#−1
.
Inversion of the above block matrices gives (again write out)
Σ−1 =
" In
−A−1B
Ok×n
Ik
# " A−1
On×k
Ok×n
(D −CA−1B)−1
# "
In
On×k
−CA−1
Ik
#
.
(A.12)
Assuming that D is invertible, we could also perform a block UL (as opposed to LU)
decomposition:
Σ =
"A −BD−1C
B
Ok×n
D
# " In
On×k
D−1C
Ik
#
,
(A.13)
Linear Algebra and Functional Analysis
371
which, after a similar calculation as t

he one above, yields
Σ−1 =
"
In
On×k
−D−1C
Ik
# "(A −BD−1C)−1
On×k
Ok×n
D−1
# " In
−BD−1
Ok×n
Ik
#
.
(A.14)
The upper-left block of Σ−1 from (A.14) must be the same as the upper-left block of Σ−1
from (A.12), leading to the Woodbury identity
Woodbury
identity
:
(A −BD−1C)−1 = A−1 + A−1B(D −CA−1B)−1CA−1.
(A.15)
From (A.11) and the fact that the determinant of a product is the product of the determ-
inants, we see that det(Σ) = det(A) det(D −CA−1B). Similarly, from (A.13) we have
det(Σ) = det(A −BD−1C) det(D), leading to the identity
det(A −BD−1C) det(D) = det(A) det(D −CA−1B).
(A.16)
The following special cases of (A.16) and (A.15) are of particular importance.
Theorem A.10: Sherman–Morrison Formula
Suppose that A ∈Rn×n is invertible and x, y ∈Rn. Then,
det(A + xy⊤) = det(A)(1 + y⊤A−1x).
If in addition y⊤A−1x , −1, then the Sherman–Morrison formula
Sherman–
Morrison
formula
holds:
(A + xy⊤)−1 = A−1 −A−1xy⊤A−1
1 + y⊤A−1x.
Proof: Take B = x, C = −y⊤, and D = 1 in (A.16) and (A.15).
□
One important application of the Sherman–Morrison formula is in the efficient solution
of the linear system Ax = b, where A is an n × n matrix of the form:
A = A0 +
p
X
j=1
aja⊤
j
for some column vectors a1, . . . , ap ∈Rn and n×n diagonal (or otherwise easily invertible)
matrix A0. Such linear systems arise, for example, in the context of ridge regression and
☞217
optimization.
☞414
To see how the Sherman–Morrison formula can be exploited, define the matrices
A0, . . . , Ap via the recursion:
Ak = Ak−1 + aka⊤
k ,
k = 1, . . . , p.
Application of Theorem A.10 for k = 1, . . . , p yields the identities:3
A−1
k = A−1
k−1 −A−1
k−1aka⊤
k A−1
k−1
1 + a⊤
k A−1
k−1ak
|Ak| = |Ak−1| ×

1 + a⊤
k A−1
k−1ak

.
3Here |A| is a shorthand notation for det(A).
372
Matrix Decompositions
Therefore, by evolving the recursive relationships up until k = p, we obtain:
A−1
p = A−1
0 −
p
X
j=1
A−1
j−1aja⊤
j A−1
j−1
1 + a⊤
j A−1
j−1aj
|Ap| = |A0| ×
p
Y
j=1

1 + a⊤
j A−1
j−1aj

.
These expressions will allow us 

to easily compute A−1 = A−1
p and |A| = |Ap| provided the
following quantities are available:
ck,j := A−1
k−1aj,
k = 1, . . . , p −1,
j = k + 1, . . . , p.
Since, by Theorem A.10, we can write:
A−1
k−1aj = A−1
k−2aj −A−1
k−2ak−1a⊤
k−1A−1
k−2
1 + a⊤
k−1A−1
k−2ak−1
aj,
the quantities {ck,j} can be computed from the recursion:
c1,j = A−1
0 aj,
j = 1, . . . , p
ck,j = ck−1,j −
a⊤
k−1ck−1,j
1 + a⊤
k−1ck−1,k−1
ck−1,k−1,
k = 2, . . . , p,
j = k, . . . , p.
(A.17)
Observe that this recursive computation takes O(p2n) time and that once {ck,j} are available,
we can express A−1 and |A| as:
A−1 = A−1
0 −
p
X
j=1
c j,j c⊤
j,j
1 + a⊤
j c j,j
|A| = |A0| ×
p
Y
j=1

1 + a⊤
j c j,j

.
In summary, we have proved the following.
Theorem A.11: Sherman–Morrison Recursion
The inverse and determinant of the n × n matrix A = A0 + Pp
k=1 aka⊤
k are given
respectively by:
A−1 = A−1
0 −CD−1C⊤
det(A) = det(A0) det(D),
where C ∈Rn×p and D ∈Rp×p are the matrices
C :=
h
c1,1, . . . , cp,p
i
,
D := diag

1 + a⊤
1 c1,1 , · · · , 1 + a⊤
p cp,p

,
and all the {cj,k} are computed from the recursion (A.17) in O(p2n) time.
Linear Algebra and Functional Analysis
373
As a consequence of Theorem A.11, the solution to the linear system Ax = b can be
computed in O(p2n) time via:
x = A−1
0 b −CD−1[C⊤b].
If n > p, the Sherman–Morrison recursion can frequently be much faster than the O(n3)
direct solution via the LU decomposition method in Section A.6.1.
☞368
In summary, the following algorithm computes the matrices C and D in Theorem A.11
via the recursion (A.17).
Algorithm A.6.1: Sherman–Morrison Recursion
input: Easily invertible matrix A0 and column vectors a1, . . . , ap.
output: Matrices C and D such that CD−1C⊤= A−1
0 −

A0 + P
j aja⊤
j
−1.
1 ck ←A−1
0 ak for k = 1, . . . , p (assuming A0 is diagonal or easily invertible matrix)
2 for k = 1, . . . , p −1 do
3
dk ←1 + a⊤
k ck
4
for j = k + 1, . . . , p do
5
cj ←c j −a⊤
k c j
dk
ck
6 dp ←1 + a⊤
p cp
7 C ←[c1, . . . , cp]
8 D ←diag(d1, . . . , dp)
9 ret

urn C and D
Finally, note that if A0 is a diagonal matrix and we only store the diagonal elements of
D and A0 (as opposed to storing the full matrices D and A0), then the storage or memory
requirements of Algorithm A.6.1 are only O(p n).
A.6.3
Cholesky Decomposition
If A is a real-valued positive definite matrix (and therefore symmetric), e.g., a covariance
matrix, then an LU decomposition can be achieved with matrices L and U = L⊤.
Theorem A.12: Cholesky Decomposition
A real-valued positive definite matrix A = [ai j] ∈Rn×n can be decomposed as
A = LL⊤,
where the real n × n lower triangular matrix L = [lk j] satisfies the recursive formula
lk j = ak j −Pj−1
i=1 lji lki
q
ajj −Pj−1
i=1 l2
ji
,
where
0
X
i=1
lji lki := 0
(A.18)
for k = 1, . . . , n and j = 1, . . . , k.
374
Matrix Decompositions
Proof: The proof is by inductive construction. For k = 1, . . . , n, let Ak be the left-upper
k ×k submatrix of A = An. With e1 := [1, 0, . . . , 0]⊤, we have A1 = a11 = e⊤
1 Ae1 > 0 by the
positive-definiteness of A. It follows that l11 = √a11. Suppose that Ak−1 has a Cholesky fac-
torization Lk−1L⊤
k−1 with Lk−1 having strictly positive diagonal elements, we can construct
a Cholesky factorization of Ak as follows. First write
Ak =
"Lk−1L⊤
k−1
ak−1
a⊤
k−1
akk
#
and propose Lk to be of the form
Lk =
"Lk−1
0
l⊤
k−1
lkk
#
for some vector lk−1 ∈Rk−1 and scalar lkk, for which it must hold that
"Lk−1L⊤
k−1
ak−1
a⊤
k−1
akk
#
=
"Lk−1
0
l⊤
k−1
lkk
# " L⊤
k−1
lk−1
0⊤
lkk
#
.
To establish that such an lk−1 and lkk exist, we must verify that the set of equations
Lk−1lk−1 = ak−1
l⊤
k−1lk−1 + l2
kk = akk
(A.19)
has a solution. The system Lk−1lk−1 = ak−1 has a unique solution, because (by assump-
tion) Lk−1 is lower diagonal with strictly positive entries down the main diagonal and we
can solve for lk−1 using forward substitution: lk−1 = L−1
k−1ak−1. We can solve the second
equation as lkk =
p
akk −∥lk−1∥2, provided that the term within the square root is positive.
We demonstrate this using

 the fact that A is a positive definite matrix. In particular, for
x ∈Rn of the form [x⊤
1 , x2, 0⊤]⊤, where x1 is a non-zero (k −1)-dimensional vector and x2
a non-zero number, we have
0 < x⊤Ax = x⊤
1 , x2
 "Lk−1L⊤
k−1
ak−1
a⊤
k−1
akk
# "x1
x2
#
= ∥L⊤
k−1x1∥2 + 2x⊤
1 ak−1x2 + akkx2
2.
Now take x1 = −x2 L−⊤
k−1 lk−1 to obtain 0 < x⊤Ax = x2
2 (akk −∥lk−1∥2). Therefore, (A.19)
can be uniquely solved. As we have already solved it for k = 1, we can solve it for any
k = 1, . . . , n, leading to the recursive formula (A.18) and Algorithm A.6.2 below.
□
An implementation of Cholesky’s decomposition that uses the notation in the proof of
Theorem A.6.3 is the following algorithm, whose running cost is O(n3).
Algorithm A.6.2: Cholesky Decomposition
input: Positive-definite n × n matrix An with entries {ai j}.
output: Lower triangular Ln such that LnL⊤
n = An.
1 L1 ←√a11
2 for k = 2, . . . , n do
3
ak−1 ←[a1k, . . . , ak−1,k]⊤
4
lk−1 ←L−1
k−1ak−1
(computed in O(k2) time via forward substitution)
5
lkk ←
p
akk −l⊤
k−1lk−1
6
Lk ←
"Lk−1
0
l⊤
k−1
lkk
#
7 return Ln
Linear Algebra and Functional Analysis
375
A.6.4
QR Decomposition and the Gram–Schmidt Procedure
Let A be an n × p matrix, where p ⩽n. Then, there exists a matrix Q ∈Rn×p satisfying
Q⊤Q = Ip, and an upper triangular matrix R ∈Rp×p, such that
A = QR.
This is the QR decomposition for real-valued matrices. When A has full column rank, such
a decomposition can be obtained via the Gram–Schmidt procedure, which constructs an
Gram–Schmidt
orthonormal basis {u1, . . . , up} of the column space of A spanned by {a1, . . . , ap}, in the
following way (see also Figure A.5):
1. Take u1 = a1/∥a1∥.
2. Let p1 be the projection of a2 onto Span {u1}. That is, p1 = ⟨u1, a2⟩u1. Now take
u2 = (a2 −p1)/∥a2 −p1∥. This vector is perpendicular to u1 and has unit length.
3. Let p2 be the projection of a3 onto Span {u1, u2}. That is, p2 = ⟨u1, a3⟩u1+⟨u2, a3⟩u2.
Now take u3 = (a3 −p2)/∥a3 −p2∥. This vector is perpendicular to both u1 and u2
and h

as unit length.
4. Continue this process to obtain u4, . . . , up.
0
1
2
-1
0
1
2
3
p1
a1
a2
a2 ! p1
u1
u2
Figure A.5: Illustration of the Gram–Schmidt procedure.
At the end of the procedure, a set {u1, . . . , up} of p orthonormal vectors are obtained.
Consequently, as a result of Theorem A.3,
aj =
j
X
i=1
⟨aj, ui⟩
| {z }
rij
ui,
j = 1, . . . , p,
376
Matrix Decompositions
for some numbers ri j, j = 1, . . . , i, i = 1, . . . , p. Denoting the corresponding upper triangu-
lar matrix [rij] by R, we have in matrix notation:
QR = [u1, . . . , up]

r11
r12
r13
. . .
r1p
0
r22
r23
. . .
r2p
...
0
...
...
...
0
0
0
. . .
rpp

= [a1, . . . , ap] = A,
which yields a QR decomposition. The QR decomposition can be used to efficiently solve
least-squares problems; this will be shown shortly. It can also be used to calculate the
determinant of the matrix A, whenever A is square. Namely, det(A) = det(Q) det(R) =
det(R); and since R is triangular, its determinant is the product of its diagonal elements.
There exist various improvements of the Gram–Schmidt process (for example, the House-
holder transformation [52]) that not only improve the numerical stability of the QR de-
composition, but also can be applied even when A is not full rank.
An important application of the QR decomposition is found in solving the least-squares
problem in O(p2 n) time:
min
β ∈Rp ∥Xβ −y∥2
for some X ∈Rn×p (model) matrix. Using the defining properties of the pseudo-inverse in
Definition A.2, one can show that ∥XX+y−y∥2 ⩽∥Xβ−y∥2 for any β. In other words, bβ :=
☞360
X+y minimizes ∥Xβ −y∥. If we have the QR decomposition X = QR, then a numerically
stable way to calculate bβ with an O(p2 n) cost is via
bβ = (QR)+y = R+Q+y = R+Q⊤y.
If X has full column rank, then R+ = R−1.
Note that while the QR decomposition is the method of choice for solving the ordinary
least-squares regression problem, the Sherman–Morrison recursion is the method of choice
☞372
for solving the regula

rized least-squares (or ridge) regression problem.
☞217
A.6.5
Singular Value Decomposition
One of the most useful matrix decompositions is the singular value decomposition
singular value
decomposition
(SVD).
Theorem A.13: Singular Value Decomposition
Any (complex) matrix m × n matrix A admits a unique decomposition
A = UΣV∗,
where U and V are unitary matrices of dimension m and n, respectively, and Σ is a
real m × n diagonal matrix. If A is real, then U and V are both orthogonal matrices.
Proof: Without loss of generality we can assume that m ⩾n (otherwise consider the trans-
pose of A). Then A∗A is a positive semidefinite Hermitian matrix, because ⟨A∗Av, v⟩=
v∗A∗Av = ∥Av∥2 ⩾0 for all v. Hence, A∗A has non-negative real eigenvalues, λ1 ⩾λ2 ⩾
Linear Algebra and Functional Analysis
377
· · · ⩾λn ⩾0. By Theorem A.7 the matrix V = [v1, . . . , vn] of right-eigenvectors is a unit-
ary matrix. Define the i-th singular value
singular value
as σi = √λi, i = 1, . . . , n and suppose λ1, . . . , λr
are all greater than 0, and λr+1, . . . , λn = 0. In particular, Avi = 0 for i = r + 1, . . . , n. Let
ui = Avi/σi, i = 1, . . . , r. Then, for i, j ⩽r,
⟨ui, uj⟩= u∗
jui =
v∗
jA∗Avi
σi σj
= λi 1{i = j}
σiσj
= 1{i = j}.
We can extend u1, . . . , ur to an orthonormal basis {u1, . . . , um} of Cm (e.g., using the Gram–
Schmidt procedure). Let U = [u1, . . . , un] be the corresponding unitary matrix. Defining Σ
to be the m × n diagonal matrix with diagonal (σ1, . . . , σr, 0, . . . , 0), we have,
UΣ = [Av1, . . . , Avr, 0, . . . , 0] = AV,
and hence A = UΣV∗.
□
Note that
AA∗= UΣV∗VΣ∗U∗= UΣΣ⊤U∗
and
A∗A = VΣ∗U∗UΣV∗= VΣ⊤ΣV∗.
So, U is a unitary matrix whose columns are eigenvectors of AA∗and V is a unitary matrix
whose columns are eigenvectors of A∗A.
The SVD makes it possible to write the matrix A as a sum of rank-1 matrices, weighted
by the singular values {σi}:
A =
h
u1, u2, . . . , um
i

σ1
0
. . .
. . .
0
0
...
0
. . .
0
0
. . .
σr
. . .
0
0
. . .
. . .
0 . . .


0
0
. . .
. . .
. . . ...
0


v∗
1
v∗
2...
v∗
n

=
rX
i=1
σi ui v∗
i ,
(A.20)
which is called the dyade or spectral representation
spectral
representation
of A.
For real-valued matrices, the SVD has a nice geometric interpretation, illustrated in
Figure A.6. The linear mapping defined by matrix A can be thought of as a succession of
three linear operations: (1) an orthogonal transformation (i.e., a rotation with a possible
flipping of some axes), corresponding to matrix V⊤, followed by (2) a simple scaling of
the unit vectors, corresponding to Σ, followed by (3) another orthogonal transformation,
corresponding to U.
-4 -2
0
2
4
-2
0
2
-4 -2
0
2
4
-2
0
2
-4 -2
0
2
4
-2
0
2
-4 -2
0
2
4
-2
0
2
Figure A.6: The figure shows how the unit circle and unit vectors (first panel) are first
rotated (second panel), then scaled (third panel), and finally rotated and flipped.
378
Matrix Decompositions
Example A.8 (Ellipses) We continue Example A.5. Using the svd method of the mod-
ule numpy.linalg, we obtain the following SVD matrices for matrix A:
U =
"−0.5430
0.8398
0.8398
0.5430
#
, Σ =
"2.4221
0
0
0.6193
#
, and V =
"−0.3975
0.9176
−0.9176
−0.3975
#
.
Figure A.4 shows the columns of the matrix UΣ as the two principal axes of the ellipse that
☞367
is obtained by applying matrix A to the points of the unit circle.
A practical method to compute the pseudo-inverse of a real-valued matrix A is via the
singular value decomposition A = UΣV⊤, where Σ is the diagonal matrix collecting all the
positive singular values, say σ1, . . . , σr, as in Theorem A.13. In this case, A+ = VΣ+U⊤,
where Σ+ is the n × m diagonal (pseudo-inverse) matrix:
Σ+ =

σ−1
1
0
. . .
. . .
0
0
...
0
. . .
0
0
. . .
σ−1
r
. . .
0
0
. . .
. . .
0 . . .
0
0
. . .
. . .
. . . ...
0

.
We conclude with a typical application of the pseudo-inverse for a least-squares optim-
ization problem from data science.
Examp

le A.9 (Rank-Deficient Least Squares) Given is an n × p data matrix
X =

x11
x12
· · ·
x1p
x21
x22
· · ·
x2p
...
...
...
...
xn1
xn2
· · ·
xnp

.
It is assumed that the matrix is of full row rank (all rows of X are linearly independent) and
that the number of rows is less than the number of columns: n < p. Under this setting, any
solution to the equation Xβ = y provides a perfect fit to the data and minimizes (to 0) the
least-squares problem
bβ = argmin
β ∈Rp ∥Xβ −y∥2.
(A.21)
In particular, if β∗minimizes ∥Xβ −y∥2 then so does β∗+ u for all u in the null space
NX := {u : Xu = 0}, which has dimension p −n. To cope with the non-uniqueness of
solutions, a possible approach is to solve instead the following optimization problem:
minimize
β ∈Rp
β⊤β
subject to
Xβ −y = 0.
That is, we are interested in a solution β with the smallest squared norm (or, equival-
ently, the smallest norm). The solution can be obtained via Lagrange’s method (see Sec-
tion B.2.2). Specifically, set L(β, λ) = β⊤β −λ⊤(Xβ −y), and solve
☞406
∇βL(β, λ) = 2β −X⊤λ = 0,
(A.22)
Linear Algebra and Functional Analysis
379
and
∇λL(β, λ) = Xβ −y = 0.
(A.23)
From (A.22) we get β = X⊤λ/2. By substituting it in (A.23), we arrive at λ = 2(XX⊤)−1y,
and hence β is given by
β = X⊤λ
2
= X⊤2(XX⊤)−1y
2
= X⊤(XX⊤)−1y = X+y.
An example Python code is given below.
svdexample.py
from numpy import diag, zeros,vstack
from numpy.random import rand, seed
from numpy.linalg import svd, pinv
seed(12345)
n = 5
p = 8
X = rand(n,p)
y = rand(n,1)
U,S,VT = svd(X)
SI
= diag(1/S)
# compute pseudo inverse
pseudo_inv = VT.T @ vstack((SI, zeros((p-n,n)))) @ U.T
b = pseudo_inv @ y
#b = pinv(X) @ y
#remove comment for the built-in pseudo inverse
print(X @ b - y)
[[5.55111512e-16]
[1.11022302e-16]
[5.55111512e-16]
[8.60422844e-16]
[2.22044605e-16]]
A.6.6
Solving Structured Matrix Equations
For a general matrix A ∈Cn×n, performing matrix–vector multiplications takes O(n2) op-
erations; and solving linear syste

ms Ax = b, and carrying out LU decompositions takes
O(n3) operations. However, when A is sparse (i.e., has relatively few non-zero elements)
or has a special structure, the computational complexity for these operations can often be
reduced. Matrices A that are “structured” in this way often satisfy a Sylvester equation
Sylvester
equation
, of
the form
M1A −AM∗
2 = G1G∗
2,
(A.24)
where Mi ∈Cn×n, i = 1, 2 are sparse matrices and Gi ∈Cn×r, i = 1, 2 are matrices of rank
r ≪n. The elements of A must be easy to recover from these matrices, e.g., with O(1) op-
erations. A typical example is a (square) Toeplitz matrix
Toeplitz matrix
, which has the following structure:
380
Matrix Decompositions
A =

a0
a−1
· · ·
a−(n−2)
a−(n−1)
a1
a0
a−1
a−(n−2)
...
a1
a0
...
...
an−2
...
...
a−1
an−1
an−2
· · ·
a1
a0

.
A general square Toeplitz matrix A is completely determined by the 2n −1 elements along
its first row and column. If A is also Hermitian (i.e., A∗= A), then clearly it is determined
by only n elements. If we define the matrices:
M1 =

0
0
· · ·
0
1
1
0
0
0
...
1
0
...
...
0
...
...
0
0
0
· · ·
1
0

and
M2 =

0
1
· · ·
0
0
0
0
1
0
...
0
0
...
...
0
...
...
1
−1
0
· · ·
0
0

,
then (A.24) is satisfied with
G1G∗
2 :=

1
0
0
a1 + a−(n−1)
0
a2 + a−(n−2)
...
...
0
an−1 + a−1

"an−1 −a−1
an−2 −a−2
. . .
a1 −a−(n−1)
2a0
0
0
. . .
0
1
#
=

an−1 −a−1
an−2 −a−2
. . .
a1 −a−(n−1)
2a0
0
0
. . .
0
a1 + a−(n−1)
...
...
. . .
...
a2 + a−(n−2)
...
...
. . .
...
...
0
0
. . .
0
an−1 + a−1

,
which has rank r ⩽2.
Example A.10 (Discrete Convolution of Vectors)
The convolution of two vectors
can be represented as multiplication of one of the vectors by a Toeplitz matrix. Sup-
pose a = [a1, . . . , an]⊤and b = [b1, . . . , bn]⊤are two complex-valued vectors. Then

, their
convolution
convolution
is defined as the vector a ∗b with i-th element
[a ∗b]i =
n
X
k=1
ak bi−k+1,
i = 1, . . . , n,
where b j := 0 for j ⩽0. It is easy to verify that the convolution can be written as
a ∗b = Ab,
Linear Algebra and Functional Analysis
381
where, denoting the d-dimensional column vector of zeros by 0d, we have that
A =

a
0
0n−1
a
...
0n−2
...
0n−2
...
a
0n−1
0
a

.
Clearly, the matrix A is a (sparse) Toeplitz matrix.
A circulant matrix
circulant
matrix
is a special Toeplitz matrix which is obtained from a vector c by
circularly permuting its indices as follows:
C =

c0
cn−1
. . .
c2
c1
c1
c0
cn−1
c2
...
c1
c0
...
...
cn−2
...
...
cn−1
cn−1
cn−2
. . .
c1
c0

.
(A.25)
Note that C is completely determined by the n elements of its first column, c.
To illustrate how structured matrices allow for faster matrix computations, consider
solving the n × n linear system:
An xn = an
for xn = [x1, . . . , xn]⊤, where an = [a1, . . . , an]⊤, and
An :=

1
a1
. . .
an−2
an−1
a1
1
...
an−2
...
...
...
...
...
an−2
...
...
a1
an−1
an−2
· · ·
a1
1

(A.26)
is a real-valued symmetric positive-definite Toeplitz matrix (so that it is invertible). Note
that the entries of An are completely determined by the right-hand side of the linear equa-
tion: vector an. As we shall see shortly in Example A.11, the solution to the more general
linear equation An xn = bn, where bn is arbitrary, can be efficiently computed using the
solution to this specific system An xn = an, obtained via a special recursive algorithm
(Algorithm A.6.3 below).
For every k = 1, . . . , n the k × k Toeplitz matrix Ak satisfies
Ak = Pk Ak Pk,
where Pk is a permutation matrix that “flips” the order of elements — rows when pre-
multiplying and columns when post-multiplying. For example,
"1
2
3
4
5
6
7
8
9
10
#
P5 =
" 5
4
3
2
1
10
9
8
7
6
#
,
where
P5

 =

0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0

.
382
Matrix Decompositions
Clearly, Pk = P⊤
k and PkPk = Ik hold, so that in fact Pk is an orthogonal matrix.
We can solve the n × n linear system Anxn = an in O(n2) time recursively, as follows.
Assume that we have somehow solved for the upper k × k block Ak xk = ak and now we
wish to solve for the (k + 1) × (k + 1) block:
Ak+1 xk+1 = ak+1
⇐⇒
" Ak
Pk ak
a⊤
k Pk
1
# "z
α
#
=
" ak
ak+1
#
.
Therefore,
α = ak+1 −a⊤
k Pk z
Ak z = ak −α Pk ak.
Since A−1
k Pk = Pk A−1
k , the second equation above simplifies to
z = A−1
k ak −α A−1
k Pk ak
= xk −α Pk xk.
Substituting z = xk −α Pk xk into α = ak+1 −a⊤
k Pk z and solving for α yields:
α = ak+1 −a⊤
k Pk xk
1 −a⊤
k xk
.
Finally, with the value of α computed above, we have
xk+1 =
"xk −α Pk xk
α
#
.
This gives the following Levinson–Durbin
Levinson–
Durbin
recursive algorithm for solving An xn = an.
Algorithm A.6.3: Levinson–Durbin Recursion for Solving An xn = an
input: First row [1, a1, . . . , an−1] = [1, a⊤
n−1] of matrix An.
output: Solution xn = A−1
n an.
1 x1 ←a1
2 for k = 1, . . . , n −1 do
3
βk ←1 −a⊤
k xk
4
˘x ←[xk,k, xk,k−1, . . . , xk,1]⊤
5
α ←(ak+1 −a⊤
k ˘x)/βk
6
xk+1 ←
"xk −α ˘x
α
#
7 return xn
In the algorithm above, we have identified xk = [xk,1, xk,2, . . . , xk,k]⊤. The advantage of
the Levinson–Durbin algorithm is that its running cost is O(n2), instead of the usual O(n3).
Using the {xk, βk} computed in Algorithm A.6.3, we construct the following lower tri-
angular matrix recursively, setting L1 = 1 and
Lk+1 =
"
Lk
0k
−(Pkxk)⊤
1
#
,
k = 1, . . . , n −1.
(A.27)
Linear Algebra and Functional Analysis
383
Then, we have the following factorization of An.
Theorem A.14: Diagonalization of Toeplitz Correlation Matrix An
For a real-valued symmetric positive-definite Toeplitz matrix An of the form (A.26),
we have
Ln An L⊤
n = Dn,
where Ln is a the lower diagonal matrix (A.27) and Dn := diag(1, β1, . . . , βn−1) is a
diagonal

 matrix.
Proof: We give a proof by induction. Obviously, L1A1L⊤
1 = 1·1·1 = 1 = D1 is true. Next,
assume that the factorization LkAkL⊤
k = Dk holds for a given k. Observe that
Lk+1Ak+1 =
"
Lk
0k
−(Pkxk)⊤
1
# " Ak
Pk ak
a⊤
k Pk
1
#
=
"
LkAk ,
LkPk ak
−(Pkxk)⊤Ak + a⊤
k Pk ,
−(Pkxk)⊤Pk ak + 1
#
.
It is straightforward to verify that [−(Pkxk)⊤Ak + a⊤
k Pk, −(Pkxk)⊤Pk ak + 1] = [0⊤
k , βk],
yielding the recursion
Lk+1Ak+1 =
"LkAk
LkPk ak
0⊤
k
βk
#
.
Secondly, observe that
Lk+1Ak+1L⊤
k+1 =
"LkAk
LkPk ak
0⊤
k
βk
# "L⊤
k
−Pkxk
0⊤
k
1
#
=
"LkAkL⊤
k ,
−LkAkPkxk + LkPkak
0⊤
k ,
βk
#
.
By noting that AkPkxk = PkPkAkPkxk = PkAkxk = Pkak, we obtain:
Lk+1Ak+1L⊤
k+1 =
"LkAkL⊤
k
0k
0⊤
k
βk
#
.
Hence, the result follows by induction.
□
Example A.11 (Solving Anxn = bn in O(n2) Time) One application of the factoriza-
tion in Theorem A.14 is in the fast solution of a linear system Anxn = bn, where the right-
hand side is an arbitrary vector bn. Since the solution xn can be written as
xn = A−1
n bn = L⊤
n D−1
n Lnbn,
we can compute xn in O(n2) time, as follows.
Algorithm A.6.4: Solving An xn = bn for a General Right-Hand Side
input: First row [1, a⊤
n−1] of matrix An and right-hand side bn.
output: Solution xn = A−1
n bn.
1 Compute Ln in (A.27) and the numbers β1, . . . , βn−1 via Algorithm A.6.3.
2 [x1, . . . , xn]⊤←Lnbn (computed in O(n2) time)
3 xi ←xi/βi−1 for i = 2, . . . , n (computed in O(n) time)
4 [x1, . . . , xn] ←[x1, . . . , xn] Ln (computed in O(n2) time)
5 return xn ←[x1, . . . , xn]⊤
384
Functional Analysis
Note that it is possible to avoid the explicit construction of the lower triangular matrix
in (A.27) via the following modification of Algorithm A.6.3, which only stores an extra
vector y at each recursive step of the Levinson–Durbin algorithm.
Algorithm A.6.5: Solving An xn = bn with O(n) Memory Cost
input: First row [1, a⊤
n−1] of matrix An and right-hand side bn.
output: Solution xn = A−1
n bn.
1 x ←b1
2 y ←a1
3 for k = 1, . . . , n −1 do
4
˘x ←[xk, xk−1, . . . , x1]


5
˘y ←[yk, yk−1, . . . , y1]
6
β ←1 −a⊤
k y
7
αx ←(bk+1 −b⊤
k ˘x)/β
8
αy ←(ak+1 −a⊤
k ˘y)/β
9
x ←[x −αx ˘x, αx]⊤
10
y ←[y −αy˘y, αy]⊤
11 return x
A.7
Functional Analysis
Much of the previous theory on Euclidean vector spaces can be generalized to vector spaces
of functions. Every element of a (real-valued) function space H is a function from some
function space
set X to R, and elements can be added and scalar multiplied as if they were vectors. In
other words, if f ∈H and g ∈H, then α f + βg ∈H for all α, β ∈R. On H we can impose
an inner product as a mapping ⟨·, ·⟩from H × H to R that satisfies
1. ⟨αf1 + βf2, g⟩= α⟨f1, g⟩+ β⟨f2, g⟩;
2. ⟨f, g⟩= ⟨g, f⟩;
3. ⟨f, f⟩⩾0;
4. ⟨f, f⟩= 0 if and only if f = 0 (the zero function).
We focus on real-valued function spaces, although the theory for complex-valued
function spaces is similar (and sometimes easier), under suitable modifications (e.g.,
⟨f, g⟩= ⟨g, f⟩).
Similar to the linear algebra setting in Section A.2, we say that two elements f and g
in H are orthogonal to each other with respect to this inner product if ⟨f, g⟩= 0. Given an
inner product, we can measure distances between elements of the function space H using
the norm
norm
∥f∥:=
p
⟨f, f⟩.
For example, the distance between two functions fm and fn is given by ∥fm −fn∥. The space
H is said to be complete
complete
if every sequence of functions f1, f2, . . . ∈H for which
∥fm −fn∥→0 as m, n →∞,
(A.28)
Linear Algebra and Functional Analysis
385
converges to some f ∈H; that is, ∥f −fn∥→0 as n →∞. A sequence that satisfies (A.28)
is called a Cauchy sequence.
Cauchy
sequence
A complete inner product space is called a Hilbert space. The most fundamental Hilbert
Hilbert space
space of functions is the space L2. An in-depth introduction to L2 requires some measure
theory [6]. For our purposes, it suffices to assume that X ⊆Rd and that on X a measure
measure
µ is
defined which assigns to each suitable4 set A a positive number µ(A) ⩾0 (e.g., its volume).
In many cases of intere

st µ is of the form
µ(A) =
Z
A
w(x) dx
(A.29)
where w ⩾0 is a positive function on X which is called the density
density
of µ with respect to
the Lebesgue measure (the natural volume measure on Rd). We write µ(dx) = w(x) dx to
indicate that µ has density w. Another important case is where
µ(A) =
X
x ∈A∩Zd
w(x),
(A.30)
where w ⩾0 is again called the density of µ, but now with respect to the counting measure
on Zd (which counts the points of Zd). Integrals with respect to measures µ in (A.29) and
(A.30) can now be defined as
Z
f(x) µ(dx) =
Z
f(x) w(x) dx,
and
Z
f(x) µ(dx) =
X
x
f(x) w(x),
respectively. We assume for simplicity that µ has the form (A.29). For measures of the
form (A.30) (so-called discrete measures), replace integrals by sums in what follows.
Definition A.4: L2 Space
Let X be a subset of Rd with measure µ(dx) = w(x) dx. The Hilbert space L2(X, µ)
is the linear space of functions from X to R that satisfy
Z
X
f(x)2 w(x) dx < ∞,
(A.31)
and with inner product
⟨f, g⟩=
Z
X
f(x) g(x) w(x) dx.
(A.32)
Let H be a Hilbert space. A set of functions { fi, i ∈I} is called an orthonormal system
orthonormal
system
if
4Not all sets have a measure. Suitable sets are Borel sets, which can be thought of as countable unions
of rectangles.
386
Functional Analysis
1. the norm of every fi is 1; that is, ⟨fi, fi⟩= 1 for all i ∈I,
2. the { fi} are orthogonal; that is, ⟨fi, f j⟩= 0 for i , j.
It follows then that the { fi} are linearly independent; that is, the only linear combination
P
j α j fj(x) that is equal to fi(x) for all x is the one where αi = 1 and α j = 0 for j , i. An
orthonormal system { fi} is called an orthonormal basis if there is no other f ∈H that is
orthonormal
basis
orthogonal to all the { fi, i ∈I} (other than the zero function). Although the general theory
allows for uncountable bases, in practice5 the set I is taken to be countable.
Example A.12 (Trigonometric Orthonormal Basis) Let H be the Hilbert space
L2((0, 2π), µ), where µ(dx) = w(x) dx and w is the 

constant function w(x) = 1, 0 < x < 2π.
Alternatively, take X = R and w the indicator function on (0, 2π). The trigonometric func-
tions
g0(x) =
1
√
2π
,
gk(x) =
1√π cos(kx),
hk(x) =
1√π sin(kx),
k = 1, 2, . . .
form a countable infinite-dimensional orthonormal basis of H.
A Hilbert space H with an orthonormal basis {f1, f2, . . .} behaves very similarly to the
familiar Euclidean vector space. In particular, every element (i.e., function) f ∈H can be
written as a unique linear combination of the basis vectors:
f =
X
i
⟨f, fi⟩fi,
(A.33)
exactly as in Theorem A.3. The right-hand side of (A.33) is called a (generalized) Fourier
expansion of f. Note that such a Fourier expansion does not require a trigonometric basis;
Fourier
expansion
any orthonormal basis will do.
Example A.13 (Example A.12 (cont.)) Consider the indicator function f(x) = 1{0 <
x < π}. As the trigonometric functions {gk} and {hk} form a basis for L2((0, 2π), 1dx), we
can write
f(x) = a0
1
√
2π
+
∞
X
k=1
ak
1√π cos(kx) +
∞
X
k=1
bk
1√π sin(kx),
(A.34)
where a0 =
R π
0 1/
√
2π dx = √π/2, ak =
R π
0 cos(kx)/√π dx and bk =
R π
0 sin(kx)/√π dx, k =
1, 2, . . .. This means that ak = 0 for all k, bk = 0 for even k, and bk = 2/(k √π) for odd k.
Consequently,
f(x) = 1
2 + 2
π
∞
X
k=1
sin(kx)
k
.
(A.35)
Figure A.7 shows several Fourier approximations obtained by truncating the infinite sum
in (A.35).
5The function spaces typically encountered in machine learning and data science are usually separable
spaces, which allows for the set I to be considered countable; see, e.g., [106].
Linear Algebra and Functional Analysis
387
0
0.5
1
1.5
2
2.5
3
3.5
0
0.5
1
Figure A.7: Fourier approximations of the unit step function f on the interval (0, π), trun-
cating the infinite sum in (A.35) to i = 2, 4, and 14 terms, giving the dotted blue, dashed
red, and solid green curves, respectively.
Starting from any countable basis, we can use the Gram–Schmidt procedure to obtain
☞375
an orthonormal basis, as illustrated in the follo

wing example.
Example A.14 (Legendre Polynomials) Take the function space L2(R, w(x) dx), where
w(x) = 1{−1 < x < 1}. We wish to construct an orthonormal basis of polynomial functions
g0, g1, g2, . . ., starting from the collection of monomials: ι0, ι1, ι2, . . ., where ιk : x 7→xk. Us-
ing Gram–Schmidt, the first normalized zero-degree polynomial is g0 = ι0/∥ι0∥= √1/2.
To find g1 (a polynomial of degree 1), project ι1 (the identity function) onto the space
spanned by g0. The resulting projection is p1 := ⟨g0, ι1⟩g0, written out as
p1(x) =
 Z 1
−1
x g0(x) dx
!
g0(x) = 1
2
Z 1
−1
x dx = 0.
Hence, g1 = (ι1 −p1)/∥ι1 −p1∥is a linear function; that is, of the form g1(x) = ax. The
constant a is found by normalization:
1 = ∥g1∥2 =
Z 1
−1
g2
1(x) dx = a2
Z 1
−1
x2 dx = a22
3,
so that g1(x) =
√3/2x. Continuing the Gram–Schmidt procedure, we find g2(x) =
√5/8(3x2 −1), g3(x) = √7/8(5x3 −3x) and, in general,
gk(x) =
√
2k + 1
2k+ 1
2k!
dk
dxk (x2 −1)k,
k = 0, 1, 2, . . . .
These are the (normalized) Legendre polynomials. The graphs of g0, g1, g2, and g3 are given
Legendre
polynomials
in Figure A.8.
388
Functional Analysis
-1
-0.5
0
0.5
1
-1
1
Figure A.8: The first 4 normalized Legendre polynomials.
As the Legendre polynomials form an orthonormal basis of L2(R, 1{−1 < x < 1} dx),
they can be used to approximate arbitrary functions in this space. For example, Figure A.9
shows an approximation using the first 51 Legendre polynomials (k = 0, 1, . . . , 50) of the
Fourier expansion of the indicator function on the interval (−1/2, 1/2). These Legendre
polynomials form the basis of a 51-dimensional linear subspace onto which the indicator
function is orthogonally projected.
-1
-0.5
0
0.5
1
0
0.2
0.4
0.6
0.8
1
Figure A.9: Approximation of the indicator function on the interval (−1/2, 1/2), using the
Legendre polynomials g0, g1, . . . , g50.
The Legendre polynomials were produced in the following way: We started with an
unnormalized probability density on R — in this case the probability

 density of the uniform
☞424
distribution on (−1, 1). We then constructed a sequence of polynomials by applying the
Gram–Schmidt procedure to the monomials 1, x, x2, . . ..
By using exactly the same procedure, but with a different probability density, we can
produce other such orthogonal polynomials. For example, the density of the standard expo-
orthogonal
polynomials
nential6 distribution, w(x) = e−x, x ⩾0, gives the Laguerre polynomials, which are defined
Laguerre
polynomials
6This can be further generalized to the density of a gamma distribution.
Linear Algebra and Functional Analysis
389
by the recurrence
(n + 1)gn+1(x) = (2n + 1 −x)gn(x) −ngn−1(x),
n = 1, 2, . . . ,
with g0(x) = 1 and g1(x) = 1 −x, for x ⩾0. The Hermite polynomials
Hermite
polynomials
are obtained when
using instead the density of the standard normal distribution: w(x) = e−x2/2/
√
2π, x ∈R.
These polynomials satisfy the recursion
gn+1(x) = xgn(x) −dgn(x)
dx
,
n = 0, 1, . . . ,
with g0(x) = 1, x ∈R. Note that the Hermite polynomials as defined above have not been
normalized to have norm 1. To normalize, use the fact that ∥gn∥2 = n!.
We conclude with a number of key results in functional analysis. The first one is the
celebrated Cauchy–Schwarz
Cauchy–
Schwarz
inequality.
Theorem A.15: Cauchy–Schwarz
Let H be a Hilbert space. For every f, g ∈H it holds that
|⟨f, g⟩| ⩽∥f∥∥g∥.
Proof: The inequality is trivially true for g = 0 (zero function). For g , 0, we can write
f = αg + h, where h ⊥g and α = ⟨f, g⟩/∥g∥2. Consequently, ∥f∥2 = |α|2 ∥g∥2 + ∥h∥2 ⩾
|α|2 ∥g∥2. The result follows after rearranging this last inequality.
□
Let V and W be two linear vector spaces (for example, Hilbert spaces) on which norms
∥· ∥V and ∥· ∥W are defined. Suppose A : V →W is a mapping from V to W. When
W = V, such a mapping is often called an operator; when W = R it is called a functional.
operator
functional
Mapping A is said to be linear if A(α f + βg) = αA(f) + βA(g). In this case we write Af
linear mapping
instead of A

(f). If there exists γ < ∞such that
∥Af∥W ⩽γ ∥f∥V,
f ∈V,
(A.36)
then A is said to be a bounded mapping. The smallest γ for which (A.36) holds is called the
bounded
mapping
norm of A; denoted by ∥A∥. A (not necessarily linear) mapping A : V →W is said to be
norm
continuous at f if for any sequence f1, f2, . . . converging to f the sequence A( f1), A( f2), . . .
continuous
mapping
converges to A(f). That is, if
∀ε > 0, ∃δ > 0 : ∀g ∈V, ∥f −g∥V < δ ⇒∥A(f) −A(g)∥W < ε.
(A.37)
If the above property holds for every f ∈V, then the mapping A itself is called continuous.
Theorem A.16: Continuity and Boundedness for Linear Mappings
For a linear mapping, continuity and boundedness are equivalent.
Proof: Let A be linear and bounded. We may assume that A is non-zero (otherwise the
statement holds trivially), and that therefore 0 < ∥A∥< ∞. Taking δ < ε/∥A∥in (A.37) now
ensures that ∥Af −Ag∥W ⩽∥A∥∥f −g∥V < ∥A∥δ < ε. This shows that A is continuous.
390
Fourier Transforms
Conversely, suppose A is continuous. In particular, it is continuous at f = 0 (the zero-
element of V). Thus, take f = 0 and let ε and δ be as in (A.37). For any g , 0, let h =
δ/(2∥g∥V) g. As ∥h∥V = δ/2 < δ, it follows from (A.37) that
∥Ah∥W =
δ
2∥g∥V
∥Ag∥W < ε.
Rearranging the last inequality gives ∥Ag∥W < 2ε/δ∥g∥V, showing that A is bounded.
□
Theorem A.17: Riesz Representation Theorem
Any bounded linear functional ϕ on a Hilbert space H can be represented as ϕ(h) =
⟨h, g⟩, for some g ∈H (depending on ϕ).
Proof: Let P be the projection of H onto the nullspace N of ϕ; that is, N = {g ∈H :
ϕ(g) = 0}. If ϕ is not the 0-functional, then there exists a g0 , 0 with ϕ(g0) , 0. Let
g1 = g0 −Pg0. Then g1 ⊥N and ϕ(g1) = ϕ(g0). Take g2 = g1/ϕ(g1). For any h ∈H,
f := h −ϕ(h)g2 lies in N. As g2 ⊥N it holds that ⟨f, g2⟩= 0, which is equivalent to
⟨h, g2⟩= ϕ(h) ∥g2∥2. By defining g = g2/∥g2∥2 we have found our representation.
□
A.8
Fourier Transforms
We will now briefly introduce the Fourier transform. Before doing so, we will

 extend the
concept of L2 space of real-valued functions as follows.
☞385
Definition A.5: Lp Space
Let X be a subset of Rd with measure µ(dx) = w(x) dx and p ∈[1, ∞). Then Lp(X, µ)
is the linear space of functions from X to C that satisfy
Z
X
| f(x)|p w(x) dx < ∞.
(A.38)
When p = 2, L2(X, µ) is in fact a Hilbert space equipped with inner product
⟨f, g⟩=
Z
X
f(x) g(x) w(x) dx.
(A.39)
We are now in a position to define the Fourier transform (with respect to the Lebesgue
measure). Note that in the following Definitions A.6 and A.7 we have chosen a particular
convention. Equivalent (but not identical) definitions exist that include scaling constants
(2π)d or (2π)−d and where −2πt is replaced with 2πt, t, or −t.
Linear Algebra and Functional Analysis
391
Definition A.6: (Multivariate) Fourier Transform
The Fourier transform
Fourier
transform
F [f] of a (real- or complex-valued) function f ∈L1(Rd) is
the function ef defined as
ef(t) :=
Z
Rd e−i 2π t⊤x f(x) dx ,
t ∈Rd.
The Fourier transform ef is continuous, uniformly bounded (since f ∈L1(Rd) im-
plies that |ef(t)| ⩽
R
Rd | f(x)| dx < ∞), and satisfies lim∥t∥→∞ef(t) = 0 (a result known as
the Riemann–Lebesgue lemma). However, |ef| does not necessarily have a finite integ-
ral. A simple example in R1 is the Fourier transform of f(x) = 1{−1/2 < x < 1/2}. Then
ef(t) = sin(πt)/(πt) = sinc(πt), which is not absolutely integrable.
Definition A.7: (Multivariate) Inverse Fourier Transform
The inverse Fourier transform
inverse Fourier
transform
F −1[ef] of a (real- or complex-valued) function
ef ∈L1(Rd) is the function ˘f defined as
˘f(x) :=
Z
Rd ei 2π t⊤x ef(t) dt,
x ∈Rd.
As one would hope, it holds that if f and F [ f] are both in L1(Rd), then f = F −1[F [f]]
almost everywhere.
The Fourier transform enjoys many interesting and useful properties, some of which
we list below.
1. Linearity: For f, g ∈L1(Rd) and constants a, b ∈R,
F [af + bg] = a F [ f] + b F [g].
2. Space Shifting and Scaling: Let A ∈Rd×d be an invertible matrix and

 b ∈Rd a con-
stant vector. Let f ∈L1(Rd) and define h(x) := f(Ax + b). Then
F [h](t) = ei 2π (A−⊤t)⊤b ef(A−⊤t)/| det(A)|,
where A−⊤:= (A⊤)−1 = (A−1)⊤.
3. Frequency Shifting and Scaling: Let A ∈Rd×d be an invertible matrix and b ∈Rd a
constant vector. Let f ∈L1(Rd) and define
h(x) := e−i 2π b⊤A−⊤x f(A−⊤x)/| det(A)|.
Then F [h](t) = ef(At + b).
4. Differentiation: Let f ∈L1(Rd) ∩C1(Rd) and let fk := ∂f/∂xk be the partial derivat-
ive of f with respect to xk. If fk ∈L1(Rd) for k = 1, . . . , d, then
F [ fk](t) = (i 2π tk) ef(t).
392
Fourier Transforms
5. Convolution: Let f, g ∈L1(Rd) be real or complex valued functions. Their convolu-
tion, f ∗g, is defined as
(f ∗g)(x) =
Z
Rd f(y) g(x −y) dy,
and is also in L1(Rd). Moreover, the Fourier transform satisfies
F [ f ∗g] = F [f] F [g].
6. Duality: Let f and F [f] both be in L1(Rd). Then F [F [f]](t) = f(−t).
7. Product Formula: Let f, g ∈L1(Rd) and denote by ef, eg their respective Fourier trans-
forms. Then ef g, f eg ∈L1(Rd), and
Z
Rd
ef(z) g(z) dz =
Z
Rd f(z)eg(z) dz.
There are many additional properties which hold if f ∈L1(Rd) ∩L2(Rd). In particular,
if f, g ∈L1(Rd) ∩L2(Rd), then ef,eg ∈L2(Rd) and ⟨ef, eg⟩= ⟨f, g⟩, a result often known as
Parseval’s formula. Putting g = f gives the result often referred to as Plancherel’s theorem.
The Fourier transform can be extended in several ways, in the first instance to functions
in L2(Rd) by continuity. A substantial extension of the theory is realized by replacing integ-
ration with respect to the Lebesgue measure (i.e.,
R
Rd · · · dx) with integration with respect
to a (finite Borel) measure µ (i.e.,
R
Rd · · · µ(dx)). Moreover, there is a close connection
between the Fourier transform and characteristic functions arising in probability theory.
Indeed, if X is a random vector with pdf f, then its characteristic function ψ satisfies
☞441
ψ(t) := E ei t⊤X = F [f](−t/(2π)).
A.8.1
Discrete Fourier Transform
Here, we introduce the (univariate) discrete Fourier transform, which can

 be viewed as a
special case of the Fourier transform introduced in Definition A.6, where d = 1, integration
is with respect to the counting measure, and f(x) = 0 for x < 0 and x > (n −1).
Definition A.8: Discrete Fourier Transform
The discrete Fourier transform
discrete
Fourier
transform
(DFT) of a vector x = [x0, . . . , xn−1]⊤∈Cn is the
vector ex = [ex0, . . . ,exn−1]⊤whose elements are given by
ext =
n−1
X
s=0
ωst xs,
t = 0, . . . , n −1,
(A.40)
where ω = exp(−i 2π/n).
In other words, ex is obtained from x via the linear transformation
ex = Fx,
Linear Algebra and Functional Analysis
393
where
F =

1
1
1
. . .
1
1
ω
ω2
. . .
ωn−1
1
ω2
ω4
. . .
ω2(n−1)
...
...
...
...
...
1
ωn−1
ω2(n−1)
. . .
ω(n−1)2

.
The matrix F is a so-called Vandermonde matrix, and is clearly symmetric (i.e., F = F⊤).
Moreover, F/√n is in fact a unitary matrix and hence its inverse is simply its complex
conjugate F/√n. Thus, F−1 = F/n and we have that the inverse discrete Fourier transform
inverse discrete
Fourier
transform
(IDFT) is given by
xt = 1
n
n−1
X
s=0
ω−st exs,
t = 0, . . . , n −1,
(A.41)
or in terms of matrices and vectors,
x = Fex/n.
Observe that the IDFT of a vector y is related to the DFT of its complex conjugate y, since
F y/n = F y/n.
Consequently, an IDFT can be computed via a DFT.
There is a close connection between circulant matrices C and the DFT. To make this
connection concrete, let C be the circulant matrix corresponding to the vector c ∈Cn and
denote by f t the t-th column of the discrete Fourier matrix F, t = 0, 1, . . . , n −1. Then, the
s-th element of C f t is
n−1
X
k=0
c(s−k) mod n ωtk =
n−1
X
y=0
cy ωt(s−y) =
ωts
|{z}
s-th element of f t
n−1
X
y=0
cy ω−ty
|      {z      }
λs
.
Hence, the eigenvalues of C are
λt = c⊤f t,
t = 0, 1, . . . , n −1,
with corresponding eigenvectors f t. Collecting the eigenvalues into the vector λ =
[λ0, . . . , λn−1]⊤= Fc, we therefore have the eigen-decomposition
C = F diag(λ) F/n.
Conseque

ntly, one can compute the circular convolution of a vector a = [a1, . . . , an]⊤
and c = [c0, . . . , cn−1]⊤by a series of DFTs as follows. Construct the circulant matrix C
corresponding to c. Then, the circular convolution of a and c is given by y = Ca. Proceed
in four steps:
1. Compute z = Fa/n.
2. Compute λ = Fc.
394
Fourier Transforms
3. Compute p = z ⊙λ = [z1 λ0, . . . , zn λn−1]⊤.
4. Compute y = Fp.
Steps 1 and 2 are (up to constants) in the form of an IDFT, and step 4 is in the form of a
DFT. These are computable via the FFT (Section A.8.2) in O(n ln n) time. Step 3 is a dot
☞394
product computable in O(n) time. Thus, the circular convolution can be computed with the
aid of the FFT in O(n ln n) time.
One can also efficiently compute the product of an n × n Toeplitz matrix T and an n × 1
vector a in O(n ln n) time by embedding T into a circulant matrix C of size 2n×2n. Namely,
define
C =
"T
B
B
T
#
,
where
B =

0
tn−1
· · ·
t2
t1
t−(n−1)
0
tn−1
t2
...
t−(n−1)
0
...
...
t−2
...
...
tn−1
t−1
t−2
· · ·
t−(n−1)
0

.
Then a product of the form y = Ta can be computed in O(n ln n) time, since we may write
C
"a
0
#
=
"T
B
B
T
# "a
0
#
=
"Ta
Ba
#
.
The left-hand side is a product of a 2n × 2n circulant matrix with vector of length 2n, and
so can be computed in O(n ln n) time via the FFT, as previously discussed.
Conceptually, one can also solve equations of the form Cx = b for a given vector b ∈Cn
and circulant matrix C (corresponding to c ∈Cn, assuming all its eigenvalues are non-zero)
via the following four steps:
1. Compute z = Fb/n.
2. Compute λ = Fc.
3. Compute p = z/λ = [z1/λ0, . . . , zn/λn−1]⊤.
4. Compute x = Fp.
Once again, Steps 1 and 2 are (up to constants) in the form of an IDFT, and Step 4 is in
the form of a DFT, all of which are computable via the FFT in O(n ln n) time, and Step 3
is computable in O(n) time, meaning the solution x can be computed using the FFT in
O(n ln n) time.
A.8.2
Fast Fourier Transform


The fast Fourier transform
fast Fourier
transform
(FFT) is a numerical algorithm for the fast evaluation of (A.40)
and (A.41). By using a divide-and-conquer strategy, the algorithm reduces the compu-
tational complexity from O(n2) (for the naïve evaluation of the linear transformation) to
O(n ln n) [60].
Linear Algebra and Functional Analysis
395
The essence of the algorithm lies in the following observation. Suppose n = r1r2. Then
one can express any index t appearing in (A.40) via a pair (t0, t1), with t = t1r1 + t0,
where t0 ∈{0, 1, . . . , r1 −1} and t1 ∈{0, 1, . . . , r2 −1}. Similarly, one can express any index
s appearing in (A.40) via a pair (s0, s1), with s = s1r2 + s0, where s0 ∈{0, 1, . . . , r2 −1} and
s1 ∈{0, 1, . . . , r1 −1}.
Identifying ext ≡ext1,t0 and xs ≡xs1,s0, we may re-express (A.40) as
ext1,t0 =
r2−1
X
s0=0
ωs0t
r1−1
X
s1=0
ωs1r2t xs1,s0 ,
t0 = 0, 1, . . . , r1 −1 , t1 = 0, 1, . . . , r2 −1.
(A.42)
Observe that ωs1r2t = ωs1r2t0 (because ωr1r2 = 1), so that the inner sum over s1 depends only
on s0 and t0. Define
yt0,s0 :=
r1−1
X
s1=0
ωs1r2t0 xs1,s0 ,
t0 = 0, 1, . . . , r1 −1 , s0 = 0, 1, . . . , r2 −1.
Computing each yt0,s0 requires O(n r1) operations. In terms of the {yt0,s0}, (A.42) can be
written as
ext1,t0 =
r2−1
X
s0=0
ωs0t yt0,s0 ,
t1 = 0, 1, . . . , r2 −1 , t0 = 0, 1, . . . , r1 −1,
requiring O(n r2) operations to compute. Thus, calculating the DFT using this two-step
procedure requires O(n (r1 + r2)) operations, rather than O(n2).
Now supposing n = r1r2 · · · rm, repeated application the above divide-and-conquer idea
yields an m-step procedure requiring O(n (r1 + r2 + · · · + rm)) operations. In particular, if
rk = r for all k = 1, 2, . . . , m, we have that n = rm and m = logr n, so that the total number
of operations is O(r n m) ≡O(r n logr(n)). Typically, the radix r is a small (not necessarily
prime) number, for instance r = 2.
Further Reading
A good reference book on matrix computations is Golub and Van Loan [52]. A useful list
of

 many common vector and matrix calculus identities can be found in [95]. Strang’s in-
troduction to linear algebra [116] is a classic textbook, and his recent book [117] combines
linear algebra with the foundations of deep learning. Fast reliable algorithms for matrices
with structure can be found in [64]. Kolmogorov and Fomin’s masterpiece on the theory
of functions and functional analysis [67] still provides one of the best introductions to the
topic. A popular choice for an advanced course in functional analysis is Rudin [106].
396
APPENDIXB
MULTIVARIATE DIFFERENTIATION AND
OPTIMIZATION
The purpose of this appendix is to review various aspects of multivariate differen-
tiation and optimization. We assume the reader is familiar with differentiating a real-
valued function.
B.1
Multivariate Differentiation
For a multivariate function f that maps a vector x = [x1, . . . , xn]⊤to a real number f(x),
the partial derivative with respect to xi, denoted ∂f
∂xi, is the derivative taken with respect
partial
derivative
to xi while all other variables are held constant. We can write all the n partial derivatives
neatly using the “scalar/vector” derivative notation:
scalar/vector:
∂f
∂x :=

∂f
∂x1...
∂f
∂xn

.
(B.1)
This vector of partial derivatives is known as the gradient of f at x and is sometimes written
gradient
as ∇f(x).
Next, suppose that f is a multivalued (vector-valued) function taking values in Rm,
defined by
x =

x1
x2
...
xn

7→

f1(x)
f2(x)
...
fm(x)

=: f(x).
We can compute each of the partial derivatives ∂fi/∂x j and organize them neatly in a “vec-
tor/vector” derivative notation:
vector/vector:
∂f
∂x :=

∂f1
∂x1
∂f2
∂x1
· · ·
∂fm
∂x1
∂f1
∂x2
∂f2
∂x2
· · ·
∂fm
∂x2
...
...
· · ·
...
∂f1
∂xn
∂f2
∂xn
· · ·
∂fm
∂xn

.
(B.2)
397
398
Multivariate Differentiation
The transpose of this matrix is known as the matrix of Jacobi
matrix of Ja

cobi
of f at x (sometimes
called the Fréchet derivative of f at x); that is,
Jf(x) :=
"∂f
∂x
#⊤
=

∂f1
∂x1
∂f1
∂x2
· · ·
∂f1
∂xn
∂f2
∂x1
∂f2
∂x2
· · ·
∂f2
∂xn
...
...
· · ·
...
∂fm
∂x1
∂fm
∂x2
· · ·
∂fm
∂xn

.
(B.3)
If we define g(x) := ∇f(x) and take the “vector/vector” derivative of g with respect to
x, we obtain the matrix of second-order partial derivatives of f:
H f(x) := ∂g
∂x =

∂2 f
∂2x1
∂2 f
∂x1∂x2
· · ·
∂2 f
∂x1∂xm
∂2 f
∂x2∂x1
∂2 f
∂2x2
· · ·
∂2 f
∂x2∂xm
...
...
· · ·
...
∂2 f
∂xm∂x1
∂2 f
∂xm∂x2
· · ·
∂2 f
∂2xm

,
(B.4)
which is known as the Hessian matrix
Hessian matrix
of f at x, also denoted as ∇2 f(x). If these second-
order partial derivatives are continuous in a region around x, then
∂f
∂xi∂x j =
∂f
∂xj∂xi and, hence,
the Hessian matrix Hf(x) is symmetric.
Finally, note that we can also define a “scalar/matrix” derivative of y with respect to
X ∈Rm×n with (i, j)-th entry xi j:
∂y
∂X :=

∂y
∂x11
∂y
∂x12
· · ·
∂y
∂x1n
∂y
∂x21
∂y
∂x22
· · ·
∂y
∂x2n
...
...
· · ·
...
∂y
∂xm1
∂y
∂xm2
· · ·
∂y
∂xmn

and a “matrix/scalar” derivative:
∂X
∂y :=

∂x11
∂y
∂x12
∂y
· · ·
∂x1n
∂y
∂x21
∂y
∂x22
∂y
· · ·
∂x2n
∂y
...
...
· · ·
...
∂xm1
∂y
∂xm2
∂y
· · ·
∂xmn
∂y

.
Example B.1 (Scalar/Matrix Derivative) Let y = a⊤Xb, where X ∈Rm×n, a ∈Rm,
and b ∈Rn. Since y is a scalar, we can write y = tr(y) = tr(Xba⊤), using the cyclic property
of the trace (see Theorem A.1). Defining C := ba⊤, we have
☞357
y =
m
X
i=1
[XC]ii =
m
X
i=1
n
X
j=1
xi jc ji,
so that ∂y/∂xij = c ji or, in matrix form,
∂y
∂X = C⊤= ab⊤.
Multivariate Differentiation and Optimization
399
Example B.2 (Scalar/Matrix Derivative via the Woodbury Identity) Let y = tr

X−1A

,
where X, A ∈Rn×n. We now prove that
∂y
∂X = −X−⊤A⊤X−⊤.
To show this, apply the Woodbury matrix identity to an infinitesimal perturbation, X + εU,
of X, and take ε ↓0 to obtain the followin

g:
☞371
(X + ε U)−1 −X−1
ε
= −X−1 U (I + ε X−1 U)−1 X−1 −→−X−1 U X−1.
Therefore, as ε ↓0
tr

(X + ε U)−1A

−tr

X−1A

ε
−→−tr

X−1U X−1A

= −tr

U X−1AX−1
.
Now, suppose that U is an all zero matrix with a one in the (i, j)-th position. We can write,
∂y
∂xij
= lim
ε↓0
tr

(X + ε U)−1A

−tr

X−1A

ε
= −tr

UX−1AX−1
= −
h
X−1AX−1i
ji .
Therefore, ∂y
∂X = −

X−1AX−1⊤.
The following two examples specify multivariate derivatives for the important special
cases of linear and quadratic functions.
Example B.3 (Gradient of a Linear Function) Let f(x) = Ax for some m×n constant
matrix A. Then, its vector/vector derivative (B.2) is the matrix
∂f
∂x = A⊤.
(B.5)
To see this, let aij denote the (i, j)-th element of A, so that
f(x) = Ax =

Pn
k=1 a1kxk
...
Pn
k=1 amkxk
.
To find the (j, i)-th element of ∂f
∂x, we differentiate the i-th element of f with respect to x j:
∂fi
∂xj
= ∂
∂x j
n
X
k=1
aikxk = ai j.
In other words, the (i, j)-th element of ∂f
∂x is a ji, the (i, j)-th element of A⊤.
Example B.4 (Gradient and Hessian of a Quadratic Function) Let f(x) = x⊤Ax for
some n × n constant matrix A. Then,
∇f(x) = (A + A⊤)x.
(B.6)
400
Multivariate Differentiation
It follows immediately that if A is symmetric, that is, A = A⊤, then ∇(x⊤Ax) = 2Ax and
∇2 (x⊤Ax) = 2A.
To prove (B.6), first observe that f(x) = x⊤Ax = Pn
i=1
Pn
j=1 ai j xix j, which is a quadratic
form in x, is real-valued, with
∂f
∂xk
= ∂
∂xk
n
X
i=1
n
X
j=1
ai jxix j =
n
X
j=1
ak jx j +
n
X
i=1
aikxi.
The first term on the right-hand side is equal to the k-th element of Ax, whereas the second
term equals the k-th element of x⊤A, or equivalently the k-th element of A⊤x.
B.1.1
Taylor Expansion
The matrix of Jacobi and the Hessian matrix feature prominently in multidimensional
Taylor expansions.
Theorem B.1: Multidimensional Taylor Expansions
Let X be an open subset of Rn and let a ∈X. If f : X →R is a continuously twice
differentiable function with Jacobian matrix Jf(x) and Hessian mat

rix H f(x), then
for every x ∈X we have the following first- and second-order Taylor expansions:
f(x) = f(a) + Jf(a) (x −a) + O(∥x −a∥2)
(B.7)
and
f(x) = f(a) + J f(a) (x −a) + 1
2(x −a)⊤H f(a) (x −a) + O(∥x −a∥3)
(B.8)
as ∥x −a∥→0. By dropping the O remainder terms, one obtains the corresponding
Taylor approximations.
The result is essentially saying that a smooth enough function behaves locally (in the
neighborhood of a point x) like a linear and quadratic function. Thus, the gradient or Hes-
sian of an approximating linear or quadratic function is a basic building block of many
approximation and optimization algorithms.
Remark B.1 (Version Without Remainder Terms) An alternative version of Taylor’s
theorem states that there exists an a′ that lies on the line segment between x and a such
that (B.7) and (B.8) hold without remainder terms, with Jf(a) in (B.7) replaced by J f(a′)
and H f(a) in (B.8) replaced by Hf(a′).
B.1.2
Chain Rule
Consider the functions f : Rk →Rm and g : Rm →Rn. The function x 7→g(f(x)) is called
the composition
composition
of g and f, written as g ◦f, and is a function from Rk to Rn. Suppose
y = f(x) and z = g(y), as in Figure B.1. Let Jf(x) and Jg(y) be the (Fréchet) derivatives
of f (at x) and g (at y), respectively. We may think of J f(x) as the matrix that describes
Multivariate Differentiation and Optimization
401
how, in a neighborhood of x, the function f can be approximated by a linear function:
f(x + h) ≈f(x) + Jf(x)h, and similarly for Jg(y). The well-known chain rule
chain rule
of calculus
simply states that the derivative of the composition g ◦f is the matrix product of the
derivatives of g and f; that is,
Jg◦f(x) = Jg(y) Jf(x).
g ◦f
x
y
z
f
g
Rn
Rm
Rk
Figure B.1: Function composition. The blue arrows symbolize the linear mappings.
In terms of our vector/vector derivative notation, we have
"∂z
∂x
#⊤
=
"∂z
∂y
#⊤"∂y
∂x
#⊤
or, more simply,
∂z
∂x = ∂y
∂x
∂z
∂y.
(B.9)
In a similar way we can establish a scalar/matrix chain rule. In part

icular, suppose X is
an n × p matrix, which is mapped to y := Xα for a fixed p-dimensional vector α. In turn, y
is mapped to a scalar z := g(y) for some function g. Denote the columns of X by x1, . . . , xp.
Then,
y = Xα =
p
X
j=1
α jxj,
and, therefore, ∂y/∂xj = α jIn. It follows by the chain rule (B.9) that
∂z
∂xi
= ∂y
∂xi
∂z
∂y = αiIn
∂z
∂y = αi
∂z
∂y.
Therefore,
∂z
∂X =
h ∂z
∂x1, . . . , ∂z
∂xp
i
=
h
α1
∂z
∂y, . . . , αp
∂z
∂y
i
= ∂z
∂y α⊤.
(B.10)
Example B.5 (Derivative of the Log-Determinant) Suppose we are given a positive
definite matrix A ∈Rp×p and wish to compute the scalar/matrix derivative ∂ln |A|
∂A . The result
is
∂ln |A|
∂A
= A−1.
To see this, we can reason as follows. By Theorem A.8, we can write A = Q D Q⊤, where
☞366
402
Optimization Theory
Q is an orthogonal matrix and D = diag(λ1, . . . , λp) is the diagonal matrix of eigenvalues of
A. The eigenvalues are strictly positive, since A is positive definite. Denoting the columns
of Q by (qi), we have
λi = q⊤
i Aqi = tr  qiAq⊤
i
 ,
i = 1, . . . , p.
(B.11)
From the properties of determinants, we have y := ln |A| = ln |Q D Q⊤| = ln(|Q| |D| |Q⊤|) =
ln |D| = Pp
i=1 ln λi. We can thus write
∂ln |A|
∂A
=
p
X
i=1
∂ln λi
∂A
=
p
X
i=1
∂λi
∂A
∂ln λi
∂λi
=
p
X
i=1
∂λi
∂A
1
λi
,
where the second equation follows from the chain rule applied to the function composition
A 7→λi 7→y. From (B.11) and Example B.1 we have ∂λi/∂A = qiq⊤
i . It follows that
∂y
∂A =
p
X
i=1
qiq⊤
i
1
λi
= Q D−1 Q⊤= A−1.
B.2
Optimization Theory
Optimization is concerned with finding minimal or maximal solutions of a real-valued
objective function
objective
function
f in some set X:
min
x∈X f(x)
or
max
x∈X f(x).
(B.12)
Since any maximization problem can easily be converted into a minimization problem via
the equivalence maxx f(x) ≡−minx −f(x), we focus only on minimization problems. We
use the following terminology. A local minimizer
local minimizer
of f(x) is an element x∗∈X such that
f(x∗) ⩽f(x) for all x in some neighborhood of x∗. If f(x∗)

 ⩽f(x) for all x ∈X, then x∗is
called a global minimizer
global
minimizer
or global solution. The set of global minimizers is denoted by
argmin
x∈X
f(x).
The function value f(x∗) corresponding to a local/global minimizer x∗is referred to as the
local/global minimum
local/global
minimum
of f(x).
Optimization problems may be classified by the set X and the objective function f.
If X is countable, the optimization problem is called discrete or combinatorial. If instead
X is a nondenumerable set such as Rn and f takes values in a nondenumerable set, then
the problem is said to be continuous. Optimization problems that are neither discrete nor
continuous are said to be mixed.
The search set X is often defined by means of constraints. A standard setting for con-
strained optimization (minimization) is the following:
min
x∈Y f(x)
subject to: hi(x) = 0,
i = 1, . . . , m,
gi(x) ⩽0,
i = 1, . . . , k.
(B.13)
Multivariate Differentiation and Optimization
403
Here, f is the objective function, and {gi} and {hi} are given functions so that hi(x) = 0
and gi(x) ⩽0 represent the equality and inequality constraints, respectively. The region
X ⊆Y where the objective function is defined and where all the constraints are satisfied
is called the feasible region
feasible region
. An optimization problem without constraints is said to be an
unconstrained problem.
For an unconstrained continuous optimization problem, the search space X is often
taken to be (a subset of) Rn, and f is assumed to be a Ck function for sufficiently high
k (typically k = 2 or 3 suffices); that is, its k-th order derivative is continuous. For a C1
function the standard approach to minimizing f(x) is to solve the equation
∇f(x) = 0,
(B.14)
where ∇f(x) is the gradient of f at x. The solutions x∗to (B.14) are called station-
☞397
ary points
stationary
points
. Stationary points can be local/global minimizers, local/global maximizers, or
saddle points (which are neither). If, in addition, the function is C2, the condi

tion
saddle points
y⊤(∇2 f(x∗)) y > 0
for all y , 0
(B.15)
ensures that the stationary point x∗is a local minimizer of f. The condition (B.15) states
that the Hessian matrix of f at x∗is positive definite. Recall that we write H ≻0 to indicate
☞398
that a matrix H is positive definite.
In Figure B.2 we have a multiextremal objective function on X = R. There are four
stationary points: two are local minimizers, one is a local maximizer, and one is neither a
minimizer nor a maximizer, but a saddle point.
x
Local minimum
Local maximum
Global minimum
Saddle point
f(x)
Figure B.2: A multiextremal objective function in one dimension.
B.2.1
Convexity and Optimization
An important class of optimization problems is related to the notion of convexity. A set X
is said to be convex if for all x1, x2 ∈X it holds that α x1 + (1 −α) x2 ∈X for all 0 ⩽α ⩽1.
In addition, the objective function f is a convex function
convex
function
provided that for each x in the
interior of X there exists a vector v such that
f(y) ⩾f(x) + (y −x)⊤v,
y ∈X.
(B.16)
404
Optimization Theory
The vector v in (B.16) may not be unique and is referred to as a subgradient of f.
subgradient
One of the crucial properties of a convex function f is that Jensen’s inequality holds
(see Exercise 14 in Chapter 2):
☞63
Ef(X) ⩾f(EX),
for any random vector X.
Example B.6 (Convexity and Directional Derivative) The directional derivative
directional
derivative
of a
multivariate function f at x in the direction d is defined as the right derivative of g(t) :=
f(x + t d) at t = 0:
lim
t↓0
f(x + t d) −f(x)
t
= lim
t↑∞t ( f(x + d/t) −f(x)).
This right derivative may not always exist. However, if f is a convex function, then the
directional derivative of f at x in the interior of its domain always exists (in any direction
d).
To see this, let t1 ⩾t2 > 0. By Jensen’s inequality we have for any x and y in the interior
of the domain:
t2
t1
f(y) +
 
1 −t2
t1
!
f(x) ⩾f
 t2
t1
y +
 
1 −t2
t1
!
x
!
.
Making the substitution y = x + t1d 

and rearranging the last equation yields:
f(x + t1 d) −f(x)
t1
⩾f(x + t2 d) −f(x)
t2
.
In other words, the function t 7→(f(x + t d) −f(x))/t is increasing for t > 0 and therefore
the directional derivative satisfies:
lim
t↓0
f(x + t d) −f(x)
t
= inf
t>0
f(x + t d) −f(x)
t
.
Hence, to show existence it is enough to show that ( f(x + t d) −f(x))/t is bounded from
below.
Since x lies in the interior of the domain of f, we can choose t small enough so that
x + t d also lies in the interior. Therefore, the convexity of f implies that there exists a
subgradient vector v such that f(x + t d) ⩾f(x) + v⊤(t d). In other words,
f(x + t d) −f(x)
t
⩾v⊤d
provides a lower bound for all t > 0, and the directional derivative of f at an interior x
always exists (in any direction).
A function f satisfying (B.16) with strict inequality is said to be strictly convex. It is
said to be a (strictly) concave function
concave
function
if −f is (strictly) convex. Assuming that X is an
open set, convexity for f ∈C1 is equivalent to
f(y) ⩾f(x) + (y −x)⊤∇f(x)
for all x, y ∈X.
Moreover, for f ∈C2 strict convexity is equivalent to the Hessian matrix being positive
definite for all x ∈X, and convexity is equivalent to the Hessian matrix being positive
semidefinite for all x; that is, y⊤
∇2 f(x)

y ⩾0 for all y and x. Recall that we write H ⪰0
to indicate that a matrix H is positive semidefinite.
☞367
Multivariate Differentiation and Optimization
405
Example B.7 (Convexity and Differentiability) If f is a continuously differentiable
multivariate function, then f is convex if and only if the univariate function
g(t) := f(x + t d),
t ∈[0, 1]
is a convex function for any x and x + d in the interior of the domain of f. This property
provides an alternative definition for convexity of a multivariate and differentiable function.
To see why it is true, first assume that f is convex and t1, t2 ∈[0, 1]. Then, using the
subgradient definition of convexity in (B.16), we have f(a) ⩾f(b) + (a −b)⊤v for some
su

bgradient v. Substituting with a = x + t1d and b = x + t2d, we obtain
g(t1) ⩾g(t2) + (t1 −t2) v⊤d
for any two points t1, t2 ∈[0, 1]. Therefore, g is convex, because we have identified the
existence of a subgradient v⊤d for each t2.
Conversely, assume that g is convex for t ∈[0, 1]. Since f is differentiable, then so is g.
Then, the convexity of g implies that there is a subgradient v at 0 such that: g(t) ⩾g(0)+t v
for all t ∈[0, 1]. Rearranging,
v ⩾g(t) −g(0)
t
,
and taking the right limit as t ↓0 we obtain v ⩾g′(0) = d⊤∇f(x). Therefore,
g(t) ⩾g(0) + t v ⩾g(0) + t d⊤∇f(x)
and substituting t = 1 yields:
f(x + d) ⩾f(x) + d⊤∇f(x),
so that there exists a subgradient vector, namely ∇f(x), for each x. Hence, f is convex by
the definition in (B.16).
An optimization program of the form (B.13) is said to be a convex programming prob-
lem if:
convex
programming
problem
1. The objective f is a convex function.
2. The inequality constraint functions {gi} are convex.
3. The equality constraint functions {hi} are affine, that is, of the form a⊤
i x −bi. This is
equivalent to both hi and −hi being convex for all i.
Table B.1 summarizes some commonly encountered problems, all of which are convex,
with the exception of the quadratic programs with A ⪰̸ 0.
406
Optimization Theory
Table B.1: Some common classes of optimization problems.
Name
f(x)
Constraints
Linear Program (LP)
c⊤x
Ax = b and x ⩾0
Inequality Form LP
c⊤x
Ax ⩽b
Quadratic Program (QP)
1
2x⊤Ax + b⊤x
Dx ⩽d, Ex = e
Convex QP
1
2x⊤Ax + b⊤x
Dx ⩽d, Ex = e
(A ⪰0)
Convex Program
f(x) convex
{gi(x)} convex, {hi(x)} of the form a⊤
i x −bi
Recognizing convex optimization problems or those that can be transformed to convex
optimization problems can be challenging. However, once formulated as convex optimiz-
ation problems, these can be efficiently solved using subgradient [112], bundle [57], and
cutting-plane methods [59].
B.2.2
Lagrangian Method
The main components of the Lagrangian method are the Lagrange multipliers and the
Lagran

ge function. The method was developed by Lagrange in 1797 for the optimization
problem (B.13) with only equality constraints. In 1951 Kuhn and Tucker extended Lag-
range’s method to inequality constraints. Given an optimization problem (B.13) containing
only equality constraints hi(x) = 0, i = 1, . . . , m, the Lagrange function
Lagrange
function
is defined as
L(x, β) = f(x) +
m
X
i=1
βi hi(x),
where the coefficients {βi} are called the Lagrange multipliers
Lagrange
multipliers
. A necessary condition for a
point x∗to be a local minimizer of f(x) subject to the equality constraints hi(x) = 0, i =
1, . . . , m, is
∇x L(x∗, β∗) = 0,
∇β L(x∗, β∗) = 0,
for some value β∗. The above conditions are also sufficient if L(x, β∗) is a convex function
of x.
Given the original optimization problem (B.13), containing both the equality and in-
equality constraints, the generalized Lagrange function, or Lagrangian
Lagrangian
, is defined as
L(x, α, β) = f(x) +
k
X
i=1
αi gi(x) +
m
X
i=1
βi hi(x).
Multivariate Differentiation and Optimization
407
Theorem B.2: Karush–Kuhn–Tucker (KKT) Conditions
A necessary condition for a point x∗to be a local minimizer of f(x) in the optimiz-
ation problem (B.13) is the existence of an α∗and β∗such that
∇x L(x∗, α∗, β∗) = 0,
∇β L(x∗, α∗, β∗) = 0,
gi(x∗) ⩽0,
i = 1, . . . , k,
α∗
i ⩾0,
i = 1, . . . , k,
α∗
i gi(x∗) = 0,
i = 1, . . . , k.
For convex programs we have the following important results [18, 43]:
1. Every local solution x∗to a convex programming problem is a global solution and
the set of global solutions is convex. If, in addition, the objective function is strictly
convex, then any global solution is unique.
2. For a strictly convex programming problem with C1 objective and constraint func-
tions, the KKT conditions are necessary and sufficient for a unique global solution.
B.2.3
Duality
The aim of duality is to provide an alternative formulation of an optimization problem
which is often more computationally efficient or has some theoreti

cal significance (see [43,
Page 219]). The original problem (B.13) is referred to as the primal
primal
problem whereas the
reformulated problem, based on Lagrange multipliers, is called the dual
dual
problem. Duality
theory is most relevant to convex optimization problems. It is well known that if the primal
optimization problem is (strictly) convex then the dual problem is (strictly) concave and
has a (unique) solution from which the (unique) optimal primal solution can be deduced.
The Lagrange dual program
Lagrange dual
program
(also called the Wolfe dual) of the primal program (B.13),
is:
max
α,β
L∗(α, β)
subject to:
α ⩾0,
where L∗is the Lagrange dual function:
L∗(α, β) = inf
x∈X L(x, α, β),
(B.17)
giving the greatest lower bound (infimum) of L(x, α, β) over all possible x ∈X.
It is not difficult to see that if f ∗is the minimal value of the primal problem, then
L∗(α, β) ⩽f ∗for any α ⩾0 and any β. This property is called weak duality. The Lag-
rangian dual program thus determines the best lower bound on f ∗. If d∗is the optimal
value for the dual problem then d∗⩽f ∗. The difference f ∗−d∗is called the duality gap.
The duality gap is extremely useful for providing lower bounds for the solutions of
primal problems that may be impossible to solve directly. It is important to note that for
408
Numerical Root-Finding and Minimization
linearly constrained problems, if the primal is infeasible (does not have a solution satisfying
the constraints), then the dual is either infeasible or unbounded. Conversely, if the dual
is infeasible then the primal has no solution. Of crucial importance is the strong duality
strong duality
theorem, which states that for convex programs (B.13) with linear constrained functions hi
and gi the duality gap is zero, and any x∗and (α∗, β∗) satisfying the KKT conditions are
(global) solutions to the primal and dual programs, respectively. In particular, this holds for
linear and convex quadratic programs (note that not all quadratic programs a

re convex).
For a convex primal program with C1 objective and constraint functions, the Lagrangian
dual function (B.17) can be obtained by simply setting the gradient (with respect to x) of
the Lagrangian L(x, α, β) to zero. One can further simplify the dual program by substitut-
ing into the Lagrangian the relations between the variables thus obtained.
Further, for a convex primal problem, if there is a strictly feasible point ex (that is, a
feasible point satisfying all of the inequality constraints with strict inequality), then the
duality gap is zero, and strong duality holds. This is known as Slater’s condition [18, Page
226].
The Lagrange dual problem is an important example of a saddle-point problem or min-
imax problem. In such problems the aim is to locate a point (x∗, y∗) ∈X × Y that satisfies
sup
y∈Y
inf
x∈X f(x, y) = inf
x∈X f(x, y∗) = f(x∗, y∗) = sup
y∈Y
f(x∗, y) = inf
x∈X sup
y∈Y
f(x, y).
The equation
sup
y∈Y
inf
x∈X f(x, y) = inf
x∈X sup
y∈Y
f(x, y)
is known as the minimax
minimax
equality. Other problems that fall into this framework are zero-
sum games in game theory; see also [24] for a number of combinatorial optimization prob-
lems that can be viewed as minimax problems.
B.3
Numerical Root-Finding and Minimization
In order to minimize a C1 function f : Rn →R one may solve
∇f(x) = 0,
which gives a stationary point of f. As a consequence, any technique for root-finding can
be transformed into an unconstrained optimization method by attempting to locate roots
of the gradient. However, as noted in Section B.2, not all stationary points are minima,
and so additional information (such as is contained in the Hessian, if f is C2) needs to be
considered in order to establish the type of stationary point.
Alternatively, a root of a continuous function g : Rn →Rn may be found by minimizing
the norm of g(x) over all x; that is, by solving minx f(x), with f(x) := ∥g(x)∥p, where for
p ⩾1 the p-norm
p-norm
of y = [y1, . . . , yn]⊤is defined as
∥y∥p :=

n
X
i=1


|yi|p1/p
.
Hence, any (un)constrained optimization method can be transformed into a technique for
locating the roots of a function.
Multivariate Differentiation and Optimization
409
Starting with an initial guess x0, most minimization and root-finding algorithms create
a sequence x0, x1, . . . using the iterative updating rule:
xt+1 = xt + αt dt,
t = 0, 1, 2, . . . ,
(B.18)
where αt > 0 is a (typically small) step size, called the learning rate
learning rate
, and the vector dt
is the search direction at step t. The iteration (B.18) continues until the sequence {xt} is
deemed to have converged to a solution, or a computational budget has been exhausted.
The performance of all such iterative methods depends crucially on the quality of the initial
guess x0.
There are two broad categories of iterative optimization algorithms of the form (B.18):
• Those of line search
line search
type, where at iteration t we first compute a direction dt and
then determine a reasonable step size αt along this direction. For example, in the
case of minimization, αt > 0 may be chosen to approximately minimize f(xt + α dt)
for fixed xt and dt.
• Those of trust region
trust region
type, where at each iteration t we first determine a suitable step
size αt and then compute an approximately optimal direction dt.
In the following sections, we review several widely-used root-finding and optimization
algorithms of the line search type.
B.3.1
Newton-Like Methods
Suppose we wish to find roots of a function f : Rn →Rn. If f is in C1, we can approximate
f around a point xt as
f(x) ≈f(xt) + Jf(xt)(x −xt),
where Jf is the matrix of Jacobi — the matrix of partial derivatives of f; see (B.3). When
☞398
Jf(xt) is invertible, this linear approximation has root xt −J−1
f (xt) f(xt). This gives the
iterative updating formula (B.18) for finding roots of f with direction dt = −J−1
f (xt) f(xt)
and learning rate αt = 1. This is known as Newton’s method
Newton’s
method
(or the Newton–Raphson
method) for root-f

inding.
Instead of a unit learning rate, sometimes it is more effective to use an αt that satisfies
the Armijo inexact line search
Armijo inexact
line search
condition:
∥f(xt + αt dt)∥< (1 −ε1 αt) ∥f(xt)∥,
where ε1 is a small heuristically chosen constant, say ε1 = 10−4. For C1 functions, such an
αt always exists by continuity and can be computed as in the following algorithm.
410
Numerical Root-Finding and Minimization
Algorithm B.3.1: Newton–Raphson for Finding Roots of f(x) = 0
input: An initial guess x and stopping error ε > 0.
output: The approximate root of f(x) = 0.
1 while ∥f(x)∥> ε and budget is not exhausted do
2
Solve the linear system Jf(x) d = −f(x).
3
α ←1
4
while ∥f(x + α d)∥> (1 −10−4α) ∥f(x)∥do
5
α ←α/2
6
x ←x + α d
7 return x
We can adapt a root-finding Newton-like method in order to minimize a differentiable
function f : Rn →R. We simply try to locate a zero of the gradient of f. When f is a
C2 function, the function ∇f : Rn →Rn is continuous, and so the root of ∇f leads to the
search direction
dt = −H−1
t ∇f(xt),
(B.19)
where Ht is the Hessian matrix at xt (the matrix of Jacobi of the gradient is the Hessian).
When the learning rate αt is equal to 1, the update xt −H−1
t ∇f(xt) can alternatively be
derived by assuming that f(x) is approximately quadratic and convex in the neighborhood
of xt, that is,
f(x) ≈f(xt) + (x −xt)⊤∇f(xt) + 1
2(x −xt)⊤Ht(x −xt),
(B.20)
and then minimizing the right-hand side of (B.20) with respect to x.
The following algorithm uses an Armijo inexact line search for minimization and
guards against the possibility that the Hessian may not be positive definite (that is, its
Cholesky decomposition does not exist).
☞373
Algorithm B.3.2: Newton–Raphson for Minimizing f(x)
input: An initial guess x; stopping error ε > 0; line search parameter ξ ∈(0, 1).
output: An approximate minimizer of f(x).
1 L ←In (the identity matrix)
2 while ∥∇f(x)∥> ε and budget is not exhausted do
3
Compute the Hessian H at x.
4
if H ≻0 then
// Cholesky 

is successful
5
Update L to be the Cholesky factor satisfying LL⊤= H.
6
else
7
Do not update the lower triangular L.
8
d ←−L−1∇f(x) (computed by forward substitution)
9
d ←L−⊤d (computed by backward substitution)
10
α ←1
11
while f(x + αd) > f(x) + α 10−4∇f(x)⊤d do
12
α ←α × ξ
13
x ←x + α d
14 return x
Multivariate Differentiation and Optimization
411
A downside with all Newton-like methods is that at each step they require the calcu-
lation and inversion of an n × n Hessian matrix, which has computing time of O(n3), and
is thus infeasible for large n. One way to avoid this cost is to use quasi-Newton methods,
described next.
B.3.2
Quasi-Newton Methods
The idea behind quasi-Newton methods is to replace the inverse Hessian in (B.19) at iter-
ation t by an n × n matrix C satisfying the secant condition
secant
condition
:
C 1 = δ,
(B.21)
where δ ←xt −xt−1 and 1 ←∇f(xt) −∇f(xt−1) are vectors stored in memory at each iter-
ation t. The secant condition is satisfied, for example, by the Broyden’s family of matrices:
A +
1
u⊤1(δ −A 1) u⊤
for some u , 0 and A. Since there is an infinite number of matrices that satisfy the condi-
tion (B.21), we need a way to determine a unique C at each iteration t such that computing
and storing C from one step to the next is fast and avoids any costly matrix inversion. The
following examples illustrate how, starting with an initial guess C = I at t = 0, such a
matrix C can be efficiently updated from one iteration to the next.
Example B.8 (Low-Rank Hessian Update)
The quadratic model (B.20) can be
strengthened by further assuming that exp(−f(x)) is proportional to a probability density
that can be approximated in the neighborhood of xt by the pdf of the N(xt+1, H−1
t ) dis-
tribution. This normal approximation allows us to measure the discrepancy between two
pairs (x1, H0) and (x2, H1) using the Kullback–Leibler divergence between the pdfs of the
☞
42
N(x1, H−1
0 ) and N(x2, H−1
1 ) distributions (see Exercise 4 on page 351):
D(x1, H−1
0 

| x2, H−1
1 ) := 1
2

tr(H1H−1
0 ) −ln |H1H−1
0 | + (x2 −x1)⊤H1(x2 −x1) −n

. (B.22)
Suppose that the latest approximation to the inverse Hessian is C and we wish to com-
pute an updated approximation for step t. One approach is to find the symmetric matrix
that minimizes its Kullback–Leibler discrepancy from C, as defined above, subject to the
constraint (B.21). In other words,
min
A D(0, C | 0, A)
subject to: A1 = δ, A = A⊤.
The solution to this constrained optimization (see Exercise 10 on page 353) yields the
Broyden–Fletcher–Goldfarb–Shanno or BFGS formula
bfgs formula
for updating the matrix C from one
iteration to the next:
CBFGS = C + 1⊤δ + 1⊤C 1
(1⊤δ)2
δδ⊤−
1
1⊤δ
 δ1⊤C + (δ1⊤C)⊤
|                                                     {z                                                     }
BFGS update
.
(B.23)
412
Numerical Root-Finding and Minimization
In a practical implementation, we keep a single copy of C in memory and apply the BFGS
update to it at every iteration. Note that if the current C is symmetric, then so is the updated
matrix. Moreover, the BFGS update is a matrix of rank two.
Since the Kullback–Leibler divergence is not symmetric, it is possible to flip the roles
of H0 and H1 in (B.22) and instead solve
min
A D(0, A | 0, C)
subject to: A1 = δ, A = A⊤.
The solution (see Exercise 10 on page 353) gives the Davidon–Fletcher–Powell or DFP
formula
dfp formula
for updating the matrix C from one iteration to the next:
CDFP = C + δδ⊤
1⊤δ −C 11⊤C
1⊤C1
|             {z             }
DFP update
.
(B.24)
Note that if the curvature condition 1⊤δ > 0 holds and the current C is symmetric positive
definite, then so is its update.
Example B.9 (Diagonal Hessian Update) The original BFGS formula requires O(n2)
storage and computation, which may be unmanageable for large n. One way to circumvent
the prohibitive quadratic cost is to only store and update a diagonal Hessian matrix from
one iteration to the next. If C is diagonal, then we may not be able to satisfy

 the secant
condition (B.21) and maintain positive definiteness. Instead the secant condition (B.21)
can be relaxed to the set of inequalities 1 ⩾C−1δ, which are related to the definition of a
subgradient for convex functions. We can then find a unique diagonal matrix by minimizing
☞403
D(xt, C | xt+1, A) with respect to A and subject to the constraints that A1 ⩾δ and A is
diagonal. The solution (Exercise 15 on page 353) yields the updating formula for a diagonal
element ci of C:
ci ←

2ci
1 +
q
1 + 4ciu2
i
,
if
2ci
1 +
q
1 + 4ciu2
i
⩾δi/1i
δi/1i,
otherwise,
(B.25)
where u := ∇f(xt) and we assume a unit learning rate: xt+1 = xt −Au.
Example B.10 (Scalar Hessian Update) If the identity matrix is used in place of the
Hessian in (B.19), one obtains steepest descent
steepest
descent
or gradient descent methods, in which the
iteration (B.18) reduces to xt+1 = xt −αt ∇f(xt).
The rationale for the name steepest descent is as follows. If we start from any point
x and make an infinitesimal move in some direction, then the function value is reduced
by the largest magnitude in the (unit norm) direction: u∗:= −∇f(x)/∥∇f(x)∥. This is seen
from the following inequality for all unit vectors u (that is, ∥u∥= 1):
d
dt f(x + t u∗)
t=0 ⩽d
dt f(x + t u)
t=0.
Observe that equality is achieved if and only if u = u∗. This inequality is an easy con-
sequence of the Cauchy–Schwarz inequality:
☞389
Multivariate Differentiation and Optimization
413
−∇f ⊤u ⩽|∇f ⊤u|
⩽
|{z}
Cauchy–Schwartz
∥u∥∥∇f∥= ∥∇f∥= −∇f ⊤u∗.
The steepest descent iteration, xt+1 = xt −αt∇f(xt), still requires a suitable choice of the
learning rate αt. An alternative way to think about the iteration is to assume that the learning
rate is always unity, and that at each iteration we use an inverse Hessian matrix of the form
αtI for some positive constant αt. Satisfying the secant condition (B.21) with a matrix of the
form C = αI is not possible. However, it is possible to choose α so that the secant conditi

on
(B.21) is satisfied in the direction of 1 (or alternatively δ). This gives the Barzilai–Borwein
formulas
Barzilai–
Borwein
formulas
for the learning rate at iteration t:
αt = 1⊤δ
∥1∥2
 
or alternatively
αt = ∥δ∥2
δ⊤1
!
.
(B.26)
B.3.3
Normal Approximation Method
Let φH−1
t (x−xt+1) denote the pdf of the N(xt+1, H−1
t ) distribution. As we already saw in Ex-
ample B.8, the quadratic approximation (B.20) of f in the neighborhood of xt is equivalent
(up to a constant) to the minus of the logarithm of the pdf φH−1
t (x −xt+1). In other words,
we use φH−1
t (x −xt+1) as a simple model for the density
exp(−f(x))
.Z
exp(−f(y)) dy.
One consequence of the normal approximation is that for x in the neighborhood of xt+1,
we can write:
−∇f(x) ≈∂
∂x ln φH−1
t (x −xt+1) = −Ht(x −xt+1).
In other words, using the fact that H⊤
t = Ht,
∇f(x)[∇f(x)]⊤≈Ht(x −xt+1)(x −xt+1)⊤Ht,
and taking expectations on both sides with respect to X ∼N(xt+1, H−1
t ) gives:
E ∇f(X) [∇f(X)]⊤≈Ht.
This suggests that, given the gradient vectors computed in the past h (where h stands for
history) of Newton iterations:
ui := ∇f(xi),
i = t −(h −1), . . . , t,
the Hessian matrix Ht can be approximated via the average
1
h
tX
i=t−h+1
uiu⊤
i .
A shortcoming of this approximation is that, unless h is large enough, the Hessian approx-
imation Pt
i=t−h+1 uiu⊤
i may not be full rank and hence not invertible. To ensure that the
414
Numerical Root-Finding and Minimization
Hessian approximation is invertible, we add a suitable diagonal matrix A0 to obtain the
regularized version of the approximation:
☞217
Ht ≈A0 + 1
h
tX
i=t−h+1
uiu⊤
i .
With this full-rank approximation of the Hessian, the Newton search direction in (B.19)
becomes:
dt = −
A0 + 1
h
tX
i=t−h+1
uiu⊤
i

−1
ut.
(B.27)
Thus, dt can be computed in O(h2 n) time via the Sherman–Morrison Algorithm A.6.1.
☞373
Further to this, the search direction (B.27) can be efficiently updated to the next one:
dt+1 = −
A0 + 1
h
t+1
X
i=t−h+2
uiu⊤
i



−1
ut+1
in O(h n) time, thus avoiding the usual O(h2 n) cost (see Exercise 6 on page 352).
B.3.4
Nonlinear Least Squares
Consider the squared-error training loss in nonlinear regression:
☞188
ℓτ(g(· | β)) = 1
n
n
X
i=1
(g(xi | β) −yi)2,
where g(· | β) is a nonlinear prediction function that depends on the parameter β (for ex-
ample, (5.29) shows the nonlinear logistic prediction function). The training loss can be
written as 1
n∥g(τ | β) −y∥2, where g(τ | β) := [g(x1 | β), . . . , g(xn | β)]⊤is the vector of out-
puts.
We wish to minimize the training loss in terms of β. In the Newton-like methods in
Section B.3.1, one derives an iterative minimization algorithm that is inspired by a Taylor
expansion of ℓτ(g(· | β)). Instead, given a current guess βt, we can consider the Taylor ex-
pansion of the nonlinear prediction function g:
g(τ | β) ≈g(τ | βt) + Gt(β −βt),
where Gt := Jg(βt) is the matrix of Jacobi of g(τ | β) at βt. Denoting the residual et :=
☞398
g(τ | βt) −y and replacing g(τ | β) with its Taylor approximation in ℓτ(g(· | β)), we obtain
the approximation to the training loss in the neighborhood of βt:
ℓτ(g(· | β)) ≈1
n
Gt(β −βt) + et

2 .
The minimization of the right-hand side is a linear least-squares problem and therefore
dt := β −βt satisfies the normal equations: G⊤
t Gtdt = G⊤
t (−et). Assuming that G⊤
t Gt is
☞28
invertible, the normal equations yield the Gauss–Newton
Gauss–Newton
search direction:
dt = −(G⊤
t Gt)−1G⊤
t et.
Multivariate Differentiation and Optimization
415
Unlike the search direction (B.19) for Newton-like algorithms, the search direction of a
Gauss–Newton algorithm does not require the computation of a Hessian matrix.
Observe that in the Gauss–Newton approach we determine dt by viewing the search
direction as coefficients in a linear regression with feature matrix Gt and response −et. This
suggests that instead of using a linear regression, we can compute dt via a ridge regression
with a suitable choice for the regularization 

parameter γ:
☞217
dt = −(G⊤
t Gt + nγIp)−1G⊤
t et.
If we replace nIp with the diagonal matrix diag(G⊤
t Gt), we then obtain the Levenberg–
Marquardt
Levenberg–
Marquardt
search direction:
dt = −(G⊤
t Gt + γ diag(G⊤
t Gt))−1G⊤
t et.
(B.28)
Recall that the ridge regularization parameter γ has the following effect on the least-squares
solution: When it is zero, then the solution dt coincides with the search direction of the
Gauss–Newton method, and when γ tends to infinity, then ∥dt∥tends to zero. Thus, γ
controls both the magnitude and direction of vector dt. A simple version of the Levenberg–
Marquardt algorithm is the following.
Algorithm B.3.3: Levenberg–Marquardt for Minimizing 1
n∥g(τ | β) −y∥2
input: An initial guess β0; stopping error ε > 0; training set τ.
output: An approximate minimizer of 1
n∥g(τ | β) −y∥2.
1 t ←0 and γ ←0.01 (or another default value)
2 while stopping condition is not met do
3
Compute the search direction dt via (B.28).
4
et+1 ←g(τ | βt + dt) −y
5
if ∥et+1∥< ∥et∥then
6
γ ←γ/10,
et+1 ←et,
βt+1 ←βt + dt
7
else
8
γ ←γ × 10
9
t ←t + 1
10 return βt
B.4
Constrained Minimization via Penalty Functions
A constrained optimization problem of the form (B.13) can sometimes be reformulated as a
simpler unconstrained problem — for example, the unconstrained set Y can be transformed
to the feasible region X of the constrained problem via a function ϕ : Rn →Rn such that
X = ϕ(Y). Then, (B.13) is equivalent to the minimization problem
min
y∈Y f(ϕ(y)),
in the sense that a solution x∗of the original problem is obtained from a transformed
solution y∗via x∗= ϕ(y∗). Table B.2 lists some examples of possible transformations.
416
Constrained Minimization via Penalty Functions
Table B.2: Some transformations to eliminate constraints.
Constrained
Unconstrained
x > 0
exp(y)
x ⩾0
y2
a ⩽x ⩽b
a + (b −a) sin2(y)
Unfortunately, an unconstrained minimization method used in combination with these
transformations is rarely effective. Instead, it is more common to use penalty

 functions.
The overarching idea of penalty functions
penalty
functions
is to transform a constrained problem into
an unconstrained problem by adding weighted constraint-violation terms to the original
objective function, with the premise that the new problem has a solution that is identical or
close to the original one.
For example, if there are only equality constraints, then
ef(x) := f(x) +
m
X
i=1
ai |hi(x)|p
for some constants a1, . . . , am > 0 and integer p ∈{1, 2}, gives an exact penalty function,
in the sense that the minimizer of the penalized function ef is equal to the minimizer of f
subject to the m equality constraints h1, . . . , hm. With the addition of inequality constraints,
one could use
ef(x) = f(x) +
m
X
i=1
ai |hi(x)|p +
k
X
j=1
bj max{gj(x), 0}
for some constants a1, . . . , am, b1, . . . , bk > 0.
Example B.11 (Alternating Direction Method of Multipliers) The Lagrange method
is designed to handle convex minimization subject to equality constraints. Nevertheless,
☞406
some practical algorithms may still use the penalty function approach in combination with
the Lagrangian method. An example is the alternating direction method of multipliers
alternating
direction
method of
multipliers
(ADMM) [17]. The ADMM solves problems of the form:
min
x∈Rn ,z∈Rm
f(x) + g(z)
subject to:
Ax + Bz = c,
(B.29)
where A ∈Rp×n, B ∈Rp×m, and c ∈Rp, and f : Rn →R and g : Rm →R are convex func-
tions. The approach is to form an augmented Lagrangian
Lϱ(x, z, β) := f(x) + g(z) + β⊤(Ax + Bz −c) + ϱ
2 ∥Ax + Bz −c∥2,
where ϱ > 0 is a penalty parameter, and β ∈Rp are dual variables. The ADMM then iterates
through updates of the following form:
x(t+1) = argmin
x∈Rn
Lϱ(x, z(t), β(t))
z(t+1) = argmin
z∈Rm
Lϱ(x(t+1), z, β(t))
β(t+1) = β(t) + ϱ

Ax(t+1) + Bz(t+1) −c

.
Multivariate Differentiation and Optimization
417
Suppose that (B.13) has inequality constraints only. Barrier functions
Barrier
functions
are an important ex-
ample of penalty functions that can handle inequalit

y constraints. The prototypical example
is a logarithmic barrier function which gives the unconstrained optimization:
ef(x) = f(x) −ν
k
X
j=1
ln(−gj(x)),
ν > 0,
such that the minimizer of ef tends to the minimizer of f as ν →0. Direct minimization
of ef via an unconstrained minimization algorithm is frequently too difficult. Instead, it
is common to combine the logarithmic barrier function with the Lagrangian method as
follows.
The idea is to introduce k nonnegative auxiliary or slack variables
slack variables
s1, . . . , sk that satisfy
the equalities gj(x) + sj = 0 for all j. These equalities ensure that the inequality constraints
are maintained: g j(x) = −sj ⩽0 for all j. Then, instead of the unconstrained optimization
of ef, we consider the unconstrained optimization of the Lagrangian:
L(x, s, β) = f(x) −ν
k
X
j=1
ln sj +
k
X
j=1
β j (g j(x) + s j),
(B.30)
where ν > 0 and β are the Lagrange multipliers for the equalities gj(x)+sj = 0, j = 1, . . . , k.
Observe how the logarithmic barrier function keeps the slack variables positive. In
addition, while the optimization of ef is over n dimensions (recall that x ∈Rn), the optimiz-
ation of the Lagrangian function L is over n + 2k dimensions. Despite this enlargement of
the search space with the variables s and β, the optimization of the Lagrangian L is easier
in practice than the direct optimization of ef.
Example B.12 (Interior-Point Method for Nonnegativity)
One of the simplest and
most common constrained optimization problems can be formulated as the minimization
of f(x) subject to nonnegative x, that is: minx⩾0 f(x). In this case, the Lagrangian with
logarithmic barrier (B.30) is:
L(x, s, β) = f(x) −ν
X
k
ln sk + β⊤(s −x).
The KKT conditions in Theorem B.2 are a necessary condition for a minimizer, and yield
the nonlinear system for [x⊤, s⊤, β⊤]⊤∈R3n:

∇f(x) −β
−ν/s + β
s −x
= 0,
where ν/s is a shorthand notation for a column vector with components {ν/sj}. To solve this
system, we can use New

ton’s method for root finding (see, for example, Algorithm B.3.1),
which requires a formula for the matrix of Jacobi of L. Here, this (3n) × (3n) matrix is:
JL(x, s, β) =

H
O
−I
O
D
I
−I
I
O
=
" H
B
B⊤
E
#
,
418
Constrained Minimization via Penalty Functions
where H is the n × n Hessian of f at x; D := diag (ν/(s ⊙s)) is an n × n diagonal matrix;
B := [O, −I] is an n × (2n) matrix, and1
E :=
"D
I
I
O
#
=
"O
I
I
−D
#−1
.
Further, we define
Hν := (H −BE−1B⊤)−1 = (H + D)−1.
Using this notation and applying the matrix blockwise inversion formula (A.14), we obtain
☞371
the inverse of the matrix of Jacobi:
" H
B
B⊤
E
#−1
=
"
Hν
−HνBE−1
−E−1B⊤Hν
E−1 + E−1B⊤HνBE−1
#
=

Hν
Hν
−HνD
Hν
Hν
I −HνD
−DHν
I −DHν
DHνD −D
.
Therefore, the search direction in Newton’s root-finding method is given by:
−J−1
L

∇f(x) −β
−ν/s + β
s −x
=

dx
dx + x −s
ν/s −β −D(dx + x −s)
,
where
dx := −(H + D)−1 h
∇f(x) −2ν/s + Dx
i
,
and we have assumed that H + D is a positive-definite matrix. If at any step of the iteration
the matrix H + D fails to be positive-definite, then Newton’s root-finding algorithm may
fail to converge. Thus, any practical implementation will have to include a fail-safe feature
to guard against this possibility.
In summary, for a given penalty parameter ν > 0, we can locate the approximate non-
negative minimizer of f using, for example, the version of the Newton–Raphson root-
finding method given in Algorithm B.4.1.
In practice, one needs to choose a sufficiently small value for ν, so that the output xν
of Algorithm B.4.1 is a good approximation to x∗= argminx⩾0 f(x). Alternatively, one
can create a decreasing sequence of penalty parameters ν1 > ν2 > · · · and compute the
corresponding solutions xν1, xν2, . . . of the penalized problems. In the so-called interior-
point method, a given xνi is used as an initial guess for computing xνi+1 and so on until the
interior-point
method
approximation to t

he minimizer x∗= argminx⩾0 f(x) is deemed accurate.
1Here O is an n × n matrix of zeros and I is the n × n identity matrix.
Multivariate Differentiation and Optimization
419
Algorithm B.4.1: Approximating x∗= argminx⩾0 f(x) with Logarithmic Barrier
input: An initial guess x and stopping error ε > 0.
output: The approximate nonnegative minimizer xν of f.
1 s ←x,
β ←ν/s,
dx ←β
2 while ∥dx∥> ε and budget is not exhausted do
3
Compute the gradient u and the Hessian H of f at x.
4
s1 ←ν/s,
s2 ←s1/s,
w ←2s1 −u −s2 ⊙x
5
if (H + diag(s2)) ≻0 then
// if Cholesky successful
6
Compute the Cholesky factor L satisfying LL⊤= H + diag(s2).
7
dx ←L−1w (computed by forward substitution)
8
dx ←L−⊤dx (computed by backward substitution)
9
else
10
dx ←w/s2
// if Cholesky fails, do steepest descent
11
ds ←dx + x −s,
dβ ←s1 −β −s2 ⊙ds,
α ←1
12
while min j{sj + α dsj} < 0 do
13
α ←α/2
// ensure nonnegative slack variables
14
x ←x + α dx,
s ←s + α ds,
β ←β + α dβ
15 return xν ←x
Further Reading
For an excellent introduction to convex optimization and Lagrangian duality see [18]. A
classical text on optimization algorithms and, in particular, on quasi-Newton methods is
[43]. For more details on the alternating direction method of multipliers see [17].
420
APPENDIXC
PROBABILITY AND STATISTICS
The purpose of this chapter is to establish the baseline probability and statistics
background for this book. We review basic concepts such as the sum and product rules
of probability, random variables and their probability distributions, expectations, in-
dependence, conditional probability, transformation rules, limit theorems, and Markov
chains. The properties of the multivariate normal distribution are discussed in more de-
tail. The main ideas from statistics are also reviewed, including estimation techniques
(such as maximum likelihood estimation), confidence intervals, and hypothesis testing.
C.1
Random Experiments and Probability Spaces
The basic notion in probability theory is that of a random e

xperiment
random
experiment
: an experiment
whose outcome cannot be determined in advance. Mathematically, a random experiment is
modeled via a triplet (Ω, H, P), where:
• Ωis the set of all possible outcomes of the experiment, called the sample space
sample space
.
• H is the collection of all subsets of Ωto which a probability can be assigned; such
subsets are called events
events
.
• P is a probability measure
probability
measure
, which assigns to each event A a number P[A] between 0
and 1, indicating the likelihood that the outcome of the random experiment lies in A.
Any probability measure P must satisfy the following Kolmogorov axioms
Kolmogorov
axioms
:
1. P[A] ⩾0 for every event A.
2. P[Ω] = 1.
3. For any sequence A1, A2, . . . of events,
P
h [
i
Ai
i
⩽
X
i
P[Ai],
(C.1)
with strict equality whenever the events are disjoint (that is, non-overlapping).
421
422
Random Variables and Probability Distributions
When (C.1) holds as an equality, it is often referred to as the sum rule
sum rule
of probability. It
simply states that if an event can happen in a number of different but not simultaneous
ways, the probability of that event is the sum of the probabilities of the comprising events.
If the events are allowed to overlap, then the inequality (C.1) is called the union bound
union bound
.
In many applications the sample space is countable; that is, Ω= {a1, a2, . . .}. In this
case the easiest way to specify a probability measure P is to first assign a number pi to
each elementary event
elementary
event
{ai}, with P
i pi = 1, and then to define
P[A] =
X
i:ai∈A
pi for all A ⊆Ω.
Here the collection of events H can be taken to be equal to the collection of all subsets
of Ω. The triple (Ω, H, P) is called a discrete probability space
discrete
probability
space
. This idea is graphically
represented in Figure C.1. Each element ai, represented by a dot, is assigned a weight (that
is, probability) pi, indicated by the size of the dot. The probability of the event A is s

imply
the sum of the weights of all the outcomes in A.
Ω
A
Figure C.1: A discrete probability space.
Remark C.1 (Equilikely Principle) A special case of a discrete probability space oc-
curs when a random experiment has finitely many outcomes that are all equally likely. In
this case the probability measure is given by
P[A] = |A|
|Ω|,
(C.2)
where |A| denotes the number of outcomes in A and |Ω| is the total number of outcomes.
Thus, the calculation of probabilities reduces to counting numbers of outcomes in events.
This is called the equilikely principle
equilikely
principle
.
C.2
Random Variables and Probability Distributions
It is often convenient to describe a random experiment via “random variables”, repres-
enting numerical measurements of the experiment. Random variables are usually denoted
by capital letters from the last part of the alphabet. From a mathematical point of view, a
random variable
random
variable
X is a function from Ωto R such that sets of the form {a < X ⩽b} :=
{ω ∈Ω: a < X(ω) ⩽b} are events (and so can be assigned a probability).
Probability and Statistics
423
All probabilities involving a random variable X can be computed, in principle, from its
cumulative distribution function
cumulative
distribution
function
(cdf), defined by
F(x) = P[X ⩽x], x ∈R.
For example P[a < X ⩽b] = P[X ⩽b] −P[X ⩽a] = F(b) −F(a). Figure C.2 shows a
generic cdf. Note that any cdf is right-continuous, increasing, and lies between 0 and 1.
0
1
Figure C.2: A cumulative distribution function (cdf).
A cdf Fd is called discrete
discrete cdf
if there exist numbers x1, x2, . . . and probabilities 0 < f(xi) ⩽
1 summing up to 1, such that for all x
Fd(x) =
X
xi⩽x
f(xi).
(C.3)
Such a cdf is piecewise constant and has jumps of sizes f(x1), f(x2), . . . at points x1, x2, . . .,
respectively. The function f(x) is called a probability mass function or discrete probability
density function (pdf).
discrete pdf
It is often easier to use the pdf rather than the cdf, since probabilities


can simply be calculated from it via summation:
P[X ∈B] =
X
x∈B
f(x),
as illustrated in Figure C.3.
Figure C.3: Discrete probability density function (pdf). The darker area corresponds to the
probability P[X ∈B].
424
Random Variables and Probability Distributions
A cdf Fc is called continuous1,
continuous cdf
if there exists a positive function f such that for all x
Fc(x) =
Z x
−∞
f(u) du.
(C.4)
Note that such an Fc is differentiable (and hence continuous) with derivative f. The func-
tion f is called the probability density function (continuous pdf).
pdf
By the fundamental the-
orem of integration, we have
P[a < X ⩽b] = F(b) −F(a) =
Z b
a
f(x) dx.
Thus, calculating probabilities reduces to integration, as illustrated in Figure C.4.
Figure C.4: Continuous probability density function (pdf). The shaded area corresponds to
the probability P[X ∈B], with B being here the interval (a, b].
Remark C.2 (Probability Density and Probability Mass) It is important to note that
we deliberately use the same name, “pdf”, and symbol, f, in both the discrete and the
continuous case, rather than distinguish between a probability mass function (pmf) and
probability density function (pdf). From a theoretical point of view the pdf plays exactly
the same role in the discrete and continuous cases. We use the notation X ∼Dist, X ∼f,
and X ∼F to indicate that X has distribution Dist, pdf f, and cdf F.
Tables C.1 and C.2 list a number of important continuous and discrete distributions.
Note that in Table C.1, Γ is the gamma function: Γ(α) =
R ∞
0 e−xxα−1 dx,
α > 0.
1In advanced probability, we would say “absolutely continuous with respect to the Lebesgue measure”.
Probability and Statistics
425
Table C.1: Commonly used continuous distributions.
Name
Notation
f(x)
x ∈
Parameters
Uniform
U[α, β]
1
β −α
[α, β]
α < β
Normal
N(µ, σ2)
1
σ
√
2π
e−1
2( x−µ
σ )
2
R
σ > 0, µ ∈R
Gamma
Gamma(α, λ)
λαxα−1e−λx
Γ(α)
R+
α, λ > 0
Inverse Gamma
InvGamma(α, λ)
λαx−α−1e−λx−1
Γ(α)
R+
α, λ > 0
Exponential
Exp(λ)


λ e−λx
R+
λ > 0
Beta
Beta(α, β)
Γ(α + β)
Γ(α)Γ(β) xα−1(1 −x)β−1
[0, 1]
α, β > 0
Weibull
Weib(α, λ)
αλ (λx)α−1e−(λx)α
R+
α, λ > 0
Pareto
Pareto(α, λ)
αλ (1 + λx)−(α+1)
R+
α, λ > 0
Student
tν
Γ( ν+1
2 )
√νπ Γ( ν
2)
 
1 + x2
ν
!−(ν+1)/2
R
ν > 0
F
F(m, n)
Γ(m+n
2 ) (m/n)m/2x(m−2)/2
Γ(m
2 ) Γ(n
2) [1 + (m/n)x](m+n)/2
R+
m, n ∈N+
The Gamma(n/2, 1/2) distribution is called the chi-squared distribution
χ2
n distribution
with n degrees
of freedom, denoted χ2
n. The t1 distribution is also called the Cauchy distribution.
Table C.2: Commonly used discrete distributions.
Name
Notation
f(x)
x ∈
Parameters
Bernoulli
Ber(p)
px(1 −p)1−x
{0, 1}
0 ⩽p ⩽1
Binomial
Bin(n, p)
 n
x
!
px(1 −p)n−x
{0, 1, . . . , n}
0 ⩽p ⩽1,
n ∈N
Discrete
uniform
U{1, . . . , n}
1
n
{1, . . . , n}
n ∈{1, 2, . . .}
Geometric
Geom(p)
p(1 −p)x−1
{1, 2, . . .}
0 ⩽p ⩽1
Poisson
Poi(λ)
e−λ λx
x!
N
λ > 0
426
Expectation
C.3
Expectation
It is often useful to consider different kinds of numerical characteristics of a random vari-
able. One such quantity is the expectation, which measures the “average” value of the
distribution.
The expectation
expectation
(or expected value or mean) of a random variable X with pdf f, denoted
by EX or2 E[X] (and sometimes µ), is defined by
EX =

P
x x f(x)
discrete case,
R ∞
−∞x f(x) dx
continuous case.
If X is a random variable, then a function of X, such as X2 or sin(X), is again a random
variable. Moreover, the expected value of a function of X is simply a weighted average of
the possible values that this function can take. That is, for any real function h
Eh(X) =

P
x h(x) f(x)
discrete case,
R ∞
−∞h(x) f(x) dx
continuous case,
provided that the sum or integral are well-defined.
The variance
variance
of a random variable X, denoted by Var X (and sometimes σ2), is defined
by
Var X = E(X −E[X])2 = EX2 −(EX)2.
The square root of the variance is called the standard deviation
standard
deviation
. Table C.3 lists the expect-
ations and variances for some well-known distrib

utions. Both variance and standard devi-
ation measure the spread or dispersion of the distribution. Note, however, that the standard
deviation measures the dispersion in the same units as the random variable, unlike the
variance, which uses squared units.
Table C.3: Expectations and variances for some well-known distributions.
Dist.
EX
Var X
Bin(n, p)
np
np(1 −p)
Geom(p)
1
p
1 −p
p2
Poi(λ)
λ
λ
U[α, β]
α + β
2
(β −α)2
12
Exp(λ)
1
λ
1
λ2
tν
0 (ν > 1)
ν
ν−2
(ν > 2)
Dist.
EX
Var X
Gamma(α, λ)
α
λ
α
λ2
N(µ, σ2)
µ
σ2
Beta(α, β)
α
α+β
αβ
(α+β)2(1+α+β)
Weib(α, λ)
Γ(1/α)
α λ
2Γ(2/α)
α
−
Γ(1/α)
α λ
2
F(m, n)
n
n−2 (n > 2)
2n2(m+n−2)
m(n−2)2(n−4)
(n > 4)
2We only use brackets in an expectation if it is unclear with respect to which random variable the ex-
pectation is taken.
Probability and Statistics
427
It is sometimes useful to consider the moment generating function
moment
generating
function
of a random variable
X. This is the function M defined by
M(s) = E esX,
s ∈R.
(C.5)
The moment generating functions of two random variables coincide if and only if the ran-
dom variables have the same distribution; see also Theorem C.12.
Example C.1 (Moment Generation Function of the Gamma(α, λ) Distribution) Let
X ∼Gamma(α, λ). For s < λ, the moment generating function of X at s is given by
M(s) = EesX
=
Z ∞
0
esx e−λx λα xα−1
Γ(α)
dx
=

λ
λ −s
α Z ∞
0
e−(λ−s)x (λ −s)α xα−1
Γ(α)
|                    {z                    }
pdf of Gamma(α,λ−s)
dx =

λ
λ −s
α
.
For s ⩾λ, M(s) = ∞. Interestingly, the moment generating function has a much simpler
formula than the pdf.
C.4
Joint Distributions
Distributions for random vectors and stochastic processes can be specified in much the
same way as for random variables. In particular, the distribution of a random vector X =
[X1, . . . , Xn]⊤is completely determined by specifying the joint cdf
joint cdf
F, defined by
F(x1, . . . , xn) = P[X1 ⩽x1, . . . , Xn ⩽xn],
xi ∈R, i = 1, . . . , n.
Similarly, the distribution of a stochastic process
st

ochastic
process
, that is, a collection of random vari-
ables {Xt, t ∈T }, for some index set T , is completely determined by its finite-dimensional
distributions; specifically, the distributions of the random vectors [Xt1, . . . , Xtn]⊤for every
choice of n and t1, . . . , tn.
By analogy to the one-dimensional case, a random vector X = [X1, . . . , Xn]⊤taking
values in Rn is said to have a pdf f if, in the continuous case,
P[X ∈B] =
Z
B
f(x) dx,
(C.6)
for all n-dimensional rectangles B. Replace the integral with a sum for the discrete case.
The pdf is also called the joint pdf
joint pdf
of X1, . . . , Xn. The pdfs of the individual components —
called marginal pdfs —
marginal pdf
can be recovered from the joint pdf by “integrating out the other
variables”. For example, for a continuous random vector [X, Y]⊤with pdf f, the pdf fX of
X is given by
fX(x) =
Z
f(x, y) dy.
428
Conditioning and Independence
C.5
Conditioning and Independence
Conditional probabilities and conditional distributions are used to model additional inform-
ation on a random experiment. Independence is used to model lack of such information.
C.5.1
Conditional Probability
Suppose some event B ⊆Ωoccurs. Given this fact, event A will occur if and only if
A ∩B occurs, and the relative chance of A occurring is therefore P[A ∩B]/P[B], provided
P[B] > 0. This leads to the definition of the conditional probability
conditional
probability
of A given B:
P[A | B] = P[A ∩B]
P[B]
,
if P[B] > 0.
(C.7)
The above definition breaks down if P[B] = 0. Such conditional probabilities must be
treated with more care [11].
Three important consequences of the definition of conditional probability are:
1. Product rule
Product rule
: For any sequence of events A1, A2, . . . , An,
P[A1 · · · An] = P[A1] P[A2 | A1] P[A3 | A1A2] · · · P[An | A1 · · · An−1],
(C.8)
using the abbreviation A1A2 · · · Ak := A1 ∩A2 ∩· · · ∩Ak.
2. Law of total probability
Law of total
probability
: If {Bi} forms a partition of Ω(that is, Bi ∩Bj = ∅,

 i , j and
∪iBi = Ω), then for any event A
P[A] =
X
i
P[A | Bi] P[Bi].
(C.9)
3. Bayes’ rule
Bayes’ rule
: Let {Bi} form a partition of Ω. Then, for any event A with P[A] > 0,
P[Bj | A] =
P[A | Bj] P[Bj]
P
i P[A | Bi] P[Bi].
(C.10)
C.5.2
Independence
Two events A and B are said to be independent
independent
events
if the knowledge that B has occurred does
not change the probability that A occurs. That is, A, B independent ⇔P[A | B] = P[A].
Since P[A | B] P[B] = P[A ∩B], an alternative definition of independence is
A, B independent ⇔P[A ∩B] = P[A] P[B].
This definition covers the case where P[B] = 0 and can be extended to arbitrarily many
events: events A1, A2, . . . are said to be (mutually) independent if for any k and any choice
of distinct indices i1, . . . , ik,
P[Ai1 ∩Ai2 ∩· · · ∩Aik] = P[Ai1] P[Ai2] · · · P[Aik].
Probability and Statistics
429
The concept of independence can also be formulated for random variables. Random
variables X1, X2, . . . are said to be independent
independent
random
variables
if the events {Xi1 ⩽xi1}, . . . , {Xin ⩽xin} are
independent for all finite choices of n distinct indices i1, . . . , in and values xi1, . . . , xin.
An important characterization of independent random variables is the following (for a
proof, see [101], for example).
Theorem C.1: Independence Characterization
Random variables X1, . . . , Xn with marginal pdfs fX1, . . . , fXn and joint pdf f are in-
dependent if and only if
f(x1, . . . , xn) = fX1(x1) · · · fXn(xn)
for all x1, . . . , xn.
(C.11)
Many probabilistic models involve random variables X1, X2, . . . that are independent
and identically distributed, abbreviated as iid
iid
. We use this abbreviation throughout this
book.
C.5.3
Expectation and Covariance
Similar to the univariate case, the expected value of a real-valued function h of a random
vector X ∼f is a weighted average of all values that h(X) can take. Specifically, in the
continuous case, Eh(X) =
R
h(x) f(x) dx. In the discrete case replace this mul

tidimensional
integral with a sum. Using this result, it is not difficult to show that for any collection of
dependent or independent random variables X1, . . . , Xn,
E[a + b1X1 + b2X2 + · · · + bnXn] = a + b1EX1 + · · · + bnEXn
(C.12)
for all constants a, b1, . . . , bn. Moreover, for independent random variables,
E[X1X2 · · · Xn] = EX1 EX2 · · · EXn.
(C.13)
We leave the proofs as an exercise.
The covariance
covariance
of two random variables X and Y with expectations µX and µY, respect-
ively, is defined as
Cov(X, Y) = E[(X −µX)(Y −µY)].
This is a measure of the amount of linear dependency between the variables. Let σ2
X =
Var X and σ2
Y = Var Y. A scaled version of the covariance is given by the correlation
coefficient
correlation
coefficient
,
ϱ(X, Y) = Cov(X, Y)
σX σY
.
The following properties follow directly from the definitions of variance and covariance.
1. Var X = EX2 −µ2
X.
2. Var[aX + b] = a2 σ2
X.
3. Cov(X, Y) = E[XY] −µX µY.
4. Cov(X, Y) = Cov(Y, X).
430
Conditioning and Independence
5. −σXσY ⩽Cov(X, Y) ⩽σXσY.
6. Cov(aX + bY, Z) = a Cov(X, Z) + b Cov(Y, Z).
7. Cov(X, X) = σ2
X.
8. Var[X + Y] = σ2
X + σ2
Y + 2 Cov(X, Y).
9. If X and Y are independent, then Cov(X, Y) = 0.
As a consequence of Properties 2 and 8 we have that for any sequence of independent
random variables X1, ..., Xn with variances σ2
1, . . . , σ2
n,
Var[a1X1 + a2X2 + · · · + anXn] = a2
1 σ2
1 + a2
2 σ2
2 + · · · + a2
n σ2
n,
(C.14)
for any choice of constants a1, . . . , an.
For random column vectors, such as X = [X1, . . . , Xn]⊤, it is convenient to write the
expectations and covariances in vector and matrix notation. For a random vector X we
define its expectation vector
expectation
vector
as the vector of expectations
µ = [µ1, . . . , µn]⊤= [EX1, . . . , EXn]⊤.
Similarly, if the expectation of a matrix is the matrix of expectations, then given two ran-
dom vectors X ∈Rn and Y ∈Rm, the n × m matrix
Cov(X, Y) = E[(X −EX)(Y −EY)⊤]
(C.15)
has (i, j)-th element Cov(Xi, Y j) = E[(Xi−EXi)(

Yj−EY j)]. A consequence of this definition
is that
Cov(AX, BY) = ACov(X, Y)B⊤,
where A and B are two matrices with n and m columns, respectively.
The covariance matrix
covariance
matrix
of the vector X is defined as the n×n matrix Cov(X, X). The co-
variance matrix is also denoted as Var(X) = Cov(X, X), in analogy with the scalar identity
Var(X) = Cov(X, X).
A useful application of the cyclic property of the trace of a matrix (see Theorem A.1)
is the following.
☞357
Theorem C.2: Expectation of a Quadratic Form
Let A be an n × n matrix and X an n-dimensional random vector with expectation
vector µ and covariance matrix Σ. The random variable Y := X⊤AX has expectation
tr(AΣ) + µ⊤Aµ.
Proof: Since Y is a scalar, it is equal to its trace. Now, using the cyclic property: EY =
E tr(Y) = E tr(X⊤AX) = E tr(AXX⊤) = tr(A E[XX⊤]) = tr(A(Σ + µµ⊤)) = tr(AΣ) +
tr(Aµµ⊤) = tr(AΣ) + µ⊤Aµ.
□
Probability and Statistics
431
C.5.4
Conditional Density and Conditional Expectation
Suppose X and Y are both discrete or both continuous, with joint pdf f, and suppose fX(x) >
0. Then, the conditional pdf
conditional pdf
of Y given X = x is given by
fY|X(y | x) = f(x, y)
fX(x)
for all y.
(C.16)
In the discrete case, the formula is a direct translation of (C.7), with fY|X(y | x) = P[Y =
y | X = x]. In the continuous case, a similar interpretation in terms of densities can be used;
see, for example, [101, Page 221]. The corresponding distribution is called the conditional
distribution of Y given X = x. Note that (C.16) implies that
conditional
distribution
f(x, y) = fX(x) fY | X(y | x).
This is useful when the marginal and conditional pdfs are given, rather than the joint one.
More generally, for the n-dimensional case we have
f(x1, . . . , xn) = fX1(x1) fX2 | X1(x2 | x1) · · · fXn | X1,...,Xn−1(xn | x1, . . . , xn−1),
(C.17)
which is in essence a rephrasing of the product rule (C.8) in terms of probability densities.
☞428
As a conditional pdf has all the properties of an ordinary pdf, we may defi

ne expecta-
tions with respect to it. The conditional expectation of a random variable Y given X = x is
conditional
expectation
defined as
E[Y | X = x] =

P
y y fY|X(y | x)
discrete case,
R
y fY|X(y | x) dy
continuous case.
(C.18)
Note that E[Y | X = x] is a function of x. The corresponding random variable is written
as E[Y | X]. A similar formalism can be used when conditioning on a sequence of random
variables X1, . . . , Xn. The conditional expectation has similar properties to the ordinary
expectation. Other useful properties (see, for example, [127]) are:
1. Tower property: If EY exists, then
E E[Y | X] = EY.
(C.19)
2. Taking out what is known: If EY exists, then
E [XY | X] = XE[Y | X].
C.6
Functions of Random Variables
Let x = [x1, . . . , xn]⊤be a column vector in Rn and A an m×n matrix. The mapping x 7→z,
with z = Ax, is a linear transformation, as discussed in Section A.1. Now consider a
☞355
random vector X = [X1, . . . , Xn]⊤and let Z := AX. Then Z is a random vector in Rm. The
following theorem details how the distribution of Z is related to that of X.
432
Functions of Random Variables
Theorem C.3: Linear Transformation
If X has an expectation vector µX and covariance matrix ΣX, then the expectation
vector of Z is
µZ = A µX
(C.20)
and the covariance matrix of Z is
ΣZ = A ΣX A⊤.
(C.21)
If, in addition, A is an invertible n × n matrix and X is a continuous random vector
with pdf fX, then the pdf of the continuous random vector Z = AX is given by
fZ(z) = fX(A−1z)
| det(A)| , z ∈Rn,
(C.22)
where | det(A)| denotes the absolute value of the determinant of A.
Proof: We have µZ = EZ = E[AX] = A EX = AµX and
ΣZ
=
E[(Z −µZ)(Z −µZ)⊤] = E[A(X −µX)(A(X −µX))⊤]
=
A E[(X −µX)(X −µX)⊤]A⊤
=
A ΣX A⊤.
For A invertible and X continuous (as opposed to discrete), let z = Ax and x = A−1z.
Consider the n-dimensional cube C = [z1, z1 + h] × · · · × [zn, zn + h]. Then,
P[Z ∈C] ≈hn fZ(z),
by definition of the joint density of Z. Let D be the image of C under A−1 — that is, 

all
points x such that Ax ∈C. Recall from Section A.1 that any matrix B linearly transforms an
☞355
n-dimensional rectangle with volume V into an n-dimensional parallelepiped with volume
V | det(B)|. Thus, in addition to the above expression for P[Z ∈C], we also have
P[Z ∈C] = P[X ∈D] ≈hn| det(A−1)| fX(x) = hn| det(A)|−1 fX(x).
Equating these two expressions for P[Z ∈C], dividing both sides by hn, and letting h go to
0, we obtain (C.22).
□
For a generalization of the linear transformation rule (C.22), consider an arbitrary map-
ping x 7→g(x), written out:

x1
x2
...
xn

7→

g1(x)
g2(x)
...
gn(x)

.
Probability and Statistics
433
Theorem C.4: Transformation Rule
Let X be an n-dimensional vector of continuous random variables with pdf fX. Let
Z = g(X), where g is an invertible mapping with inverse g−1 and matrix of Jacobi
Jg; that is, the matrix of partial derivatives of g. Then, at z = g(x) the random vector
Z has pdf
fZ(z) =
fX(x)
| det(Jg(x))| = fX(g−1(z)) | det(Jg−1(z))|, z ∈Rn.
(C.23)
Proof: For a fixed x, let z = g(x); and thus x = g−1(z). In the neighborhood of x, the
function g behaves like a linear function, in the sense that g(x + δ) ≈g(x) + Jg(x) δ for
small vectors δ; see also Section B.1. Consequently, an infinitesimally small n-dimensional
☞397
rectangle at x with volume V is transformed into an infinitesimally small n-dimensional
parallelepiped at z with volume V | det(Jg(x))|. Now, as in the proof of the linear case, let
C be a small cube around z = g(x) with volume hn. Let D be the image of C under g−1.
Then,
hn fZ(z) ≈P[Z ∈C] ≈hn| det(Jg−1(z))| fX(x),
and since | det(Jg−1(z))| = 1/| det(Jg(x))|, (C.23) follows as h goes to 0.
□
Typically, in coordinate transformations it is g−1 that is given — that is, an expres-
sion for x as a function of z.
Example C.2 (Polar Transform) Suppose X, Y are independent and have standard nor-
mal distribution. The joint pdf is
fX,Y(x, y) = 1
2πe−1
2(x2

+y2),
(x, y) ∈R2.
In polar coordinates we have
X = R cos Θ
and
Y = R sin Θ,
(C.24)
where R ⩾0 is the radius and Θ ∈[0, 2π) the angle of the point (X, Y). What is the joint pdf
of R and Θ? By the radial symmetry of the bivariate normal distribution, we would expect
Θ to be uniform on (0, 2π). But what is the pdf of R? To work out the joint pdf, consider
the inverse transformation g−1, defined by
"r
θ
#
g−1
7−→
"r cos θ
r sin θ
#
=
"x
y
#
.
The corresponding matrix of Jacobi is
Jg−1(r, θ) =
"cos θ
−r sin θ
sin θ
r cos θ
#
,
434
Multivariate Normal Distribution
which has determinant r. Since x2 + y2 = r2(cos2 θ + sin2 θ) = r2, it follows by the trans-
formation rule (C.23) that the joint pdf of R and Θ is given by
fR,Θ(r, θ) = fX,Y(x, y) r = 1
2πe−1
2 r2 r,
θ ∈(0, 2π),
r ⩾0.
By integrating out θ and r, respectively, we find fR(r) = r e−r2/2 and fΘ(θ) = 1/(2π). Since
fR,Θ is the product of fR and fΘ, the random variables R and Θ are independent.
C.7
Multivariate Normal Distribution
The normal (or Gaussian) distribution — especially its multidimensional version — plays
a central role in data science and machine learning. Recall from Table C.1 that a random
variable X is said to have a normal
normal
distribution
distribution with parameters µ and σ2 if its pdf is given
by
f(x) =
1
σ
√
2π
e−1
2(
x−µ
σ )
2
,
x ∈R.
(C.25)
We write X ∼N(µ, σ2). The parameters µ and σ2 are the expectation and variance of the
distribution, respectively. If µ = 0 and σ = 1 then
f(x) =
1
√
2π
e−x2/2,
and the distribution is known as the standard normal
standard
normal
distribution. The cdf of the standard
normal distribution is often denoted by Φ and its pdf by φ. In Figure C.5 the pdf of the
N(µ, σ2) distribution for various µ and σ2 is plotted.
-4
-2
0
2
4
6
0
0.2
0.4
0.6
0.8
N(0,1)
N(0,1/4)
N(2,1)
Figure C.5: The pdf of the N(µ, σ2) distribution for various µ and σ2.
We next consider some important properties of the normal distribution.
Theorem C.5: Standardization
Let X ∼N(µ, σ2) and define Z

 = (X −µ)/σ. Then Z has a standard normal distribu-
tion.
Probability and Statistics
435
Proof: The cdf of Z is given by
P[Z ⩽z] = P[(X −µ)/σ ⩽z] = P[X ⩽µ + σz]
=
Z µ+σz
−∞
1
σ
√
2π
e−1
2(
x−µ
σ )
2
dx =
Z z
−∞
1
√
2π
e−y2/2dy = Φ(z),
where we make a change of variable y = (x −µ)/σ in the fourth equation. Hence, Z ∼
N(0, 1).
□
The rescaling procedure in Theorem C.5 is called standardization
standardization
. It follows from The-
orem C.5 that any X ∼N(µ, σ2) can be written as
X = µ + σZ,
where Z ∼N(0, 1).
In other words, any normal random variable can be viewed as an affine transformation
affine
transformation
—
that is, a linear transformation plus a constant — of a standard normal random variable.
We now generalize this to n dimensions. Let Z1, . . . , Zn be independent and standard
normal random variables. The joint pdf of Z = [Z1, . . . , Zn]⊤is given by
fZ(z) =
n
Y
i=1
1
√
2π
e−1
2 z2
i = (2π)−n
2 e−1
2 z⊤z,
z ∈Rn.
(C.26)
We write Z ∼N(0, I), where I is the identity matrix. Consider the affine transformation
X = µ + B Z
(C.27)
for some m × n matrix B and m-dimensional vector µ. Note that, by (C.20) and (C.21), X
☞432
has expectation vector µ and covariance matrix Σ = BB⊤. We say that X has a multivariate
normal
multivariate
normal
or multivariate Gaussian distribution with mean vector µ and covariance matrix Σ.
We write X ∼N(µ, Σ).
The following theorem states that any affine combination of independent multivariate
normal random variables is again multivariate normal.
Theorem C.6: Affine Transformation of Normal Random Vectors
Let X1, X2, . . . , Xr be independent mi-dimensional normal random vectors, with
Xi ∼N(µi, Σi), i = 1, . . . , r. Then, for any n × 1 vector a and n × mi matrices
B1, . . . , Br,
a +
rX
i=1
Bi Xi ∼N

a +
rX
i=1
Bi µi,
rX
i=1
Bi Σi B⊤
i

.
(C.28)
Proof: Denote the n-dimensional random vector in the left-hand side of (C.28) by Y. By
definition, each Xi can be written as µi +AiZi, where the {Zi} are independent (because the
{Xi} are indepe

ndent), so that
Y = a +
rX
i=1
Bi (µi + AiZi) = a +
rX
i=1
Bi µi +
rX
i=1
BiAiZi,
436
Multivariate Normal Distribution
which is an affine combination of independent standard normal random vectors. Hence, Y
is multivariate normal. Its expectation vector and covariance matrix can be found easily
from Theorem C.3.
□
☞432
The next theorem shows that the distribution of a subvector of a multivariate normal
random vector is again normal.
Theorem C.7: Marginal Distributions of Normal Random Vectors
Let X ∼N(µ, Σ) be an n-dimensional normal random vector. Decompose X, µ, and
Σ as
X =
"Xp
Xq
#
,
µ =
"µp
µq
#
,
Σ =
"Σp
Σr
Σ⊤
r
Σq
#
,
(C.29)
where Σp is the upper left p × p corner of Σ and Σq is the lower right q × q corner of
Σ. Then, Xp ∼N(µp, Σp).
Proof:
We give a proof assuming that Σ is positive definite. Let BB⊤be the (lower)
Cholesky decomposition of Σ. We can write
☞373
"Xp
Xq
#
=
"µp
µq
#
+
"Bp
O
Cr
Cq
#
|     {z     }
B
"Zp
Zq
#
,
(C.30)
where Zp and Zq are independent p- and q-dimensional standard normal random vectors.
In particular, Xp = µp + BpZp, which means that Xp ∼N(µp, Σp), since BpB⊤
p = Σp.
□
By relabeling the elements of X we see that Theorem C.7 implies that any subvector of
X has a multivariate normal distribution. For example, Xq ∼N(µq, Σq).
The following theorem shows that not only the marginal distributions of a normal ran-
dom vector are normal, but also its conditional distributions.
Theorem C.8: Conditional Distributions of Normal Random Vectors
Let X ∼N(µ, Σ) be an n-dimensional normal random vector with det(Σ) > 0. If X
is decomposed as in (C.29), then

Xq | Xp = xp

∼N(µq + Σ⊤
r Σ−1
p (xp −µp), Σq −Σ⊤
r Σ−1
p Σr).
(C.31)
As a consequence, Xp and Xq are independent if and only if they are uncorrelated;
that is, if Σr = O (zero matrix).
Proof: From (C.30) we see that Xp = µp+BpZp and Xq = µq+CrZp+CqZq. Consequently,
(Xq | Xp = xp) = µq + Cr B−1
p (xp −µp) + CqZq,
where Zq is a q-dimensional multivariate standard normal random vector. It follows

 that
Xq conditional on Xp = xp has a N(µq + Cr B−1
p (xp −µp), CqC⊤
q ) distribution. The proof of
Probability and Statistics
437
(C.31) is completed by observing that Σ⊤
r Σ−1
p = CrB⊤
p(B⊤
p)−1B−1
p = Cr B−1
p , and
Σq −Σ⊤
r Σ−1
p Σr = CrC⊤
r + CqC⊤
q −CrB−1
p
Σr
|{z}
BpC⊤r
= CqC⊤
q .
If Xp and Xq are independent, then they are obviously uncorrelated, as Σr = E[(Xp −
µp)(Xq −µq)⊤] = E(Xp −µp) E(Xq −µq)⊤= O. Conversely, if Σr = O, then by (C.31) the
conditional distribution of Xq given Xp is the same as the unconditional distribution of Xq;
that is, N(µq, Σq). In other words, Xq is independent of Xp.
□
The next few results are about the relationships between the normal, chi-squared,
χ2 distribution
Student, and F distributions, defined in Table C.1. Recall that the chi-squared family of
distributions, denoted by χ2
n, are simply Gamma(n/2, 1/2) distributions, where the para-
meter n ∈{1, 2, 3, . . .} is called the degrees of freedom.
Theorem C.9: Relationship Between Normal and χ2 Distributions
If X ∼N(µ, Σ) is an n-dimensional normal random vector with det(Σ) > 0, then
(X −µ)⊤Σ−1(X −µ) ∼χ2
n.
(C.32)
Proof: Let BB⊤be the Cholesky decomposition of Σ, where B is invertible. Since X can
be written as µ + BZ, where Z = [Z1, . . . , Zn]⊤is a vector of independent standard normal
random variables, we have
(X −µ)⊤Σ−1(X −µ) = (X −µ)⊤(BB⊤)−1(X −µ) = Z⊤Z =
n
X
i=1
Z2
i .
Using the independence of Z1, . . . , Zn, the moment generating function of Y = Pn
i=1 Z2
i is
☞427
given by
E esY = E es(Z2
1+···+Z2
n) = E [esZ2
1 · · · esZ2
n] =

E esZ2n ,
where Z ∼N(0, 1). The moment generating function of Z2 is
E esZ2 =
Z ∞
−∞
esz2
1
√
2π
e−z2/2dz =
1
√
2π
Z ∞
−∞
e−1
2 (1−2s)z2dz =
1
√
1 −2s
,
so that E esY =

1
2/( 1
2 −s)
 n
2, s < 1
2, which is the moment generating function of the
Gamma(n/2, 1/2) distribution; that is, the χ2
n distribution — see Example C.1. The res-
☞427
ult now follows from the uniqueness of the moment generating function.
□
A consequence of Theorem C.9 is 

that if X = [X1, . . . , Xn]⊤is n-dimensional standard
normal, then the squared length ∥X∥2 = X2
1 + · · · + X2
n has a χ2
n distribution. If instead Xi ∼
N(µi, 1), i = 1, . . ., then ∥X∥2 is said to have a noncentral χ2
n distribution
noncentral χ2
n
distribution
. This distribution
depends on the {µi} only through the norm ∥µ∥. We write ∥X∥2 ∼χ2
n(θ), where θ = ∥µ∥is
the noncentrality parameter
noncentrality
parameter
.
Such distributions frequently occur when considering projections of multivariate nor-
mal random variables, as summarized in the following theorem.
438
Multivariate Normal Distribution
Theorem C.10: Relationship Between Normal and Noncentral χ2 Distributions
Let X ∼N(µ, In) be an n-dimensional normal random vector and let Vk ⊂Vm be
linear subspaces of dimensions k and m, respectively, with k < m ⩽n. Let Xk and
Xm be orthogonal projections of X onto Vk and Vm, and let µk and µm be the cor-
responding projections of µ. Then, the following holds.
1. The random vectors Xk, Xm −Xk, and X −Xm are independent.
2. ∥Xk∥2 ∼χ2
k(∥µk∥), ∥Xm −Xk∥2 ∼χ2
m−k(∥µm −µk∥), and ∥X −Xm∥2 ∼χ2
n−m(∥µ −
µm∥).
Proof: Let v1, . . . , vn be an orthonormal basis of Rn such that v1, . . . , vk spans Vk and
v1, . . . , vm spans Vm. By (A.8) we can write the orthogonal projection matrices onto V j,
☞362
as Pj = Pj
i=1 viv⊤
i , j = k, m, n, where Vn is defined as Rn. Note that Pn is simply the iden-
tity matrix. Let V := [v1, . . . , vn] and define Z := [Z1, . . . , Zn]⊤= V⊤X. Recall from Sec-
tion A.2 that any orthogonal transformation such as z = V⊤x is length preserving; that is,
☞361
∥z∥= ∥x∥.
To prove the first statement of the theorem, note that V⊤X j = V⊤P jX = [Z1, . . . , Z j,
0, . . . , 0]⊤, j = k, m. It follows that V⊤(Xm −Xk) = [0, . . . , 0, Zk+1, . . . , Zm, 0, . . . , 0]⊤and
V⊤(X −Xm) = [0, . . . , 0, Zm+1, . . . , Zn]⊤. Moreover, being a linear transformation of a nor-
mal random vector, Z is also normal, with covariance matrix V⊤V = In. In particular, the
{Zi} are

 independent. This shows that Xk, Xm −Xk and X −Xm are independent as well.
Next, observe that ∥Xk∥= ∥V⊤Xk∥= ∥Zk∥, where Zk := [Z1, . . . , Zk]⊤. The latter vector
has independent components with variances 1, and its squared norm has therefore (by
definition) a χ2
k(θ) distribution. The noncentrality parameter is θ = ∥EZk∥= ∥EXk∥= ∥µk∥,
again by the length-preserving property of orthogonal transformations. This shows that
∥Xk∥2 ∼χ2
k(∥µk∥). The distributions of ∥Xm −Xk∥2 and ∥X −Xm∥2 follow by analogy.
□
Theorem C.10 is frequently used in the statistical analysis of normal linear models; see
Section 5.4. In typical situations µ lies in the subspace Vm or even Vk — in which case
☞182
∥Xm −Xk∥2 ∼χ2
m−k and ∥X −Xm∥2 ∼χ2
n−m, independently. The (scaled) quotient then turns
out to have an F distribution — a consequence of the following theorem.
Theorem C.11: Relationship Between χ2 and F Distributions
Let U ∼χ2
m and V ∼χ2
n be independent. Then,
U/m
V/n ∼F(m, n).
Proof: For notational simplicity, let c = m/2 and d = n/2. The pdf of W = U/V is
given by fW(w) =
R ∞
0
fU(wv) v fV(v) dv. Substituting the pdfs of the corresponding Gamma
Probability and Statistics
439
distributions, we have
fW(w)
=
Z ∞
0
(wv)c−1 e−wv/2
Γ(c) 2c
v vd−1e−v/2
Γ(d) 2d dv =
wc−1
Γ(c) Γ(d) 2c+d
Z ∞
0
vc+d−1 e−(1+w)v/2 dv
=
Γ(c + d)
Γ(c) Γ(d)
wc−1
(1 + w)c+d ,
where the last equality follows from the fact that the integrand is equal to Γ(α)λ−α times
the density of the Gamma(α, λ) distribution with α = c + d and λ = (1 + w)/2. The density
of Z = n
m
U
V is given by
fZ(z) = fW(z m/n) m/n.
The proof is completed by comparing the resulting expression with the pdf of the F distri-
bution given in Table C.1.
□
☞425
Corollary C.1 (Relationship Between Normal, χ2, and t Distributions) Let Z ∼N(0, 1)
and V ∼χ2
n be independent. Then,
Z
√V/n
∼tn.
Proof: Let T = Z/√V/n. Because Z2 ∼χ2
1, we have by Theorem C.11 that T 2 ∼F(1, n).
The result follows now from the symmetry around 0 of the pdf of T and the fact tha

t the
square of a tn random variable has an F(1, n) distribution.
□
C.8
Convergence of Random Variables
Recall that a random variable X is a function from Ωto R. If we have a sequence of random
variables X1, X2, . . . (for instance, Xn(ω) = X(ω)+ 1
n for each ω ∈Ω), then one can consider
the pointwise convergence:
lim
n→∞Xn(ω) = X(ω),
for all ω ∈Ω,
in which case we say that X1, X2, . . . converges surely
sure
convergence
to X. A more interesting type of
convergence uses the probability measure P associated with X.
Definition C.1: Convergence in Probability
The sequence of random variables X1, X2, . . . converges in probability to a random
variable X if, for all ε > 0,
lim
n→∞P [|Xn −X| > ε] = 0.
We denote the convergence in probability
convergence in
probability
as Xn
P
−→X.
440
Convergence of Random Variables
Convergence in probability refers only to the distribution of Xn. Instead, if the sequence
X1, X2, . . . is defined on a common probability space, then we can consider the following
mode of convergence that uses the joint distribution of the sequence of random variables.
Definition C.2: Almost Sure Convergence
The sequence of random variables X1, X2, . . . converges almost surely to a random
variable X if for every ε > 0
lim
n→∞P
"
sup
k⩾n
|Xk −X| > ε
#
= 0.
We denote the almost sure convergence
almost sure
convergence
as Xn
a.s.
−→X.
Note that in accordance with these definitions Xn
a.s.
−→0 is equivalent to supk⩾n |Xk|
P
−→0.
Example C.3 (Convergence in Probability Versus Almost Sure Convergence) Since
the event {|Xn −X| > ε} is contained in {supk⩾n |Xk −X| > ε}, we can conclude that almost
sure convergence implies convergence in probability. However, the converse is not true in
general. For instance, consider the iid sequence X1, X2, . . . with marginal distribution
P[Xn = 1] = 1 −P[Xn = 0] = 1/n.
Clearly, Xn
P
−→0. However, for ε < 1 and any n = 1, 2, . . . we have,
P
"
sup
k⩾n
|Xk| ⩽ε
#
= P[Xn ⩽ε, Xn+1 ⩽ε, . . .]
= P[Xn ⩽ε] × P[Xn+1 ⩽ε] × · · · (using ind

ependence)
= lim
m→∞
m
Y
k=n
P[Xk ⩽ε] = lim
m→∞
m
Y
k=n
 
1 −1
k
!
= lim
m→∞
n −1
n
×
n
n + 1 × · · · × m −1
m
= 0.
It follows that P[supk⩾n |Xk −0| > ε] = 1 for any 0 < ε < 1 and all n ⩾1. In other words, it
is not true that Xn
a.s.
−→0.
Another important type of convergence is useful when we are interested in estimating
expectations or multidimensional integrals via Monte Carlo methodology.
☞67
Definition C.3: Convergence in Distribution
The sequence of random variables X1, X2, . . . is said to converge in distribution to a
random variable X with distribution function FX(x) = P[X ⩽x] provided that:
lim
n→∞P[Xn ⩽x] = FX(x) for all x such that lim
a→x FX(a) = FX(x).
(C.33)
We denote the convergence in distribution
convergence in
distribution
as either Xn
d
−→X, or Xn
d
−→FX.
Probability and Statistics
441
The generalization to random vectors replaces (C.33) with
lim
n→∞P[Xn ∈A] = P[X ∈A] for all A ⊂Rn such that P[X ∈∂A] = 0,
(C.34)
where ∂A denotes the boundary of the set A.
A useful tool for demonstrating convergence in distribution is the characteristic func-
tion ψX of a random vector X, defined as the expectation:
characteristic
function
☞225
ψX(t) := E eit⊤X,
t ∈Rn.
(C.35)
The moment generating function in (C.5) is a special case of the characteristic function
evaluated at t = −is. Note that while the moment generating function of a random variable
may not exist, its characteristic function always exists. The characteristic function of a
random vector X ∼f is closely related to the Fourier transform of its pdf f.
☞390
Example C.4 (Characteristic Function of a Multivariate Gaussian Random Vector)
The density of the multivariate standard normal distribution is given in (C.26) and thus the
characteristic function of Z ∼N(0, In) is
ψZ(t) = E eit⊤Z = (2π)−n/2
Z
Rn eit⊤z−1
2∥z∥2dz
= e−∥t∥2/2(2π)−n/2
Z
Rn e−1
2 ∥z−it⊤∥2dz = e−∥t∥2/2,
t ∈Rn.
Hence, the characteristic function of the random vector X = µ + BZ in (C.27) with mul-
tivariate normal distribution N(µ, Σ) is g

iven by
☞435
ψX(t) = E eit⊤X = E eit⊤(µ+BZ)
= eit⊤µE ei(B⊤t)⊤Z = eit⊤µψZ(B⊤t)
= eit⊤µ−∥B⊤t∥2/2 = eit⊤µ−t⊤Σt/2.
The importance of the characteristic function is mainly derived from the following
result, for which a proof can be found, for example, in [11].
Theorem C.12: Characteristic Function
Suppose that ψX1(t), ψX2(t), . . . are the characteristic functions of the sequence of
random vectors X1, X2, . . . and ψX(t) is the characteristic function of X. Then, the
following three statements are equivalent:
1. limn→∞ψXn(t) = ψX(t) for all t ∈Rn.
2. Xn
d
−→X.
3. limn→∞Eh(Xn) = Eh(X) for all bounded continuous functions h : Rd 7→R.
442
Convergence of Random Variables
Example C.5 (Convergence in Distribution) Define the random variables Y1, Y2, . . .
as
Yn :=
n
X
k=1
Xk
 1
2
!k
,
n = 1, 2, . . . ,
where X1, X2, . . .
iid∼Ber(1/2). We now show that Yn
d
−→U(0, 1). First, note that
E exp(itYn) =
n
Y
k=1
E exp(itXk/2k) = 2−n
n
Y
k=1
(1 + exp(it/2k)).
Second, from the collapsing product, (1 −exp(it/2n)) Qn
k=1(1 + exp(it/2k)) = 1 −exp(it),
we have
E exp(itYn) = (1 −exp(it))
1/2n
1 −exp(it/2n).
It follows that limn→∞E exp(itYn) = (exp(it) −1)/(it), which we recognize as the charac-
teristic function of the U(0, 1) distribution.
☞441
Yet another mode of convergence is the following.
Definition C.4: Convergence in Lp-norm
The sequence of random variables X1, X2, . . . converges in Lp-norm to a random
variable X if
lim
n→∞E|Xn −X|p = 0,
p ⩾1.
We denote the convergence in Lp-norm
convergence in
Lp-norm
as Xn
Lp
−→X.
The case for p = 2 corresponds to convergence in mean squared error. The following
example illustrates that convergence in Lp-norm is qualitatively different from convergence
in distribution.
Example C.6 (Comparison of Modes of Convergence) Define Xn := 1 −X, where X
has a uniform distribution on the interval (0,1). Clearly, Xn
d
−→U(0, 1). However, E|Xn −
X| −→E|1 −2X| = 1/2 and so the sequence does not converge in L1-norm. In addition,
P[|Xn −X| > ε] −→1 −ε , 0 and s

o Xn does not converge in probability as well.
Thus, in general Xn
d
−→X implies neither Xn
P
−→X, nor Xn
L1
−→X.
We mention, however, that if Xn
d
−→c for some constant c, then Xn
P
−→c as well. To
see this, note that Xn
d
−→c stands for
lim
n→∞P[Xn ⩽x] =

1,
x > c
0,
x < c .
In other words, we can write:
P[|Xn −c| > ε] ⩽1 −P[Xn ⩽c + ε] + P[Xn ⩽c −ε] −→1 −1 + 0 = 0,
n →∞,
which shows that Xn
P
−→c by definition.
Probability and Statistics
443
Definition C.5: Complete Convergence
The sequence of random variables X1, X2, . . . is said to converge completely to X if
for all ε > 0
X
n
P[|Xn −X| > ε] < ∞.
We denote the complete convergence
complete
convergence
as Xn
cpl.
−→X.
Example C.7 (Complete and Almost Sure Convergence) We show that complete
convergence implies almost sure convergence. We can bound the criterion for almost sure
convergence as follows:
P[sup
k⩾n
|Xk −X| > ε] = P[∪k⩾n{|Xk −X| > ε}]
⩽
X
k⩾n
P[|Xk −X| > ε]
by union bound in (C.1)
⩽
∞
X
k=1
P[|Xk −X| > ε]
|                  {z                  }
= c<∞from Xn
cpl.
−→X
−
n−1
X
k=1
P[|Xk −X| > ε]
⩽c −
n−1
X
k=1
P[|Xk −X| > ε] −→c −c = 0,
n →∞.
Hence, by definition Xn
a.s.
−→X.
The next theorem shows how the different types of convergence are related to each
other. For example, in the diagram below, the notation
p⩾q
⇒means that Lp-norm convergence
implies Lq-norm convergence under the assumption that p ⩾q ⩾1.
Theorem C.13: Modes of Convergence
The most general relationships among the various modes of convergence for numer-
ical random variables are shown on the following hierarchical diagram:
Xn
cpl.
−→X
⇒
Xn
a.s.
−→X
⇓
Xn
P
−→X ⇒Xn
d
−→X
⇑
Xn
Lp
−→X
p⩾q
⇒Xn
Lq
−→X
.
444
Convergence of Random Variables
Proof: 1. First, we show that Xn
P
−→X ⇒Xn
d
−→X using the inequality P[A ∩B] ⩽P[A]
for any event B. To this end, consider the distribution function FX of X:
FXn(x) = P[Xn ⩽x] = P[Xn ⩽x, |Xn −X| > ε] + P[Xn ⩽x, |Xn −X| ⩽ε]
⩽P[|Xn −X| > ε] + P[Xn ⩽x, X ⩽Xn + ε]
⩽P[|Xn −X| > ε] + P[X ⩽x + ε].
Now, in th

e arguments above we can switch the roles of Xn and X (there is a symmetry) to
deduce the analogous result: FX(x) ⩽P[|X −Xn| > ε] + P[Xn ⩽x + ε]. Therefore, making
the switch x →x −ε gives FX(x −ε) ⩽P[|X −Xn| > ε] + FXn(x). Putting it all together
gives:
FX(x −ε) −P[|X −Xn| > ε] ⩽FXn(x) ⩽P[|Xn −X| > ε] + FX(x + ε).
Taking n →∞on both sides yields for any ε > 0:
FX(x −ε) ⩽lim
n→∞FXn(x) ⩽FX(x + ε).
Since FX is continuous at x by assumption we can take ε ↓0 to conclude that
limn→∞FXn(x) = FX(x).
2. Second, we show that Xn
Lp
−→X ⇒Xn
Lq
−→X for p ⩾q ⩾1. Since the function
f(x) = xq/p is concave for q/p ⩽1, Jensen’s inequality yields:
☞63
(E|X|p)q/p = f(E|X|p) ⩾E f(|X|p) = E|X|q.
In other words, (E|Xn−X|q)1/q ⩽(E|Xn−X|p)1/p −→0, proving the statement of the theorem.
3. Third, we show that Xn
L1
−→X ⇒Xn
P
−→X. First note that for any random variable
Y, we can write: E|Y| ⩾E[|Y| 1{|Y|>ε}] ⩾E[|ε| 1{|Y|>ε}] = ε P[|Y| > ε]. Therefore, we obtain
Chebyshev’s inequality
Chebyshev’s
inequality
:
P[|Y| > ε] ⩽E|Y|
ε .
(C.36)
Using Chebyshev’s inequality and Xn
L1
−→X, we can write
P[|Xn −X| > ε] ⩽E|Xn −X|
ε
−→0,
n →∞.
Hence, by definition Xn
P
−→X.
4. Finally, Xn
cpl.
−→X ⇒Xn
a.s.
−→X ⇒Xn
P
−→X is proved in Examples C.7 and C.3.
□
Finally, we will make use of the following theorem.
Theorem C.14: Slutsky
Let g(x, y) be a continuous scalar function of vectors x and y. Suppose that Xn
d
−→X
and Yn
P
−→c for some finite constant c. Then,
g(Xn, Yn)
d
−→g(X, c).
Probability and Statistics
445
Proof: We prove the theorem for scalar X and Y. The proof for random vectors is analog-
ous. First, we show that Zn :=
"Xn
Yn
#
d
−→
"X
c
#
=: Z using, for example, Theorem C.12. In
☞441
other words, we wish to show that the characteristic function of the joint distribution of Xn
and Yn converges pointwise as n →∞:
ψXn,Yn(t) = E ei(t1Xn+t2Yn) −→eit2cE eit1X = ψX,c(t),
∀t ∈R2.
To show the limit above, consider
|ψXn,Yn(t) −ψX,c(t)| ⩽|ψXn,c(t) −ψX,c(t)| + |ψXn,Yn(t) −ψXn,c(t)|
= |eit2c E (eit1Xn −eit1X

)| + |E ei(t1Xn+t2c)(eit2(Yn−c) −1)|
⩽|eit2c| × |E(eit1Xn −eit1X)| + E |ei(t1Xn+t2c)| × |eit2(Yn−c) −1|
⩽|ψXn(t1) −ψX(t1)| + E |eit2(Yn−c) −1|.
Since Xn
d
−→X, Theorem C.12 implies that ψXn(t1) −→ψX(t1), and the first term |ψXn(t1) −
ψX(t1)| goes to zero. For the second term we use the fact that
|eix −1| =

R x
0 i eiθ dθ
 ⩽

R x
0 |i eiθ| dθ
 = |x|,
x ∈R
to obtain the bound:
E|eit2(Yn−c) −1| = E|eit2(Yn−c) −1|1{|Yn−c|>ε} + E|eit2(Yn−c) −1|1{|Yn−c|⩽ε}
⩽2E 1{|Yn−c|>ε} + E|t2(Yn −c)|1{|Yn−c|⩽ε}
⩽2P[|Yn −c| > ε] + |t2|ε −→|t2|ε,
n →∞.
Since ε is arbitrary, we can let ε ↓0 to conclude that limn→∞|ψXn,Yn(t)−ψX,c(t)| = 0. In other
words, Zn
d
−→Z, and by the continuity of g, we have g(Zn)
d
−→g(Z) or g(Xn, Yn)
d
−→
g(X, c).
□
Example C.8 (Necessity of Slutsky’s Condition) The condition that Yn converges in
probability to a constant cannot be relaxed. For example, suppose that g(x, y) = x + y,
Xn
d
−→X ∼N(0, 1) and Yn
d
−→Y ∼N(0, 1). Then, our intuition tempts us to incorrectly
conclude that Xn + Yn
d
−→N(0, 2). This intuition is false, because we can have Yn = −Xn
for all n so that Xn + Yn = 0, while both X and Y have the same marginal distribution (in
this case standard normal).
C.9
Law of Large Numbers and Central Limit Theorem
Two main results in probability are the law of large numbers and the central limit theorem.
Both are limit theorems involving sums of independent random variables. In particular,
consider a sequence X1, X2, . . . of iid random variables with finite expectation µ and finite
variance σ2. For each n define Xn := (X1+· · ·+Xn)/n. What can we say about the (random)
sequence of averages X1, X2, X3, . . .? By (C.12) and (C.14) we have E Xn = µ and Var Xn =
☞429
σ2/n. Hence, as n increases, the variance of the (random) average Xn goes to 0. This means
446
Law of Large Numbers and Central Limit Theorem
that by Definition C.8, the average Xn converges to µ in L2-norm as n →∞, that is, Xn
L2
−→
µ.
In fact, to obtain convergence in probability th

e variance need not be finite — it is
sufficient to assume that µ = EX < ∞.
Theorem C.15: Weak Law of Large Numbers
law of large
numbers
If X1, . . . , Xn are iid with finite expectation µ, then for all ε > 0
lim
n→∞P
h
|Xn −µ| > ε
i
= 0.
In other words, Xn
P
−→µ.
The theorem has a natural generalization for random vectors. Namely, if µ = EX < ∞,
then P
h
∥Xn −µ∥> ε
i
→0, where ∥· ∥is the Euclidean norm. We give a proof in the scalar
☞355
case.
Proof: Let Zk := Xk −µ for all k, so that EZ = 0. We thus need to show that Zn
P
−→0.
We use the properties of the characteristic function of Z denoted as ψZ. Due to the iid
☞441
assumption, we have
ψZn(t) = E eitZn = E
n
Y
i=1
eitZi/n =
n
Y
i=1
E eiZit/n =
n
Y
i=1
ψZ(t/n) = [ψZ(t/n)]n.
(C.37)
An application of Taylor’s Theorem B.1 in the neighborhood of t = 0 yields
ψZ(t/n) = ψZ(0) + o(t/n).
Since ψZ(0) = 1, we have:
ψZn(t) = [ψZ(t/n)]n = [1 + o(1/n)]n −→1,
n →∞.
The characteristic function of a random variable that always equals zero is 1. Therefore,
Theorem C.12 implies that Zn
d
−→0. However, according to Example C.6, convergence in
distribution to a constant implies convergence in probability. Hence, Zn
P
−→0.
□
There is also a stronger version of this theorem, as follows.
Theorem C.16: Strong Law of Large Numbers
strong law of
large numbers
If X1, . . . , Xn are iid with expectation µ and EX2 < ∞, then for all ε > 0
lim
n→∞P
"
sup
k⩾n
|Xk −µ| > ε
#
= 0.
In other words, Xn
a.s.
−→µ.
Probability and Statistics
447
Proof: First, note that any random variable X can be written as the difference of two non-
negative random variables: X = X+ −X−, where X+ := max{X, 0} and X−:= −min{X, 0}.
Thus, without loss of generality, we assume that the random variables in the theorem above
are nonnegative.
Second, from the sequence {X1, X2, X3, . . .} we can pick up the subsequence {X1, X4, X9,
X16, . . .} =: {X j2}. Then, from Chebyshev’s inequality (C.36) and the iid condition, we have
∞
X
j=1
P
hX j2 −µ
 > ε
i
⩽Var X
ε2
∞
X
j=1
1


j2 < ∞.
Therefore, by definition Xn2
cpl.
−→µ and from Theorem C.13 we conclude that Xn2
a.s.
−→µ.
Third, for any arbitrary n, we can find a k, say k = ⌊√n⌋, so that k2 ⩽n ⩽(k + 1)2. For
such a k and nonnegative X1, X2, . . ., it holds that
k2
(k + 1)2 Xk2 ⩽Xn ⩽X(k+1)2 (k + 1)2
k2
.
Since Xk2 and X(k+1)2 converge almost surely to µ as k (and hence n) goes to infinity, we
conclude that Xn
a.s.
−→µ.
□
Note that the condition EX2 < ∞in Theorem C.16 can be weakened to E |X| < ∞and
the iid condition on the variables X1, . . . , Xn can be relaxed to mere pairwise independence.
The corresponding proof, however, is significantly more difficult.
The Central Limit Theorem
Central Limit
Theorem
describes the approximate distribution of Xn, and it applies
to both continuous and discrete random variables. Loosely, it states that
the average of a large number of iid random variables
approximately has a normal distribution.
Specifically, the random variable Xn has a distribution that is approximately normal, with
expectation µ and variance σ2/n.
Theorem C.17: Central Limit Theorem
If X1, . . . , Xn are iid with finite expectation µ and finite variance σ2, then for all
x ∈R,
lim
n→∞P

Xn −µ
σ/√n ⩽x
= Φ(x),
where Φ is the cdf of the standard normal distribution.
Proof: Let Zk := (Xk −µ)/σ for all k, so that E Z = 0 and E Z2 = 1. We thus need to show
that √n Zn
d
−→N(0, 1). We again use the properties of the characteristic function. Let ψZ
☞441
be the characteristic function of an iid copy of Z, then due to the iid assumption a similar
calculation to the one in (C.37) yields:
ψ √n Zn(t) = E eit √n Zn = [ψZ(t/√n)]n.
448
Law of Large Numbers and Central Limit Theorem
An application of Taylor’s Theorem B.1 in the neighborhood of t = 0 yields
ψZ(t/√n) = 1 +
t√nψ′
Z(0) + t2
2nψ′′
Z(0) + o(t2/n).
Since ψ′
Z(0) = E d
dteitZ t=0 = i EZ = 0 and ψ′′
Z(0) = i2 EZ2 = −1, we have:
ψ√n Zn(t) =
h
ψZ(t/√n)
in =
"
1 −t2
2n + o(1/n)
#n
−→e−t2/2,
n →∞.
From Example C.4, we recognize e−t

2/2 as the characteristic function of the standard normal
distribution. Thus, from Theorem C.12 we conclude that √n Zn
d
−→N(0, 1).
□
Figure C.6 shows the central limit theorem in action. The left part shows the pdfs of
X1, 2X2, . . . , 4X4 for the case where the {Xi} have a U[0, 1] distribution. The right part
shows the same for the Exp(1) distribution. In both cases, we clearly see convergence to a
bell-shaped curve, characteristic of the normal distribution.
0
1
2
3
4
0
0.2
0.4
0.6
0.8
1
0
2
4
6
8
0
0.2
0.4
0.6
0.8
1
Figure C.6: Illustration of the central limit theorem for (left) the uniform distribution and
(right) the exponential distribution.
The multivariate version of the central limit theorem is the basis for many asymptotic
(in the size of the training set) results in machine learning and data science.
Theorem C.18: Multivariate Central Limit Theorem
Let X1, . . . , Xn be iid random vectors with expectation vector µ and finite covariance
matrix Σ. Define Xn := (X1 + · · · + Xn)/n. Then,
√n (Xn −µ)
d
−→N(0, Σ)
as n →∞.
One application is as follows. Suppose that a parameter of interest, θ∗, is the unique
solution of the system of equations E ψ(X | θ∗) = 0, where ψ is a vector-valued (or multi-
valued) function and the distribution of X does not depend on θ. An M-estimator
M-estimator
of θ∗,
Probability and Statistics
449
denoted bθn, is the solution to the system of equations that results from approximating the
expectation with respect to X using an average of n iid copies of X:
ψn(θ) := 1
n
n
X
i=1
ψ(Xi | θ).
Thus, ψn(bθn) = 0.
Theorem C.19: M-estimator
The M-estimator is asymptotically normal as n →∞:
√n (bθn −θ∗)
d
−→N(0, A−1BA−⊤),
(C.38)
where A := −E ∂ψ
∂θ (X | θ∗)
☞398
and B := E ψ(X | θ∗) ψ(X | θ∗)⊤ is the covariance matrix
of ψ(X | θ∗).
Proof: We give a proof under the simplifying assumption3 that bθn is a unique root, that is,
for any θ and ε, there exists a δ > 0 such that ∥bθn −θ∥> ε implies that ∥ψn(θ)∥> δ.
First, we argue that bθn
P
−→θ∗; th

at is, P[∥bθn −θ∗∥> ε] →0. From the multivariate
extension of Theorem C.15, we have that
ψn(θ∗)
P
−→E ψn(θ∗) = E ψ(X | θ∗) = 0.
Therefore, using the uniqueness ofbθn, we can show thatbθn
P
−→θ∗via the bound:
P
h
∥bθn −θ∗∥> ε
i
⩽P
h
∥ψn(θ∗)∥> δ
i
= P
h
∥ψn(θ∗) −E ψn(θ∗)∥> δ
i
→0,
n →∞.
Second, we take a Taylor expansion of each component of the vector ψn(bθn) around θ∗to
obtain:
ψn(bθn) = ψn(θ∗) + Jn(θ′)(bθn −θ∗),
where Jn(θ) is the Jacobian of ψn at θ, and θ′ lies on the line segment joining bθn and θ∗.
Rearrange the last equation and multiply both sides by √n A−1 to obtain:
−A−1Jn(θ′)√n (bθn −θ∗) = A−1√n ψn(θ∗).
By the central limit theorem, √n ψ(θ∗) converges in distribution to N(0, B). Therefore,
−A−1Jn(θ′)√n (bθn −θ∗)
d
−→N(0, A−1BA−⊤).
Theorem C.15 (the weak law of large numbers) applied to the iid random matrices
{ ∂
∂θ ψ(Xi | θ)} shows that
Jn(θ)
P
−→E ∂
∂θ ψ(X | θ).
Moreover, sincebθn
P
−→θ∗and Jn is continuous in θ, we have that Jn(θ′)
P
−→−A. Therefore,
by Slutsky’s theorem, −A−1Jn(θ′)√n (bθn −θ∗) −√n (bθn −θ∗)
P
−→0.
□
☞444
3The result holds under far less stringent assumptions.
450
Law of Large Numbers and Central Limit Theorem
Finally, we mention Laplace’s approximation
Laplace’s
approximation
, which shows how integrals or expecta-
tions behave under the normal distribution with a vanishingly small variance.
Theorem C.20: Laplace’s Approximation
Suppose that θn →θ∗, where θ∗lies in the interior of the open set Θ ⊆Rp and that
Σn is a p × p covariance matrix such that Σn →Σ∗. Let g : Θ 7→R be a continuous
function with g(θ∗) , 0. Then, as n →∞,
np/2
Z
Θ
g(θ) e−n
2 (θ−θn)⊤Σ−1
n (θ−θn) dθ →g(θ∗)
p
|2π Σ∗|.
(C.39)
Proof: (Sketch for a bounded domain Θ.) The left-hand side of (C.39) can be written as
the expectation with respect to the N(θn, Σn/n) distribution:
p
|2π Σn|
Z
Θ
g(θ)
exp

−n
2(θ −θn)⊤Σ−1
n (θ −θn)

|2π Σn/n|1/2
dθ =
p
|2π Σn| E[g(Xn)1{Xn ∈Θ}],
where Xn ∼N(θn, Σn/n). Let Z ∼N(0, I). Then, θn + Σ1/2
n Z/√n has the same distribution
as Xn and

θ

n + Σ1/2
n Z/√n

→θ∗as n →∞. By continuity of g(θ)1{θ ∈Θ} in the interior
of Θ, as n →∞:4
E[g(Xn)1{Xn ∈Θ}] = E

g

θn + Σ1/2
n
Z
√n

1

θn + Σ1/2
n
Z
√n

∈Θ

−→g(θ∗)1{θ∗∈Θ}.
Since θ∗lies in the interior of Θ, we have 1{θ∗∈Θ} = 1, completing the proof.
□
As an application of Theorem C.20 we can show the following.
Theorem C.21: Approximation of Integrals
Suppose that r : θ 7→R is twice continuously differentiable with a unique global
minimum at θ∗and g : θ 7→R is continuous with g(θ∗) > 0. Then, as n →∞,
ln
Z
Rp g(θ) e−n r(θ) dθ ≃−n r(θ∗) −p
2 ln n.
(C.40)
More generally, if rn has a unique global minimum θn and rn →r ⇒θn →θ∗, then
ln
Z
Rp g(θ) e−n rn(θ) dθ ≃−n r(θ∗) −p
2 ln n.
Proof: We only sketch the proof of (C.40). Let H(θ) be the Hessian matrix of r at θ. By
☞400
Taylor’s theorem we can write
r(θ) −r(θ∗) = (θ −θ∗)⊤∂r(θ∗)
∂θ
| {z }
= 0
+1
2(θ −θ∗)⊤H(θ)(θ −θ∗),
4We can exchange the limit and expectation, as g(θ)1{θ ∈Θ} ⩽maxθ∈Θ g(θ) and
R
Θ maxθ∈Θ g(θ) dθ =
|Θ| maxθ∈Θ g(θ) < ∞.
Probability and Statistics
451
where θ is a point that lies on the line segment joining θ∗and θ. Since θ∗is a unique
global minimum, there must be a small enough neighborhood of θ∗, say Θ, such that r is a
strictly (also known as strongly) convex function on Θ. In other words, H(θ) is a positive
☞403
definite matrix for all θ ∈Θ and there exists a smallest positive eigenvalue λ1 > 0 such
that x⊤H(θ)x ⩾λ1∥x∥2 for all x. In addition, since the maximum eigenvalue of H(θ) is a
continuous function of θ ∈Θ and Θ is bounded, there must exist a constant λ2 > λ1 such
that x⊤H(θ)x ⩽λ2∥x∥2 for all x. In other words, denoting r∗:= r(θ∗), we have the bounds:
−λ2
2 ∥θ −θ∗∥2 ⩽−(r(θ) −r∗) ⩽−λ1
2 ∥θ −θ∗∥2,
θ ∈Θ.
Therefore,
e−n r∗Z
Θ
g(θ) e−nλ2
2 ∥θ−θ∗∥2dθ ⩽
Z
Θ
g(θ) e−n r(θ)dθ ⩽e−n r∗Z
Θ
g(θ) e−nλ1
2 ∥θ−θ∗∥2dθ.
An application of Theorem C.20 yields
R
Θ g(θ) e−n r(θ) dθ = O(e−n r∗/np/2) and, more import-
antly,
ln
Z
Θ
g(θ) e−n r(θ) dθ ≃−n r∗−p
2 ln n.
Thus, the proof will be complete once we show th

at
R
Θ g(θ) e−n r(θ) dθ, with Θ := Rp \ Θ, is
asymptotically negligible compared to
R
Θ g(θ) e−n r(θ) dθ. Since θ∗is a global minimum that
lies outside any neighborhood of Θ, there must exists a constant c > 0 such that r(θ)−r∗> c
for all θ ∈Θ. Therefore,
Z
Θ
g(θ) e−n r(θ) dθ = e−(n−1) r∗Z
Θ
g(θ) e−r(θ) e−(n−1)(r(θ)−r∗) dθ
⩽e−(n−1) r∗Z
Θ
g(θ) e−r(θ) e−(n−1)c dθ
⩽e−(n−1)(r∗+c)
Z
Rp g(θ) e−r(θ) dθ = O(e−n(r∗+c)).
The last expression is of order o(e−n r∗/np/2), concluding the proof.
□
C.10
Markov Chains
Definition C.6: Markov Chain
A Markov chain
Markov chain
is a collection {Xt, t = 0, 1, 2, . . .} of random variables (or ran-
dom vectors) whose futures are conditionally independent of their pasts given their
present values. That is,
P[Xt+1 ∈A | Xs, s ⩽t] = P[Xt+1 ∈A | Xt]
for all t.
(C.41)
In other words, the conditional distribution of the future variable Xt+1, given the entire
past {Xs, s ⩽t}, is the same as the conditional distribution of Xt+1 given only the present Xt.
Property (C.41) is called the Markov property
Markov
property
.
452
Markov Chains
The index t in Xt is usually seen as a “time” or “step” parameter. The index set
{0, 1, 2, . . .} in the definition above was chosen out of convenience. It can be replaced by any
countable index set. We restrict ourselves to time-homogeneous
time-
homogeneous
Markov chains — Markov
chains for which the conditional pdfs fXt+1 | Xt(y | x) do not depend on t; we abbreviate these
as q(y | x). The {q(y | x)} are called the (one-step) transition densities
transition
density
of the Markov chain.
Note that the random variables or vectors {Xt} may be discrete (e.g., taking values in some
set {1, . . . , r}) or continuous (e.g., taking values in an interval [0, 1] or Rd). In particular, in
the discrete case, each q(y | x) is a probability: q(y | x) = P[Xt+1 = y | Xt = x].
The distribution of X0 is called the initial distribution
initial
distribution
of the Markov chain. The one-
step transition densities and the initial distrib

ution completely specify the distribution of
the random vector [X0, X1, . . . , Xt]⊤. Namely, we have by the product rule (C.17) and the
☞431
Markov property that the joint pdf is given by
fX0,...,Xt(x0, . . . , xt) = fX0(x0) fX1 | X0(x1 | x0) · · · fXt | Xt−1,...,X0(xt | xt−1, . . . , x0)
= fX0(x0) fX1 | X0(x1 | x0) · · · fXt | Xt−1(xt | xt−1)
= fX0(x0) q(x1 | x0) q(x2 | x1) · · · q(xt | xt−1).
A Markov chain is said to be ergodic
ergodic
if the probability distribution of Xt converges to
a fixed distribution as t →∞. Ergodicity is a property of many Markov chains. Intuitively,
the probability of encountering the Markov chain in a state x at a time t far into the future
should not depend on the t, provided that the Markov chain can reach every state from any
other state — such Markov chains are said to be irreducible — and does not “escape” to
infinity. Thus, for an ergodic Markov chain the pdf fXt(x) converges to a fixed limiting pdf
limiting pdf
f(x) as t →∞, irrespective of the starting state. For the discrete case, f(x) corresponds to
the long-run fraction of times that the Markov process visits x.
Under mild conditions (such as irreducibility) the limiting pdf f(x) can be found by
solving the global balance equations
global balance
equations
:
f(x) =

P
y f(y) q(x | y)
(discrete case),
R
f(y) q(x | y) dy
(continuous case).
(C.42)
For the discrete case the rationale behind this is as follows. Since f(x) is the long-run
proportion of time that the Markov chain spends in x, the proportion of transitions out of
x is f(x). This should be balanced with the proportion of transitions into state x, which is
P
y f(y) q(x | y).
One is often interested in a stronger type of balance equations. Imagine that we have
taken a video of the evolution of the Markov chain, which we may run in forward and
reverse time. If we cannot determine whether the video is running forward or backward
(we cannot determine any systematic “looping”, which would indicate in which direction


time is flowing), the chain is said to be time-reversible or simply reversible
reversible
.
Although not every Markov chain is reversible, each ergodic Markov chain, when run
backwards, gives another Markov chain — the reverse Markov chain
reverse
Markov chain
— with transition
densities eq(y | x) = f(y) q(x | y)/ f(x). To see this, first observe that f(x) is the long-run
proportion of time spent in x for both the original and reverse Markov chain. Secondly,
the “probability flux” from x to y in the reversed chain must be equal to the probability
flux from y to x in the original chain, meaning f(x)eq(y | x) = f(y) q(x | y), which yields the
Probability and Statistics
453
stated transition probabilities for the reversed chain. In particular, for a reversible Markov
chain we have
f(x) q(y | x) = f(y) q(x | y)
for all x, y.
(C.43)
These are the detailed (or local) balance equations. Note that the detailed balance equa-
local balance
equations
tions imply the global balance equations. Hence, if a Markov chain is irreducible and there
exists a pdf such that (C.43) holds, then f(x) must be the limiting pdf. In the discrete state
space case an additional condition is that the chain must be aperiodic
aperiodic
, meaning that the
return times to the same state cannot always be a multiple of some integer ⩾2.
Example C.9 (Random Walk on a Graph) Consider a Markov chain that performs a
“random walk” on the graph in Figure C.7, at each step jumping from the current vertex
(node) to one of the adjacent vertices, with equal probability. Clearly this Markov chain is
reversible. It is also irreducible and aperiodic. Let f(x) denote the limiting probability that
the chain is in vertex x. By symmetry, f(1) = f(2) = f(7) = f(8), f(4) = f(5) and f(3) =
f(6). Moreover, by the detailed balance equations, f(4)/5 = f(1)/3, and f(3)/4 = f(1)/3.
It follows that f(1) + · · · + f(8) = 4f(1) + 2 × 5/3 f(1) + 2 × 4/3 f(1) = 10 f(1) = 1, so
that f(1) = 1/10, f(3) = 2/15, and f(4) = 1/6.
1
2
3
4
5


6
7
8
Figure C.7: The random walk on this graph is reversible.
C.11
Statistics
Statistics deals with the gathering, summarization, analysis, and interpretation of data. The
two main branches of statistics are:
1. Classical or frequentist statistics
frequentist
statistics
: Here the observed data τ is viewed as the out-
come of random data T described by a probabilistic model — usually the model is
specified up to a (multidimensional) parameter; that is, T ∼g(· | θ) for some θ. The
statistical inference is then purely concerned with the model and in particular with
the parameter θ. For example, on the basis of the data one may wish to
(a) estimate the parameter,
(b) perform statistical tests on the parameter, or
454
Estimation
(c) validate the model.
2. Bayesian statistics
Bayesian
statistics
: In this approach we average over all possible values of the
parameter θ using a user-specified weight function g(θ) and obtain the model
T ∼
R
g(· | θ) g(θ) dθ. For practical computations, this means that we can treat θ as a
random variable with pdf g(θ). Bayes’ formula g(θ | τ) ∝g(τ | θ) g(θ) is used to learn
☞48
θ based on the observed data τ.
Example C.10 (Iid Sample) The most fundamental statistical model is where the data
T = X1, . . . , Xn is such that the random variables X1, . . . , Xn are assumed to be independent
and identically distributed:
X1, . . . , Xn
iid∼Dist,
according to some known or unknown distribution Dist. An iid sample is often called a
random sample
random sample
in the statistics literature. Note that the word “sample” can refer to both a
collection of random variables and to a single random variable. It should be clear from the
context which meaning is being used.
Often our guess or model for the true distribution is specified up to an unknown para-
meter θ, with θ ∈Θ. The most common model is:
X1, . . . , Xn
iid∼N(µ, σ2),
in which case θ = (µ, σ2) and Θ = R × R+.
C.12
Estimation
Suppose the model g(· | θ) for the data T is completely specified up t

o an unknown para-
meter vector θ. The aim is to estimate θ on the basis of the observed data τ only (an altern-
ative goal could be to estimate η = ψ(θ) for some vector-valued function ψ). Specifically,
the goal is to find an estimator T = T(T ) that is close to the unknown θ. The correspond-
estimator
ing outcome t = T(τ) is the estimate of θ. The bias of an estimator T of θ is defined as
estimate
bias
ET −θ. An estimator T of θ is said to be unbiased if EθT = θ. We often write bθ for both
an estimator and estimate of θ. The mean squared error (MSE) of a real-valued estimator
mean squared
error
T is defined as
MSE = Eθ(T −θ)2.
An estimator T1 is said to be more efficient
efficient
than an estimator T2 if the MSE of T1 is smaller
than the MSE of T2. The MSE can be written as the sum
MSE = (EθT −θ)2 + VarθT.
The first term measures the unbiasedness and the second is the variance of the estimator.
In particular, for an unbiased estimator the MSE of an estimator is simply equal to its
variance.
For simulation purposes it is often important to include the running time of the estim-
ator in efficiency comparisons. One way to compare two unbiased estimators T1 and T2 is
to compare their relative time variance products
relative time
variance
products
,
ri Var Ti
(E Ti)2 ,
i = 1, 2,
(C.44)
Probability and Statistics
455
where r1 and r2 are the times required to calculate the estimators T1 and T2, respectively.
In this scheme, T1 is considered more efficient than T2 if its relative time variance product
is smaller. We discuss next two systematic approaches for constructing sound estimators.
C.12.1
Method of Moments
Suppose x1, . . . , xn are outcomes from an iid sample X1, . . . , Xn ∼iid g(x | θ), where θ =
[θ1, . . . , θk]⊤is unknown. The moments of the sampling distribution can be easily estim-
ated. Namely, if X ∼g(x | θ), then the r-th moment of X, that is µr(θ) = EθXr (assuming
it exists), can be estimated through the sample r-th moment: 1
n
Pn
i=1 xr
i. The method of

 mo-
sample r-th
moment
ments involves choosing the estimate bθ of θ such that each of the first k sample and true
method of
moments
moments are matched:
1
n
n
X
i=1
xr
i = µr(bθ),
r = 1, 2, . . . , k.
In general, this set of equations is nonlinear and so its solution often has to be found
numerically.
Example C.11 (Sample Mean and Sample Variance) Suppose the data is given by
T = {X1, . . . , Xn}, where the {Xi} form an iid sample from a general distribution with mean
µ and variance σ2 < ∞. Matching the first two moments gives the set of equations
1
n
n
X
i=1
xi = µ,
1
n
n
X
i=1
x2
i = µ2 + σ2.
The method of moments estimates for µ and σ2 are therefore the sample mean
sample mean
bµ = x = 1
n
n
X
i=1
xi,
(C.45)
and
c
σ2 = 1
n
n
X
i=1
x2
i −(x)2 = 1
n
n
X
i=1
(xi −x)2.
(C.46)
The corresponding estimator for µ, X, is unbiased. However, the estimator for σ2 is biased:
E c
σ2 = σ2(n −1)/n. An unbiased estimator is the sample variance
sample variance
S 2 = c
σ2
n
n −1 =
1
n −1
n
X
i=1
(Xi −X)2.
Its square root, S =
√
S 2, is called the sample standard deviation
sample
standard
deviation
.
Example C.12 (Sample Covariance Matrix) The method of moments can also be
used to estimate the covariance matrix of a random vector. In particular, let the X1, . . . , Xn
456
Estimation
be iid copies of a d-dimensional random vector X with mean vector µ and covari-
ance matrix Σ. We assume n ⩾d. The moment estimator for µ is, as in the d = 1 case,
X = (X1 + · · · + Xn)/n. As the covariance matrix can be written (see (C.15)) as
☞430
Σ = E(X −µ)(X −µ)⊤,
the method of moments yields the estimator
bΣ = 1
n
n
X
i=1
(Xi −X)(Xi −X)⊤.
(C.47)
Similar to the one-dimensional case (d = 1), replacing the factor 1/n with 1/(n −1) gives
an unbiased estimator, called the sample covariance matrix
sample
covariance
matrix
.
C.12.2
Maximum Likelihood Method
The concept of likelihood is central in statistics. It describes in a precise way the informa-
tion about model parameters that is contained in the o

bserved data.
Let T be a (random) data object that is modeled as a draw from the pdf g(τ | θ) (dis-
crete or continuous) with parameter vector θ ∈Θ. Let τ be an outcome of T . The function
L(θ | τ) := g(τ | θ), θ ∈Θ, is called the likelihood function of θ, based on τ. The (nat-
likelihood
function
ural) logarithm of the likelihood function is called the log-likelihood function and is often
log-likelihood
function
denoted by a lower case l.
Note that L(θ | τ) and g(τ | θ) have the same formula, but the first is viewed as a
function of θ for fixed τ, where the second is viewed as a function of τ for fixed θ.
The concept of likelihood is particularly useful when T is modeled as an iid sample
{X1, . . . , Xn} from some pdf ˚g. In that case, the likelihood of the data τ = {x1, . . . , xn}, as a
function of θ, is given by the product
L(θ | τ) =
n
Y
i=1
˚g(xi | θ).
(C.48)
Let τ be an observation from T ∼g(τ | θ), and suppose that g(τ | θ) takes its largest
value at θ = bθ. In a way this bθ is our best estimate for θ, as it maximizes the probability
(density) for the observation τ. It is called the maximum likelihood estimate (MLE) of θ.
Note thatbθ = bθ(τ) is a function of τ. The corresponding random variable, also denotedbθ is
the maximum likelihood estimator
maximum
likelihood
estimator
(also abbreviated as MLE).
Maximization of L(θ | τ) as a function of θ is equivalent (when searching for the max-
imizer) to maximizing the log-likelihood l(θ | τ), as the natural logarithm is an increasing
function. This is often easier, especially when T is an iid sample from some sampling
distribution. For example, for L of the form (C.48), we have
l(θ | τ) =
n
X
i=1
ln ˚g(xi | θ).
Probability and Statistics
457
If l(θ | τ) is a differentiable function with respect to θ and the maximum is attained in the
interior of Θ, and there exists a unique maximum point, then we can find the MLE of θ by
solving the equations
∂
∂θi
l(θ | τ) = 0,
i = 1, . . . , d.
Example C.13 (Bernoulli Random Samp

le) Suppose we have data τn = {x1, . . . , xn}
and assume the model X1, . . . , Xn ∼iid Ber(θ). Then, the likelihood function is given by
L(θ | τ) =
n
Y
i=1
θxi(1 −θ)1−xi = θs(1 −θ)n−s,
0 < θ < 1,
(C.49)
where s := x1 +· · ·+ xn =: nx. The log-likelihood is l(θ) = s ln θ+(n−s) ln(1−θ). Through
differentiation with respect to θ, we find the derivative
s
θ −n −s
1 −θ =
s
θ(1 −θ) −
n
1 −θ.
(C.50)
Solving l′(θ) = 0 gives the ML estimate bθ = x and ML estimator bθ = X.
C.13
Confidence Intervals
An essential part in any estimation procedure is to provide an assessment of the accuracy
of the estimate. Indeed, without information on its accuracy the estimate itself would be
meaningless. Confidence intervals (also called interval estimates
interval
estimates
) provide a precise way of
describing the uncertainty in the estimate.
Let X1, . . . , Xn be random variables with a joint distribution depending on a parameter
θ ∈Θ. Let T1 < T2 be statistics; that is, Ti = Ti(X1, . . . , Xn), i = 1, 2 are functions of the
data, but not of θ.
1. The random interval (T1, T2) is called a stochastic confidence interval
stochastic
confidence
interval
for θ with
confidence 1 −α if
Pθ[T1 < θ < T2] ⩾1 −α
for all θ ∈Θ.
(C.51)
2. If t1 and t2 are the observed values of T1 and T2, then the interval (t1, t2) is called the
(numerical) confidence interval
(numerical)
confidence
interval
for θ with confidence 1 −α for every θ ∈Θ.
3. If the right-hand side of (C.51) is merely a heuristic estimate or approximation of
the true probability, then the resulting interval is called an approximate confidence
interval.
4. The probability Pθ[T1 < θ < T2] is called the coverage probability
coverage
probability
. For a 1 −α
confidence interval, it must be at least 1 −α.
For multidimensional parameters θ ∈Rd the stochastic confidence interval is replaced
with a stochastic confidence region
confidence
region
C ⊂Rd such that Pθ[θ ∈C] ⩾1 −α for all θ.
458
Hypothesis Testing
Example C.14 (Approximate Confidence Interv

al for the Mean) Let X1, X2, . . . , Xn
be an iid sample from a distribution with mean µ and variance σ2 < ∞(both assumed
to be unknown). By the central limit theorem and the law of large numbers,
☞447
T = X −µ
S/√n
approx.
∼
N(0, 1),
for large n, where S is the sample standard deviation. Rearranging the approximate equality
P[|T| ⩽z1−α/2] ≈1 −α, where z1−α/2 is the 1 −α/2 quantile of the standard normal
distribution, yields
P
"
X −z1−α/2
S√n ⩽µ ⩽X + z1−α/2
S√n
#
≈1 −α,
so that
 
X −z1−α/2
S√n, X + z1−α/2
S√n
!
, abbreviated as X ± z1−α/2
S√n,
(C.52)
is an approximate stochastic (1 −α) confidence interval for µ.
Since (C.52) is an asymptotic result only, care should be taken when applying it to
cases where the sample size is small or moderate and the sampling distribution is heavily
skewed.
C.14
Hypothesis Testing
Suppose the model for the data T is described by a family of probability distributions that
depend on a parameter θ ∈Θ. The aim of hypothesis testing is to decide, on the basis of
hypothesis
testing
the observed data τ, which of two competing hypotheses holds true; these being the null
hypothesis, H0 : θ ∈Θ0, and the alternative hypothesis, H1 : θ ∈Θ1.
null hypothesis
alternative
hypothesis
In classical statistics the null hypothesis and alternative hypothesis do not play equival-
ent roles. H0 contains the “status quo” statement and is only rejected if the observed data
are very unlikely to have happened under H0.
The decision whether to accept or reject H0 is dependent on the outcome of a test
statistic
test statistic
T = T(T ). For simplicity, we discuss only the one-dimensional case T ≡T. Two
(related) types of decision rules are generally used:
1. Decision rule 1: Reject H0 if T falls in the critical region.
Here the critical region
critical region
is any appropriately chosen region in R. In practice a critical
region is one of the following:
• left one-sided: (−∞, c],
• right one-sided: [c, ∞),
• two-sided: (−∞, c1] ∪[c2, ∞).
For example, for a right

 one-sided test, H0 is rejected if the outcome of the test
statistic is too large. The endpoints c, c1, and c2 of the critical regions are called
critical values
critical values
.
Probability and Statistics
459
2. Decision rule 2: Reject H0 if the P-value is smaller than some significance level α.
The P-value
P-value
is the probability that, under H0, the (random) test statistic takes a value
as extreme as or more extreme than the one observed. In particular, if t is the observed
outcome of the test statistic T, then
• left one-sided test: P := PH0[T ⩽t],
• right one-sided: P := PH0[T ⩾t],
• two-sided: P := min{2PH0[T ⩽t], 2PH0[T ⩾t]}.
The smaller the P-value, the greater the strength of the evidence against H0 provided
by the data. As a rule of thumb:
P < 0.10
suggestive evidence,
P < 0.05
reasonable evidence,
P < 0.01
strong evidence.
Whether the first or the second decision rule is used, one can make two types of errors,
as depicted in Table C.4.
Table C.4: Type I and II errors in hypothesis testing.
True statement
Decision
H0 is true
H1 is true
Accept H0
Correct
Type II Error
Reject H0
Type I Error
Correct
The choice of the test statistic and the corresponding critical region involves a multiob-
jective optimization criterion, whereby both the probabilities of a type I and type II error
should, ideally, be chosen as small as possible. Unfortunately, these probabilities compete
with each other. For example, if the critical region is made larger (smaller), the probability
of a type II error is reduced (increased), but at the same time the probability of a type I
error is increased (reduced).
Since the type I error is considered more serious, Neyman and Pearson [93] suggested
the following approach: choose the critical region such that the probability of a type II error
is as small as possible, while keeping the probability of a type I error below a predetermined
small significance level
significance
level
α.
Remark C.3 (Equivalence of Decision Rules) Note that dec

ision rule 1 and 2 are
equivalent in the following sense:
Reject H0 if T falls in the critical region, at significance level α.
⇔
Reject H0 if the P-value is ⩽significance level α.
460
Hypothesis Testing
In other words, the P-value of the test is the smallest level of significance that would lead
to the rejection of H0.
In general, a statistical test involves the following steps:
1. Formulate an appropriate statistical model for the data.
2. Give the null (H0) and alternative (H1) hypotheses in terms of the parameters
of the model.
3. Determine the test statistic (a function of the data only).
4. Determine the (approximate) distribution of the test statistic under H0.
5. Calculate the outcome of the test statistic.
6. Calculate the P-value or the critical region, given a preselected significance
level α.
7. Accept or reject H0.
The actual choice of an appropriate test statistic is akin to selecting a good estimator
for the unknown parameter θ. The test statistic should summarize the information about θ
and make it possible to distinguish between the alternative hypotheses.
Example C.15 (Hypothesis Testing) We are given outcomes x1, . . . , xm and y1, . . . , yn
of two simulation studies obtained via independent runs, with m = 100 and n = 50. The
sample means and standard deviations are x = 1.3, sX = 0.1 and y = 1.5, sY = 0.3. Thus,
the {xi} are outcomes of iid random variables {Xi}, the {yi} are outcomes of iid random
variables {Yi}, and the {Xi} and {Yi} are independent. We wish to assess whether the expect-
ations µX = EXi and µY = EYi are the same or not. Going through the 7 steps above, we
have:
1. The model is already specified above.
2. H0 : µX −µY = 0 versus H1 : µX −µY , 0.
3. For similar reasons as in Example C.14, take
T =
X −Y
q
S 2
X/m + S 2
Y/n
.
4. By the central limit theorem, the statistic T has, under H0, approximately a standard
normal distribution (assuming the variances are finite).
5. The outcome of T is t = (x −y)/
q
s2
X/m + s2
Y/n ≈−4.59.
6. 

As this is a two-sided test, the P-value is 2PH0[T ⩽−4.59] ≈4 · 10−6.
7. Because the P-value is extremely small, there is overwhelming evidence that the two
expectations are not the same.
Probability and Statistics
461
Further Reading
Accessible treatises on probability and stochastic processes include [27, 26, 39, 54, 101].
Kallenberg’s book [61] provides a complete graduate-level overview of the foundations of
modern probability. Details on the convergence of probability measures and limit theorems
can be found in [11]. For an accessible introduction to mathematical statistics with simple
applications see, for example, [69, 74, 124]. For a more detailed overview of statistical
inference, see [10, 25]. A standard reference for classical (frequentist) statistical inference
is [78].
462
APPENDIXD
PYTHON PRIMER
Python has become the programming language of choice for many researchers and
practitioners in data science and machine learning. This appendix gives a brief intro-
duction to the language. As the language is under constant development and each year
many new packages are being released, we do not pretend to be exhaustive in this in-
troduction. Instead, we hope to provide enough information for novices to get started
with this beautiful and carefully thought-out language.
D.1
Getting Started
The main website for Python is
https://www.python.org/,
where you will find documentation, a tutorial, beginners’ guides, software examples, and
so on. It is important to note that there are two incompatible “branches” of Python, called
Python 3 and Python 2. Further development of the language will involve only Python 3,
and in this appendix (and indeed the rest of the book) we only consider Python 3. As there
are many interdependent packages that are frequently used with a Python installation, it
is convenient to install a distribution — for instance, the Anaconda
Anaconda
Python distribution,
available from
https://www.anaconda.com/.
The Anaconda installer automatically 

installs the most important packages and also
provides a convenient interactive development environment (IDE), called Spyder.
Use the Anaconda Navigator to launch Spyder, Jupyter notebook, install and update
packages, or open a command-line terminal.
To get started1, try out the Python statements in the input boxes that follow. You can
either type these statements at the IPython command prompt or run them as (very short)
1We assume that you have installed all the necessary files and have launched Spyder.
463
464
Getting Started
Python programs. The output for these two modes of input can differ slightly. For ex-
ample, typing a variable name in the console causes its contents to be automatically printed,
whereas in a Python program this must be done explicitly by calling the print function.
Selecting (highlighting) several program lines in Spyder and then pressing function key2
F9 is equivalent to executing these lines one by one in the console.
In Python, data is represented as an object
object
or relation between objects (see also Sec-
tion D.2). Basic data types are numeric types (including integers, booleans, and floats),
sequence types (including strings, tuples, and lists), sets, and mappings (currently, diction-
aries are the only built-in mapping type).
Strings are sequences of characters, enclosed by single or double quotes. We can print
strings via the print function.
print("Hello World!")
Hello World!
For pretty-printing output, Python strings can be formatted using the format function. The
bracket syntax {i} provides a placeholder for the i-th variable to be printed, with 0 being
the first index. Individual variables can be formatted separately and as desired; formatting
syntax is discussed in more detail in Section D.9.
☞476
print("Name:{1} (height {2} m, age {0})".format(111,"Bilbo",0.84))
Name:Bilbo (height 0.84 m, age 111)
Lists can contain different types of objects, and are created using square brackets as in the
following example:
x = [1,'string',

"another string"]
# Quote type is not important
[1, 'string ', 'another string ']
Elements in lists are indexed starting from 0, and are mutable
mutable
(can be changed):
x = [1,2]
x[0] = 2
# Note that the first index is 0
x
[2,2]
In contrast, tuples (with round brackets) are immutable
immutable
(cannot be changed). Strings are
immutable as well.
x = (1,2)
x[0] = 2
TypeError: 'tuple' object does not support item assignment
2This may depend on the keyboard and operating system.
Python Primer
465
Lists can be accessed via the slice
slice
notation [start:end]. It is important to note that end
is the index of the first element that will not be selected, and that the first element has index
0. To gain familiarity with the slice notation, execute each of the following lines.
a = [2, 3, 5, 7, 11, 13, 17, 19, 23]
a[1:4]
# Elements with index from 1 to 3
a[:4]
# All elements with index less than 4
a[3:]
# All elements with index 3 or more
a[-2:]
# The last two elements
[3, 5, 7]
[2, 3, 5, 7]
[7, 11, 13, 17, 19, 23]
[19, 23]
An operator
operator
is a programming language construct that performs an action on one or more
operands. The action of an operator in Python depends on the type of the operand(s). For
example, operators such as +, ∗, −, and % that are arithmetic operators when the operands
are of a numeric type, can have different meanings for objects of non-numeric type (such
as strings).
'hello' + 'world'
# String concatenation
'helloworld '
'hello' * 2
# String repetition
'hellohello '
[1,2] * 2
# List repetition
[1, 2, 1, 2]
15 % 4
# Remainder of 15/4
3
Some common Python operators are given in Table D.1.
☞468
D.2
Python Objects
As mentioned in the previous section, data in Python is represented by objects or relations
between objects. We recall that basic data types included strings and numeric types (such
as integers, booleans, and floats).
As Python is an object-oriented programming language, functions are objects too
(everything is an object!). Each object has an

 identity (unique to each object and immutable
— that is, cannot be changed — once created), a type (which determines which operations
can be applied to the object, and is considered immutable), and a value (which is either
466
Types and Operators
mutable or immutable). The unique identity assigned to an object obj can be found by
calling id, as in id(obj).
Each object has a list of attributes
attributes
, and each attribute is a reference to another object.
The function dir applied to an object returns the list of attributes. For example, a string
object has many useful attributes, as we shall shortly see. Functions are objects with the
__call__ attribute.
A class (see Section D.8) can be thought of as a template for creating a custom type of
object.
s = "hello"
d = dir(s)
print(d,flush=True)
# Print the list in "flushed" format
['__add__ ', '__class__ ', '__contains__ ', '__delattr__ ', '__dir__ ',
... (many left out) ... 'replace ', 'rfind',
'rindex ', 'rjust', 'rpartition ', 'rsplit ', 'rstrip ', 'split',
'splitlines ', 'startswith ', 'strip', 'swapcase ', 'title',
'translate ', 'upper', 'zfill ']
Any attribute attr of an object obj can be accessed via the dot notation
dot notation
: obj.attr. To
find more information about any object use the help function.
s = "hello"
help(s.replace)
replace(...) method of builtins.str instance
S.replace(old, new[, count]) -> str
Return a copy of S with all occurrences of substring
old replaced by new.
If the optional argument count is
given, only the first count occurrences are replaced.
This shows that the attribute replace is in fact a function. An attribute that is a function is
called a method
method
. We can use the replace method to create a new string from the old one
by changing certain characters.
s = 'hello'
s1 = s.replace('e','a')
print(s1)
hallo
In many Python editors, pressing the TAB key, as in objectname.<TAB>, will bring
up a list of possible attributes via the editor’s autocompletion feature.
D.3
Types and Ope

rators
Each object has a type
type
. Three basic data types in Python are str (for string), int (for
integers), and float (for floating point numbers). The function type returns the type of
Python Primer
467
an object.
t1 = type([1,2,3])
t2 = type((1,2,3))
t3 = type({1,2,3})
print(t1,t2,t3)
<class 'list'> <class 'tuple'> <class 'set'>
The assignment
assignment
operator, =, assigns an object to a variable; e.g., x = 12. An expression
is a combination of values, operators, and variables that yields another value or variable.
Variable names are case sensitive and can only contain letters, numbers, and under-
scores. They must start with either a letter or underscore. Note that reserved words
such as True and False are case sensitive as well.
Python is a dynamically typed language, and the type of a variable at a particular point
during program execution is determined by its most recent object assignment. That is, the
type of a variable does not need to be explicitly declared from the outset (as is the case in
C or Java), but instead the type of the variable is determined by the object that is currently
assigned to it.
It is important to understand that a variable in Python is a reference
reference
to an object —
think of it as a label on a shoe box. Even though the label is a simple entity, the contents
of the shoe box (the object to which the variable refers) can be arbitrarily complex. Instead
of moving the contents of one shoe box to another, it is much simpler to merely move the
label.
x = [1,2]
y = x
# y refers to the same object as x
print(id(x) == id(y))
# check that the object id's are the same
y[0] = 100
# change the contents of the list that y refers to
print(x)
True
[100,2]
x = [1,2]
y = x
# y refers to the same object as x
y = [100,2]
# now y refers to a different object
print(id(x) == id(y))
print(x)
False
[1,2]
Table D.1 shows a selection of Python operators for numerical and logical variables.
468
Functions and Methods
Table D.1: Common numerical (left) 

and logical (right) operators.
+
addition
~
binary NOT
-
subtraction
&
binary AND
*
multiplication
^
binary XOR
**
power
|
binary OR
/
division
==
equal to
//
integer division
!=
not equal to
%
modulus
Several of the numerical operators can be combined with an assignment operator, as in
x += 1 to mean x = x + 1. Operators such as + and * can be defined for other data types
as well, where they take on a different meaning. This is called operator overloading, an
example of which is the use of <List>␣*␣<Integer> for list repetition as we saw earlier.
D.4
Functions and Methods
Functions make it easier to divide a complex program into simpler parts. To create a
function
function
, use the following syntax:
def <function name>(<parameter_list>):
<statements>
A function takes a list of input variables that are references to objects. Inside the func-
tion, a number of statements are executed which may modify the objects, but not the ref-
erence itself. In addition, the function may return an output object (or will return the value
None if not explicitly instructed to return output). Think again of the shoe box analogy. The
input variables of a function are labels of shoe boxes, and the objects to which they refer
are the contents of the shoe boxes. The following program highlights some of the subtleties
of variables and objects in Python.
Note that the statements within a function must be indented. This is Python’s way to
define where a function begins and ends.
x = [1,2,3]
def change_list(y):
y.append(100) # Append an element to the list referenced by y
y[0]=0
# Modify the first element of the same list
y = [2,3,4]
# The local y now refers to a different list
# The list to which y first referred does not change
return sum(y)
print(change_list(x))
print(x)
9
[0, 2, 3, 100]
Python Primer
469
Variables that are defined inside a function only have local scope; that is, they are
recognized only within that function. This allows the same variable name to be used in
different fun

ctions without creating a conflict. If any variable is used within a function,
Python first checks if the variable has local scope. If this is not the case (the variable has
not been defined inside the function), then Python searches for that variable outside the
function (the global scope). The following program illustrates several important points.
from numpy import array, square, sqrt
x = array([1.2,2.3,4.5])
def stat(x):
n = len(x)
#the length of x
meanx = sum(x)/n
stdx = sqrt(sum(square(x - meanx))/n)
return [meanx,stdx]
print(stat(x))
[2.6666666666666665, 1.3719410418171119]
1. Basic math functions such as sqrt are unknown to the standard Python interpreter
and need to be imported. More on this in Section D.5 below.
2. As was already mentioned, indentation is crucial. It shows where the function begins
and ends.
3. No semicolons3 are needed to end lines, but the first line of the function definition
(here line 5) must end with a colon (:).
4. Lists are not arrays (vectors of numbers), and vector operations cannot be performed
on lists. However, the numpy module is designed specifically with efficient vec-
tor/matrix operations in mind. On the second code line, we define x as a vector
(ndarray) object. Functions such as square, sum, and sqrt are then applied to
such arrays. Note that we used the default Python functions len and sum. More on
numpy in Section D.10.
5. Running the program with stat(x) instead of print(stat(x)) in line 11 will not
show any output in the console.
To display the complete list of built-in functions, type (using double underscores)
dir(__builtin__) .
3Semicolons can be used to put multiple commands on a single line.
470
Modules
D.5
Modules
A Python module
module
is a programming construct that is useful for organizing code into
manageable parts. To each module with name module_name is associated a Python file
module_name.py containing any number of definitions, e.g., of functions, classes, and
variables, as well as executable statement

s. Modules can be imported into other programs
using the syntax: import␣<module_name>␣as␣<alias_name>, where <alias_name>
is a shorthand name for the module.
When imported into another Python file, the module name is treated as a namespace
namespace
,
providing a naming system where each object has its unique name. For example, different
modules mod1 and mod2 can have different sum functions, but they can be distinguished by
prefixing the function name with the module name via the dot notation, as in mod1.sum and
mod2.sum. For example, the following code uses the sqrt function of the numpy module.
import numpy as np
np.sqrt(2)
1.4142135623730951
A Python package is simply a directory of Python modules; that is, a collection of
modules with additional startup information (some of which may be found in its __path__
attribute). Python’s built-in module is called __builtins__. Of the great many useful
Python modules, Table D.2 gives a few.
Table D.2: A few useful Python modules/packages.
datetime
Module for manipulating dates and times.
matplotlib
MATLABTM-type plotting package
numpy
Fundamental package for scientific computing, including random
number generation and linear algebra tools. Defines the ubiquitous
ndarray class.
os
Python interface to the operating system.
pandas
Fundamental module for data analysis. Defines the powerful
DataFrame class.
pytorch
Machine learning library that supports GPU computation.
scipy
Ecosystem for mathematics, science, and engineering, containing
many tools for numerical computing, including those for integration,
solving differential equations, and optimization.
requests
Library for performing HTTP requests and interfacing with the web.
seaborn
Package for statistical data visualization.
sklearn
Easy to use machine learning library.
statsmodels
Package for the analysis of statistical models.
The numpy package contains various subpackages, such as random, linalg, and fft.
More details are given in Section D.10.
Python Primer
471
When

 using Spyder, press Ctrl+I in front of any object, to display its help file in a
separate window.
As we have already seen, it is also possible to import only specific functions from a
module using the syntax: from␣<module_name>␣import␣<fnc1,␣fnc2,␣...>.
from numpy import sqrt, cos
sqrt(2)
cos(1)
1.4142135623730951
0.54030230586813965
This avoids the tedious prefixing of functions via the (alias) of the module name. However,
for large programs it is good practice to always use the prefix/alias name construction, to
be able to clearly ascertain precisely which module a function being used belongs to.
D.6
Flow Control
Flow control in Python is similar to that of many programming languages, with conditional
statements as well as while and for loops. The syntax for if-then-else flow control is
as follows.
if <condition1>:
<statements>
elif <condition2>:
<statements>
else:
<statements>
Here, <condition1> and <condition2> are logical conditions that are either True or
False; logical conditions often involve comparison operators (such as ==,␣>,␣<=,␣!=).
In the example above, there is one elif part, which allows for an “else if” conditional
statement. In general, there can be more than one elif part, or it can be omitted. The else
part can also be omitted. The colons are essential, as are the indentations.
The while and for loops have the following syntax.
while <condition>:
<statements>
for <variable> in <collection>:
<statements>
Above, <collection> is an iterable object (see Section D.7 below). For further con-
trol in for and while loops, one can use a break statement to exit the current loop, and
the continue statement to continue with the next iteration of the loop, while abandoning
any remaining statements in the current iteration. Here is an example.
472
Iteration
import numpy as np
ans = 'y'
while ans != 'n':
outcome = np.random.randint(1,6+1)
if outcome == 6:
print("Hooray a 6!")
break
else:
print("Bad luck, a", outcome)
ans = input("Again? (y/n) ")
D.7
Iteration


Iterating over a sequence of objects, such as used in a for loop, is a common operation.
To better understand how iteration works, we consider the following code.
s = "Hello"
for c in s:
print(c,'*', end=' ')
H * e * l * l * o *
A string is an example of a Python object that can be iterated. One of the methods of a
string object is __iter__. Any object that has such a method is called an iterable
iterable
. Calling
this method creates an iterator
iterator
— an object that returns the next element in the sequence
to be iterated. This is done via the method __next__.
s = "Hello"
t = s.__iter__()
# t is now an iterator. Same as iter(s)
print(t.__next__() ) # same as next(t)
print(t.__next__() )
print(t.__next__() )
H
e
l
The inbuilt functions next and iter simply call these corresponding double-
underscore functions of an object. When executing a for loop, the sequence/collection
over which to iterate must be an iterable. During the execution of the for loop, an iterator
is created and the next function is executed until there is no next element. An iterator is
also an iterable, so can be used in a for loop as well. Lists, tuples, and strings are so-called
sequence
sequence
objects and are iterables, where the elements are iterated by their index.
The most common iterator in Python is the range
range
iterator, which allows iteration over
a range of indices. Note that range returns a range object, not a list.
for i in range(4,20):
print(i, end=' ')
print(range(4,20))
Python Primer
473
4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19
range(4,20)
Similar to Python’s slice operator [i : j], the iterator range(i, j) ranges from i to j,
not including the index j.
Two other common iterables are sets and dictionaries. Python sets
sets
are, as in mathem-
atics, unordered collections of unique objects. Sets are defined with curly brackets { }, as
opposed to round brackets ( ) for tuples, and square brackets [ ] for lists. Unlike lists, sets do
not have duplicate elements. Many of the u

sual set operations are implemented in Python,
including the union A␣|␣B and intersection A␣&␣B.
A = {3, 2, 2, 4}
B = {4, 3, 1}
C = A & B
for i in A:
print(i)
print(C)
2
3
4
{3, 4}
A useful way to construct lists is by list comprehension
list
comprehension
; that is, by expressions of the
form
<expression>␣for␣<element>␣in␣<list>␣if␣<condition>
For sets a similar construction holds. In this way, lists and sets can be defined using very
similar syntax as in mathematics. Compare, for example, the mathematical definition of
the sets A := {3, 2, 4, 2} = {2, 3, 4} (no order and no duplication of elements) and B := {x2 :
x ∈A} with the Python code below.
setA = {3, 2, 4, 2}
setB = {x**2 for x in setA}
print(setB)
listA = [3, 2, 4, 2]
listB = [x**2 for x in listA]
print(listB)
{16, 9, 4}
[9, 4, 16, 4]
A dictionary
dictionary
is a set-like data structure, containing one or more key:value pairs en-
closed in curly brackets. The keys are often of the same type, but do not have to be; the
same holds for the values. Here is a simple example, storing the ages of Lord of the Rings
characters in a dictionary.
474
Classes
DICT = {'Gimly': 140, 'Frodo':51, 'Aragorn': 88}
for key in DICT:
print(key, DICT[key])
Gimly 140
Frodo 51
Aragorn 88
D.8
Classes
Recall that objects are of fundamental importance in Python — indeed, data types and
functions are all objects. A class
class
is an object type, and writing a class definition can be
thought of as creating a template for a new type of object. Each class contains a number
of attributes, including a number of inbuilt methods. The basic syntax for the creation of a
class is:
class <class_name>:
def __init__(self):
<statements>
<statements>
The main inbuilt method is __init__, which creates an instance
instance
of a class object.
For example, str is a class object (string class), but s␣=␣str('Hello') or simply
s␣=␣'Hello', creates an instance, s, of the str class. Instance attributes are created dur-
ing initialization and their values may 

be different for different instances. In contrast, the
values of class attributes are the same for every instance. The variable self in the initializ-
ation method refers to the current instance that is being created. Here is a simple example,
explaining how attributes are assigned.
class shire_person:
def __init__(self,name): # initialization method
self.name = name
# instance attribute
self.age = 0
# instance attribute
address = 'The Shire'
# class attribute
print(dir(shire_person)[1:5],'...',dir(shire_person)[-2:])
# list of class attributes
p1 = shire_person('Sam')
# create an instance
p2 = shire_person('Frodo') # create another instance
print(p1.__dict__)
# list of instance attributes
p2.race = 'Hobbit'
# add another attribute to instance p2
p2.age = 33
# change instance attribute
print(p2.__dict__)
print(getattr(p1,'address'))
# content of p1's class attribute
['__delattr__ ', '__dict__ ', '__dir__ ', '__doc__ '] ...
['__weakref__ ', 'address ']
Python Primer
475
{'name': 'Sam', 'age': 0}
{'name': 'Frodo', 'age': 33, 'race': 'Hobbit '}
The Shire
It is good practice to create all the attributes of the class object in the __init__ method,
but, as seen in the example above, attributes can be created and assigned everywhere, even
outside the class definition. More generally, attributes can be added to any object that has
a __dict__.
An “empty” class can be created via
class␣<class_name>:
␣␣␣pass
Python classes can be derived from a parent class by inheritance
inheritance
, via the following
syntax.
class <class_name>(<parent_class_name>):
<statements>
The derived class (initially) inherits all of the attributes of the parent class.
As an example, the class shire_person below inherits the attributes name, age, and
address from its parent class person. This is done using the super function, used here
to refer to the parent class person without naming it explicitly. When creating a new
object of type shire_person, the __init__ method of the parent class is invoked, a

nd
an additional instance attribute Shire_address is created. The dir function confirms that
Shire_address is an attribute only of shire_person instances.
class person:
def __init__(self,name):
self.name = name
self.age = 0
self.address= ' '
class shire_person(person):
def __init__(self,name):
super().__init__(name)
self.Shire_address = 'Bag End'
p1 = shire_person("Frodo")
p2 = person("Gandalf")
print(dir(p1)[:1],dir(p1)[-3:] )
print(dir(p2)[:1],dir(p2)[-3:] )
['Shire_address '] ['address ', 'age', 'name']
['__class__ '] ['address ', 'age', 'name ']
476
Files
D.9
Files
To write to or read from a file, a file first needs to be opened. The open function in Python
creates a file object that is iterable, and thus can be processed in a sequential manner in a
for or while loop. Here is a simple example.
fout = open('output.txt','w')
for i in range(0,41):
if i%10 == 0:
fout.write('{:3d}\n'.format(i))
fout.close()
The first argument of open is the name of the file. The second argument specifies
if the file is opened for reading ('r'), writing ('w'), appending ('a'), and so on. See
help(open). Files are written in text mode by default, but it is also possible to write in
binary mode. The above program creates a file output.txt with 5 lines, containing the
strings 0, 10, ..., 40. Note that if we had written fout.write(i) in the fourth line of the
code above, an error message would be produced, as the variable i is an integer, and not a
string. Recall that the expression string.format() is Python’s way to specify the format
of the output string.
The formatting syntax {:3d} indicates that the output should be constrained to a spe-
cific width of three characters, each of which is a decimal value. As mentioned in the
introduction, bracket syntax {i} provides a placeholder for the i-th variable to be printed,
with 0 being the first index. The format for the output is further specified by {i:format},
where format is typically4 of the form:
[width][.precision][type]
In this specifi

cation:
• width specifies the minimum width of output;
• precision specifies the number of digits to be displayed after the decimal point for
a floating point values of type f, or the number of digits before and after the decimal
point for a floating point values of type g;
• type specifies the type of output. The most common types are s for strings, d for
integers, b for binary numbers, f for floating point numbers (floats) in fixed-point
notation, g for floats in general notation, e for floats in scientific notation.
The following illustrates some behavior of formatting on numbers.
'{:5d}'.format(123)
'{:.4e}'.format(1234567890)
'{:.2f}'.format(1234567890)
'{:.2f}'.format(2.718281828)
'{:.3f}'.format(2.718281828)
'{:.3g}'.format(2.718281828)
'{:.3e}'.format(2.718281828)
4More formatting options are possible.
Python Primer
477
'{0:3.3f}; {2:.4e};'.format(123.456789,
0.00123456789)
'
123'
'1.2346e+09'
'1234567890.00'
'2.72'
'2.718'
'2.72'
'2.718e+00'
'123.457; 1.2346e-03;'
The following code reads the text file output.txt line by line, and prints the output
on the screen. To remove the newline \n character, we have used the strip method for
strings, which removes any whitespace from the start and end of a string.
fin = open('output.txt','r')
for line in fin:
line = line.strip()
# strips a newline character
print(line)
fin.close()
0
10
20
30
40
When dealing with file input and output it is important to always close files. Files that
remain open, e.g., when a program finishes unexpectedly due to a programming error, can
cause considerable system problems. For this reason it is recommended to open files via
context management. The syntax is as follows.
with open('output.txt', 'w') as f:
f.write('Hi there!')
Context management ensures that a file is correctly closed even when the program is
terminated prematurely. An example is given in the next program, which outputs the most-
frequent words in Dicken’s A Tale of Two Cities, which can be downloaded from the book’s
GitH

ub site as ataleof2cities.txt.
Note that in the next program, the file ataleof2cities.txt must be placed in the cur-
rent working directory. The current working directory can be determined via import␣os
followed by cwd␣=␣os.getcwd().
numline = 0
DICT = {}
with open('ataleof2cities.txt', encoding="utf8") as fin:
for line in fin:
words = line.split()
for w in words:
if w not in DICT:
DICT[w] = 1
478
NumPy
else:
DICT[w] +=1
numline += 1
sd = sorted(DICT,key=DICT.get,reverse=True) #sort the dictionary
print("Number of unique words: {}\n".format(len(DICT)))
print("Ten most frequent words:\n")
print("{:8} {}".format("word", "count"))
print(15*'-')
for i in range(0,10):
print("{:8} {}".format(sd[i], DICT[sd[i]]))
Number of unique words: 19091
Ten most frequent words:
word
count
---------------
the
7348
and
4679
of
3949
to
3387
a
2768
in
2390
his
1911
was
1672
that
1650
I
1444
D.10
NumPy
The package NumPy (module name numpy) provides the building blocks for scientific
computing in Python. It contains all the standard mathematical functions, such as sin,
cos, tan, etc., as well as efficient functions for random number generation, linear algebra,
and statistical computation.
import numpy as np
#import the package
x = np.cos(1)
data = [1,2,3,4,5]
y = np.mean(data)
z = np.std(data)
print('cos(1) = {0:1.8f}
mean = {1}
std = {2}'.format(x,y,z))
cos(1) = 0.54030231
mean = 3.0
std = 1.4142135623730951
D.10.1
Creating and Shaping Arrays
The fundamental data type in numpy is the ndarray. This data type allows for fast matrix
operations via highly optimized numerical libraries such as LAPACK and BLAS; this in
Python Primer
479
contrast to (nested) lists. As such, numpy is often essential when dealing with large amounts
of quantitative data.
ndarray objects can be created in various ways. The following code creates a 2×3×2
array of zeros. Think of it as a 3-dimensional matrix or two stacked 3 × 2 matrices.
A = np.zeros([2,3,2])
# 2 by 3 by 2 array of zeros
print(A)
print(A.shape)
# num

ber of rows and columns
print(type(A))
# A is an ndarray
[[[ 0.
0.]
[ 0.
0.]
[ 0.
0.]]
[[ 0.
0.]
[ 0.
0.]
[ 0.
0.]]]
(2, 3, 2)
<class 'numpy.ndarray '>
We will be mostly working with 2D arrays; that is, ndarrays that represent ordinary
matrices. We can also use the range method and lists to create ndarrays via the array
method. Note that arange is numpy’s version of range, with the difference that arange
returns an ndarray object.
a = np.array(range(4))
# equivalent to np.arange(4)
b = np.array([0,1,2,3])
C = np.array([[1,2,3],[3,2,1]])
print(a, '\n', b,'\n' , C)
[0 1 2 3]
[0 1 2 3]
[[1 2 3]
[3 2 1]]
The dimension of an ndarray can be obtained via its shape method, which returns a
tuple. Arrays can be reshaped via the reshape method. This does not change the current
ndarray object. To make the change permanent, a new instance needs to be created.
a = np.array(range(9)) #a is an ndarray of shape (9,)
print(a.shape)
A = a.reshape(3,3)
#A is an ndarray of shape (3,3)
print(a)
print(A)
[0 1 2 3 4 5 6 7 8]
(9,)
[[0, 1, 2]
[3, 4, 5]
[6, 7, 8]]
480
NumPy
One shape dimension for reshape can be specified as −1. The dimension is then
inferred from the other dimension(s).
The 'T' attribute of an ndarray gives its transpose. Note that the transpose of a “vector”
with shape (n, ) is the same vector. To distinguish between column and row vectors, reshape
such a vector to an n × 1 and 1 × n array, respectively.
a = np.arange(3)
#1D array (vector) of shape (3,)
print(a)
print(a.shape)
b = a.reshape(-1,1) # 3x1 array (matrix) of shape (3,1)
print(b)
print(b.T)
A = np.arange(9).reshape(3,3)
print(A.T)
[0 1 2]
(3,)
[[0]
[1]
[2]]
[[0 1 2]]
[[0 3 6]
[1 4 7]
[2 5 8]]
Two useful methods of joining arrays are hstack and vstack, where the arrays are
joined horizontally and vertically, respectively.
A = np.ones((3,3))
B = np.zeros((3,2))
C = np.hstack((A,B))
print(C)
[[ 1.
1.
1.
0.
0.]
[ 1.
1.
1.
0.
0.]
[ 1.
1.
1.
0.
0.]]
D.10.2
Slicing
Arrays can be sliced similarly to Python lists. If an a

rray has several dimensions, a slice for
each dimension needs to be specified. Recall that Python indexing starts at '0' and ends
at 'len(obj)-1'. The following program illustrates various slicing operations.
A = np.array(range(9)).reshape(3,3)
print(A)
print(A[0])
# first row
print(A[:,1])
# second column
print(A[0,1])
# element in first row and second column
print(A[0:1,1:2])
# (1,1) ndarray containing A[0,1] = 1
print(A[1:,-1]) # elements in 2nd and 3rd rows, and last column
Python Primer
481
[[0 1 2]
[3 4 5]
[6 7 8]]
[0 1 2]
[1 4 7]
1
[[1]]
[5 8]
Note that ndarrays are mutable objects, so that elements can be modified directly, without
having to create a new object.
A[1:,1] = [0,0] # change two elements in the matrix A above
print(A)
[[0, 1, 2]
[3, 0, 5]
[6, 0, 8]]
D.10.3
Array Operations
Basic mathematical operators and functions act element-wise on ndarray objects.
x = np.array([[2,4],[6,8]])
y = np.array([[1,1],[2,2]])
print(x+y)
[[ 3,
5]
[ 8, 10]]
print(np.divide(x,y))
# same as x/y
[[ 2.
4.]
[ 3.
4.]]
print(np.sqrt(x))
[[1.41421356
2.
]
[2.44948974
2.82842712]]
In order to compute matrix multiplications and compute inner products of vectors,
numpy’s dot function can be used, either as a method of an ndarray instance or as a
method of np.
print(np.dot(x,y))
[[10, 10]
[22, 22]]
print(x.dot(x))
# same as np.dot(x,x)
482
NumPy
[[28, 40]
[60, 88]]
Since version 3.5 of Python, it is possible to multiply two ndarrays using the @
operator
@ operator
(which implements the np.matmul method). For matrices, this is similar to using
the dot method. For higher-dimensional arrays the two methods behave differently.
print(x @ y)
[[10 10]
[22 22]]
NumPy allows arithmetic operations on arrays of different shapes (dimensions). Spe-
cifically, suppose two arrays have dimensions (m1, m2, . . . , mp) and (n1, n2, . . . , np), respect-
ively. The arrays or shapes are said to be aligned
aligned
if for all i = 1, . . . , p it holds that
• mi = ni, or
• min{mi, ni} = 1, or
• either 

mi or ni, or both are missing.
For example, shapes (1, 2, 3) and (4, 2, 1) are aligned, as are (2, , ) and (1, 2, 3). However,
(2, 2, 2) and (1, 2, 3) are not aligned. NumPy “duplicates” the array elements across the
smaller dimension to match the larger dimension. This process is called broadcasting
broadcasting
and
is carried out without actually making copies, thus providing efficient memory use. Below
are some examples.
import numpy as np
A= np.arange(4).reshape(2,2) # (2,2) array
x1 = np.array([40,500])
# (2,) array
x2 = x1.reshape(2,1)
# (2,1) array
print(A + x1) # shapes (2,2) and (2,)
print(A * x2) # shapes (2,2) and (2,1)
[[ 40 501]
[ 42 503]]
[[
0
40]
[1000 1500]]
Note that above x1 is duplicated row-wise and x2 column-wise. Broadcasting also applies
to the matrix-wise operator @, as illustrated below. Here, the matrix b is duplicated across
the third dimension resulting in the two matrix multiplications
"0
1
2
3
# "0
1
2
3
#
and
"4
5
6
7
# "0
1
2
3
#
.
Python Primer
483
B = np.arange(8).reshape(2,2,2)
b = np.arange(4).reshape(2,2)
print(B@b)
[[[ 2
3]
[ 6 11]]
[[10 19]
[14 27]]]
Functions such as sum, mean, and std can also be executed as methods of an ndarray
instance. The argument axis can be passed to specify along which dimension the function
is applied. By default axis=None.
a = np.array(range(4)).reshape(2,2)
print(a.sum(axis=0)) #summing over rows gives column totals
[2, 4]
D.10.4
Random Numbers
One of the sub-modules in numpy is random. It contains many functions for random vari-
able generation.
import numpy as np
np.random.seed(123)
# set the seed for the random number generator
x = np.random.random()
# uniform (0,1)
y = np.random.randint(5,9)
# discrete uniform 5,...,8
z = np.random.randn(4)
# array of four standard normals
print(x,y,'\n',z)
0.6964691855978616 7
[ 1.77399501 -0.66475792 -0.07351368
1.81403277]
For more information on random variable generation in numpy, see
https://docs.scipy.org/doc/numpy/reference/random/index.html.
D.11
Matpl

otlib
The main Python graphics library for 2D and 3D plotting is matplotlib, and its subpack-
age pyplot contains a collection of functions that make plotting in Python similar to that
in MATLAB.
D.11.1
Creating a Basic Plot
The code below illustrates various possibilities for creating plots. The style and color of
lines and markers can be changed, as well as the font size of the labels. Figure D.1 shows
the result.
484
Matplotlib
sqrtplot.py
import matplotlib.pyplot as plt
import numpy as np
x = np.arange(0, 10, 0.1)
u = np.arange(0,10)
y = np.sqrt(x)
v = u/3
plt.figure(figsize = [4,2])
# size of plot in inches
plt.plot(x,y, 'g--')
# plot green dashed line
plt.plot(u,v,'r.')
# plot red dots
plt.xlabel('x')
plt.ylabel('y')
plt.tight_layout()
plt.savefig('sqrtplot.pdf',format='pdf')
# saving as pdf
plt.show()
# both plots will now be drawn
0
2
4
6
8
10
x
0
1
2
3
y
Figure D.1: A simple plot created using pyplot.
The library matplotlib also allows the creation of subplots. The scatterplot and histogram
in Figure D.2 have been produced using the code below. When creating a histogram there
are several optional arguments that affect the layout of the graph. The number of bins is
determined by the parameter bins (the default is 10). Scatterplots also take a number of
parameters, such as a string c which determines the color of the dots, and alpha which
affects the transparency of the dots.
histscat.py
import matplotlib.pyplot as plt
import numpy as np
x = np.random.randn(1000)
u = np.random.randn(100)
v = np.random.randn(100)
plt.subplot(121)
# first subplot
plt.hist(x,bins=25, facecolor='b')
plt.xlabel('X Variable')
plt.ylabel('Counts')
plt.subplot(122)
# second subplot
plt.scatter(u,v,c='b', alpha=0.5)
plt.show()
Python Primer
485
2
0
2
X Variable
0
20
40
60
80
100
120
Counts
2
0
2
2
1
0
1
2
Figure D.2: A histogram and scatterplot.
One can also create three-dimensional plots as illustrated below.
surf3dscat.py
import matplotlib.pyplot as plt
import numpy as np
from mpl_t

oolkits.mplot3d import Axes3D
def npdf(x,y):
return np.exp(-0.5*(pow(x,2)+pow(y,2)))/np.sqrt(2*np.pi)
x, y
= np.random.randn(100), np.random.randn(100)
z = npdf(x,y)
xgrid, ygrid = np.linspace(-3,3,100), np.linspace(-3,3,100)
Xarray, Yarray = np.meshgrid(xgrid,ygrid)
Zarray = npdf(Xarray,Yarray)
fig = plt.figure(figsize=plt.figaspect(0.4))
ax1 = fig.add_subplot(121, projection='3d')
ax1.scatter(x,y,z, c='g')
ax1.set_xlabel('$x$')
ax1.set_ylabel('$y$')
ax1.set_zlabel('$f(x,y)$')
ax2 = fig.add_subplot(122, projection='3d')
ax2.plot_surface(Xarray,Yarray,Zarray,cmap='viridis',
edgecolor='none')
ax2.set_xlabel('$x$')
ax2.set_ylabel('$y$')
ax2.set_zlabel('$f(x,y)$')
486
Pandas
plt.show()
x
2
1
0
1
2
3
y
2
1
0
1
2
f(x, y)
0.0
0.1
0.2
0.3
0.4
x
3
2
1 0
1
2
3
y
3
2
1
0
1
2
3
f(x, y)
0.05
0.10
0.15
0.20
0.25
0.30
0.35
Figure D.3: Three-dimensional scatter- and surface plots.
D.12
Pandas
The Python package Pandas (module name pandas) provides various tools and data struc-
tures for data analytics, including the fundamental DataFrame class.
For the code in this section we assume that pandas has been imported via
import␣pandas␣as␣pd.
D.12.1
Series and DataFrame
The two main data structures in pandas are Series and DataFrame. A Series object can
be thought of as a combination of a dictionary and an 1-dimensional ndarray. The syntax
for creating a Series object is
series = pd.Series(<data>, index=['index'])
Here, <data> some 1-dimensional data structure, such as a 1-dimensional ndarray, a list,
or a dictionary, and index is a list of names of the same length as <data>. When <data>
is a dictionary, the index is created from the keys of the dictionary. When <data> is an
ndarray and index is omitted, the default index will be [0,␣...,␣len(data)-1].
DICT = {'one':1, 'two':2, 'three':3, 'four':4}
print(pd.Series(DICT))
Python Primer
487
one
1
two
2
three
3
four
4
dtype: int64
years =
['2000','2001','2002']
cost = [2.34, 2.89, 3.01]
print(pd.Series(cost,index = years, name = 'MySeries'

)) #name it
2000
2.34
2001
2.89
2002
3.01
Name: MySeries , dtype: float64
The most commonly-used data structure in pandas is the two-dimensional DataFrame,
which can be thought of as pandas’ implementation of a spreadsheet or as a diction-
ary in which each “key” of the dictionary corresponds to a column name and the dic-
tionary “value” is the data in that column. To create a DataFrame one can use the
pandas DataFrame method, which has three main arguments: data, index (row labels),
and columns (column labels).
DataFrame(<data>, index=['<row_name>'], columns=['<column_name>'])
If the index is not specified, the default index is [0,␣...,␣len(data)-1]. Data can
also be read directly from a CSV or Excel file, as is done in Section 1.1. If a dictionary is
☞
1
used to create the data frame (as below), the dictionary keys are used as the column names.
DICT = {'numbers':[1,2,3,4], 'squared':[1,4,9,16] }
df = pd.DataFrame(DICT, index = list('abcd'))
print(df)
numbers
squared
a
1
1
b
2
4
c
3
9
d
4
16
D.12.2
Manipulating Data Frames
Often data encoded in DataFrame or Series objects need to be extracted, altered, or com-
bined. Getting, setting, and deleting columns works in a similar manner as for dictionaries.
The following code illustrates various operations.
ages = [6,3,5,6,5,8,0,3]
d={'Gender':['M', 'F']*4, 'Age': ages}
df1 = pd.DataFrame(d)
488
Pandas
df1.at[0,'Age']= 60
# change an element
df1.at[1,'Gender'] = 'Female' # change another element
df2 = df1.drop('Age',1)
# drop a column
df3 = df2.copy();
# create a separate copy of df2
df3['Age'] = ages
# add the original column
dfcomb = pd.concat([df1,df2,df3],axis=1)
# combine the three dfs
print(dfcomb)
Gender
Age
Gender
Gender
Age
0
M
60
M
M
6
1
Female
3
Female
Female
3
2
M
5
M
M
5
3
F
6
F
F
6
4
M
5
M
M
5
5
F
8
F
F
8
6
M
0
M
M
0
7
F
3
F
F
3
Note that the above DataFrame object has two Age columns. The expression
dfcomb[’Age’] will return a DataFrame with both these columns.
Table D.3: Useful pandas methods for data man

ipulation.
agg
Aggregate the data using one or more functions.
apply
Apply a function to a column or row.
astype
Change the data type of a variable.
concat
Concatenate data objects.
replace
Find and replace values.
read_csv
Read a CSV file into a DataFrame.
sort_values
Sort by values along rows or columns.
stack
Stack a DataFrame.
to_excel
Write a DataFrame to an Excel file.
It is important to correctly specify the data type of a variable before embarking on
data summarization and visualization tasks, as Python may treat different types of objects
in dissimilar ways. Common data types for entries in a DataFrame object are float,
category, datetime, bool, and int. A generic object type is object.
d={'Gender':['M', 'F', 'F']*4, 'Age': [6,3,5,6,5,8,0,3,6,6,7,7]}
df=pd.DataFrame(d)
print(df.dtypes)
df['Gender'] = df['Gender'].astype('category')
#change the type
print(df.dtypes)
Gender
object
Age
int64
dtype: object
Gender
category
Age
int64
dtype: object
Python Primer
489
D.12.3
Extracting Information
Extracting statistical information from a DataFrame object is facilitated by a large col-
lection of methods (functions) in pandas. Table D.4 gives a selection of data inspection
methods. See Chapter 1 for their practical use. The code below provides several examples
☞
1
of useful methods. The apply method allows one to apply general functions to columns
or rows of a DataFrame. These operations do not change the data. The loc method allows
for accessing elements (or ranges) in a data frame and acts similar to the slicing operation
for lists and arrays, with the difference that the “stop” value is included, as illustrated in
the code below.
import numpy as np
import pandas as pd
ages = [6,3,5,6,5,8,0,3]
np.random.seed(123)
df = pd.DataFrame(np.random.randn(3,4), index = list('abc'),
columns = list('ABCD'))
print(df)
df1 = df.loc["b":"c","B":"C"]
# create a partial data frame
print(df1)
meanA = df['A'].mean()
# mean of 'A' column
print('mean of column A = {}'.format(meanA))


expA = df['A'].apply(np.exp)
# exp of all elements in 'A' column
print(expA)
A
B
C
D
a -1.085631
0.997345
0.282978 -1.506295
b -0.578600
1.651437 -2.426679 -0.428913
c
1.265936 -0.866740 -0.678886 -0.094709
B
C
b
1.651437 -2.426679
c -0.866740 -0.678886
mean of column A = -0.13276486552118785
a
0.337689
b
0.560683
c
3.546412
Name: A, dtype: float64
The groupby method of a DataFrame object is useful for summarizing and displaying
the data in manipulated ways. It groups data according to one or more specified columns,
such that methods such as count and mean can be applied to the grouped data.
df = pd.DataFrame({'W':['a','a','b','a','a','b'],
'X':np.random.rand(6),
'Y':['c','d','d','d','c','c'], 'Z':np.random.rand(6)})
print(df)
W
X
Y
Z
0
a
0.993329
c
0.641084
1
a
0.925746
d
0.428412
2
b
0.266772
d
0.460665
3
a
0.201974
d
0.261879
4
a
0.529505
c
0.503112
490
Pandas
Table D.4: Useful pandas methods for data inspection.
columns
Column names.
count
Counts number of non-NA cells.
crosstab
Cross-tabulate two or more categories.
describe
Summary statistics.
dtypes
Data types for each column.
head
Display the top rows of a DataFrame.
groupby
Group data by column(s).
info
Display information about the DataFrame.
loc
Access a group or rows or columns.
mean
Column/row mean.
plot
Plot of columns.
std
Column/row standard deviation.
sum
Returns column/row sum.
tail
Display the bottom rows of a DataFrame.
value_counts
Counts of different non-null values.
var
Variance.
5
b
0.006231
c
0.849683
print(df.groupby('W').mean())
X
Z
W
a
0.662639
0.458622
b
0.136502
0.655174
print(df.groupby(['W', 'Y']).mean())
X
Z
W Y
a c
0.761417
0.572098
d
0.563860
0.345145
b c
0.006231
0.849683
d
0.266772
0.460665
To allow for multiple functions to be calculated at once, the agg method can be used.
It can take a list, dictionary, or string of functions.
print(df.groupby('W').agg([sum,np.mean]))
X
Z
sum
mean
sum
mean
W
a
2.650555
0.662639
1.834487
0.458622
b
0.273003
0.136502
1.310348
0.655174
Python Pri

mer
491
D.12.4
Plotting
The plot method of a DataFrame makes plots of a DataFrame using Matplotlib. Different
types of plot can be accessed via the kind = 'str' construction, where str is one of
line (default), bar, hist, box, kde, and several more. Finer control, such as modifying
the font, is obtained by using matplotlib directly. The following code produces the line
and box plots in Figure D.4.
import numpy as np
import pandas as pd
import matplotlib
df = pd.DataFrame({'normal':np.random.randn(100),
'Uniform':np.random.uniform(0,1,100)})
font = {'family' : 'serif', 'size'
: 14} #set font
matplotlib.rc('font', **font)
# change font
df.plot()
# line plot (default)
df.plot(kind = 'box')
# box plot
matplotlib.pyplot.show()
#render plots
0
20
40
60
80
100
2
0
2
Normal
Uniform
Normal
Uniform
2
0
2
Figure D.4: A line and box plot using the plot method of DataFrame.
D.13
Scikit-learn
Scikit-learn is an open-source machine learning and data science library for Python. The
library includes a range of algorithms relating to the chapters in this book. It is widely
used due to its simplicity and its breadth. The module name is sklearn. Below is a brief
introduction into modeling the data with sklearn. The full documentation can be found
at
https://scikit-learn.org/.
D.13.1
Partitioning the Data
Randomly partitioning the data in order to test the model may be achieved easily with
sklearn’s function train_test_split. For example, suppose that the training data is
described by the matrix X of explanatory variables and the vector y of responses. Then the
492
Scikit-learn
following code splits the data set into training and testing sets, with the testing set being
half of the total set.
from sklearn.model_selection import train_test_split
X_train , X_test, y_train , y_test = train_test_split(X, y,
test_size = 0.5)
As an example, the following code generates a synthetic data set and splits it into
equally-sized training and test sets.
syndat.py
import numpy as np
import matplotlib.

pyplot as plt
from sklearn.model_selection import train_test_split
np.random.seed(1234)
X=np.pi*(2*np.random.random(size=(400,2))-1)
y=(np.cos(X[:,0])*np.sin(X[:,1]) >=0)
X_train , X_test , y_train , y_test = train_test_split(X, y,
test_size=0.5)
fig = plt.figure()
ax = fig.add_subplot(111)
ax.scatter(X_train[y_train==0,0],X_train[y_train==0,1], c='g',
marker='o',alpha=0.5)
ax.scatter(X_train[y_train==1,0],X_train[y_train==1,1], c='b',
marker='o',alpha=0.5)
ax.scatter(X_test[y_test==0,0],X_test[y_test==0,1], c='g',
marker='s',alpha=0.5)
ax.scatter(X_test[y_test==1,0],X_test[y_test==1,1], c='b',
marker='s',alpha=0.5)
plt.savefig('sklearntraintest.pdf',format='pdf')
plt.show()
D.13.2
Standardization
In some instances it may be necessary to standardize the data. This may be done in
sklearn with scaling methods such as MinMaxScaler or StandardScaler. Scaling may
improve the convergence of gradient-based estimators and is useful when visualizing data
on vastly different scales. For example, suppose that X is our explanatory data (e.g., stored
as a numpy array), and we wish to standardize such that each value lies between 0 and 1.
from sklearn import preprocessing
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
x_scaled = min_max_scaler.fit_transform(X)
# equivalent to:
x_scaled = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
Python Primer
493
3
2
1
0
1
2
3
3
2
1
0
1
2
3
Figure D.5: Example training (circles) and test (squares) set for two class classification.
Explanatory variables are the (x, y) coordinates, classes are zero (green) or one (blue).
D.13.3
Fitting and Prediction
Once the data has been partitioned and standardized if necessary, the data may be fitted to
a statistical model, e.g., a classification or regression model. For example, continuing with
our data from above, the following fits a model to the data and predicts the responses for
the test set.
from sklearn.someSubpackage import someClassifier
clf = someClassifier()
# choose a

ppropriate classifier
clf.fit(X_train , y_train)
# fit the data
y_prediction = clf.predict(X_test)
# predict
Specific classifiers for logistic regression, naïve Bayes, linear and quadratic discrimin-
ant analysis, K-nearest neighbors, and support vector machines are given in Section 7.8.
☞277
D.13.4
Testing the Model
Once the model has made its prediction we may test its effectiveness, using relevant met-
rics. For example, for classification we may wish to produce the confusion matrix for the
test data. The following code does this for the data shown in Figure D.5, using a support
vector machine classifier.
from sklearn import svm
clf = svm.SVC(kernel = 'rbf')
clf.fit(X_train , y_train)
y_prediction = clf.predict(X_test)
494
System Calls, URL Access, and Speed-Up
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test , y_prediction))
[[102
12]
[
1
85]]
D.14
System Calls, URL Access, and Speed-Up
Operating system commands (whether in Windows, MacOS, or Linux) for creating dir-
ectories, copying or removing files, or executing programs from the system shell can be
issued from within Python by using the package os. Another useful package is requests
which enables direct downloads of files and webpages from URLs. The following Python
script uses both. It also illustrates a simple example of exception handling in Python.
misc.py
import os
import requests
for c in "123456":
try:
# if it does not yet exist
os.mkdir("MyDir"+ c)
# make a directory
except:
# otherwise
pass
# do nothing
uname = "https://github.com/DSML-book/Programs/tree/master/
Appendices/Python Primer/"
fname = "ataleof2cities.txt"
r = requests.get(uname + fname)
print(r.text)
open('MyDir1/ato2c.txt', 'wb').write(r.content) #write to a file
# bytes mode is important here
The package numba can significantly speed up calculations via smart compilation. First
run the following code.
jitex.py
import timeit
import numpy as np
from numba import jit
n = 10**8
#@jit
def myfun(s,n):
for i in rang

e(1,n):
s = s+ 1/i
return s
start = timeit.time.clock()
print("Euler's constant is approximately {:9.8f}".format(
Python Primer
495
myfun(0,n) - np.log(n)))
end = timeit.time.clock()
print("elapsed time: {:3.2f} seconds".format(end-start))
Euler's constant is approximately 0.57721566
elapsed time: 5.72 seconds
Now remove the # character before the @ character in the code above, in order to
activate the “just in time” compiler. This gives a 15-fold speedup:
Euler's constant is approximately 0.57721566
elapsed time: 0.39 seconds
Further Reading
To learn Python, we recommend [82] and [110]. However, as Python is constantly evolving,
the most up-to-date references will be available from the Internet.
496
System Calls, URL Access, and Speed-Up
BIBLIOGRAPHY
[1] S. C. Ahalt, A. K. Krishnamurthy, P. Chen, and D. E. Melton. Competitive learning
algorithms for vector quantization. Neural Networks, 3:277–290, 1990.
[2] H. Akaike. A new look at the statistical model identification. IEEE Transactions on
Automatic Control, 19(6):716–723, 1974.
[3] N. Aronszajn. Theory of reproducing kernels. Transactions of the American Math-
ematical Society, 68:337–404, 1950.
[4] D. Arthur and S. Vassilvitskii. K-means++: The advantages of careful seeding.
In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Al-
gorithms, pages 1027–1035, Philadelphia, 2007. Society for Industrial and Applied
Mathematics.
[5] S. Asmussen and P. W. Glynn. Stochastic Simulation: Algorithms and Analysis.
Springer, New York, 2007.
[6] R. G. Bartle. The Elements of Integration and Lebesgue Measure. John Wiley &
Sons, Hoboken, 1995.
[7] D. Bates and D. Watts. Nonlinear Regression Analysis and Its Applications. John
Wiley & Sons, Hoboken, 1988.
[8] J. O. Berger. Statistical Decision Theory and Bayesian Analysis. Springer, New
York, second edition, 1985.
[9] J. Bezdek. Pattern Recognition with Fuzzy Objective Function Algorithms. Plenum
Press, New York, 1981.
[10] P. J. Bickel and K. A. Doksum. Mathem

atical Statistics, volume I. Pearson Prentice
Hall, Upper Saddle River, second edition, 2007.
497
498
Bibliography
[11] P. Billingsley.
Probability and Measure.
John Wiley & Sons, New York, third
edition, 1995.
[12] C. M. Bishop. Pattern Recognition and Machine Learning. Springer, New York,
2006.
[13] P. T. Boggs and R. H. Byrd. Adaptive, limited-memory BFGS algorithms for un-
constrained optimization. SIAM Journal on Optimization, 29(2):1282–1299, 2019.
[14] Z. I. Botev, J. F. Grotowski, and D. P. Kroese. Kernel density estimation via diffu-
sion. Annals of Statistics, 38(5):2916–2957, 2010.
[15] Z. I. Botev and D. P. Kroese. Global likelihood optimization via the cross-entropy
method, with an application to mixture models. In R. G. Ingalls, M. D. Rossetti,
J. S. Smith, and B. A. Peters, editors, Proceedings of the 2004 Winter Simulation
Conference, pages 529–535, Washington, DC, December 2004.
[16] Z. I. Botev, D. P. Kroese, R. Y. Rubinstein, and P. L’Ecuyer. The cross-entropy
method for optimization. In V. Govindaraju and C.R. Rao, editors, Machine Learn-
ing: Theory and Applications, volume 31 of Handbook of Statistics, pages 35–59.
Elsevier, 2013.
[17] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and
statistical learning via the alternating direction method of multipliers. Foundations
and Trends in Machine Learning, 3:1–122, 2010.
[18] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press,
Cambridge, 2004. Seventh printing with corrections, 2009.
[19] R. A. Boyles.
On the convergence of the EM algorithm.
Journal of the Royal
Statistical Society, Series B, 45(1):47–50, 1983.
[20] L. Breiman. Classification and Regression Trees. CRC Press, Boca Raton, 1987.
[21] L. Breiman. Bagging predictors. Machine Learning, 24(2):123–140, 1996.
[22] L. Breiman. Heuristics of instability and stabilization in model selection. Annals of
Statistics, 24(6):2350–2383, 12 1996.
[23] L. Breiman. Random forests. Machine Learn

ing, 45(1):5–32, 2001.
[24] F. Cao, D.-Z. Du, B. Gao, P.-J. Wan, and P. M. Pardalos. Minimax problems in
combinatorial optimization. In D.-Z. Du and P. M. Pardalos, editors, Minimax and
Applications, pages 269–292. Kluwer, Dordrecht, 1995.
[25] G. Casella and R. L. Berger. Statistical Inference. Duxbury Press, Pacific Grove,
second edition, 2001.
[26] K. L. Chung. A Course in Probability Theory. Academic Press, New York, second
edition, 1974.
Bibliography
499
[27] E. Cinlar. Introduction to Stochastic Processes. Prentice Hall, Englewood Cliffs,
1975.
[28] T. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley &
Sons, New York, 1991.
[29] J. W. Daniel, W. B. Gragg, L. Kaufman, and G. W. Stewart. Reorthogonalization and
stable algorithms for updating the Gram-Schmidt QR factorization. Mathematics of
Computation, 30(136):772–795, 1976.
[30] P.-T. de Boer, D. P. Kroese, S. Mannor, and R. Y. Rubinstein. A tutorial on the
cross-entropy method. Annals of Operations Research, 134(1):19–67, 2005.
[31] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incom-
plete data via the EM algorithm. Journal of the Royal Statistical Society, 39(1):1 –
38, 1977.
[32] L. Devroye. Non-Uniform Random Variate Generation. Springer, New York, 1986.
[33] N. R. Draper and H. Smith. Applied Regression Analysis. John Wiley & Sons, New
York, third edition, 1998.
[34] Q. Duan and D. P. Kroese. Splitting for optimization. Computers & Operations
Research, 73:119–131, 2016.
[35] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classification. John Wiley & Sons,
New York, 2001.
[36] B. Efron and T. J. Hastie. Computer Age Statistical Inference: Algorithms, Evidence,
and Data Science. Cambridge University Press, Cambridge, 2016.
[37] B. Efron and R. Tibshirani. An Introduction to the Bootstrap. Chapman & Hall,
New York, 1994.
[38] T. Fawcett.
An introduction to ROC analysis.
Pattern Recognition Letters,
27(8):861–874, June 2006.
[39] W. Feller. An Introduction to Pr

obability Theory and Its Applications, volume I.
John Wiley & Sons, Hoboken, second edition, 1970.
[40] J. C. Ferreira and V. A. Menegatto. Eigenvalues of integral operators defined by
smooth positive definite kernels. Integral Equations and Operator Theory, 64:61–
81, 2009.
[41] N. I. Fisher and P. K. Sen, editors.
The Collected Works of Wassily Hoeffding.
Springer, New York, 1994.
[42] G. S. Fishman. Monte Carlo: Concepts, Algorithms and Applications. Springer,
New York, 1996.
[43] R. Fletcher. Practical Methods of Optimization. John Wiley & Sons, New York,
1987.
500
Bibliography
[44] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning
and an application to boosting. J. Comput. Syst. Sci., 55(1):119–139, 1997.
[45] J. H. Friedman. Greedy function approximation: A gradient boosting machine. An-
nals of Statistics, 29:1189–1232, 2000.
[46] A. Gelman. Bayesian Data Analysis. Chapman & Hall, New York, second edition,
2004.
[47] A. Gelman and J. Hall. Data Analysis Using Regression and Multilevel/Hierarchical
Models. Cambridge University Press, Cambridge, 2006.
[48] S. Geman and D. Geman. Stochastic relaxation, Gibbs distribution and the Bayesian
restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 6(6):721–741, 1984.
[49] J. E. Gentle. Random Number Generation and Monte Carlo Methods. Springer,
New York, second edition, 2003.
[50] W. R. Gilks, S. Richardson, and D. J. Spiegelhalter. Markov Chain Monte Carlo in
Practice. Chapman & Hall, New York, 1996.
[51] P. Glasserman. Monte Carlo Methods in Financial Engineering. Springer, New
York, 2004.
[52] G. H. Golub and C. F. Van Loan. Matrix Computations. Johns Hopkins University
Press, Baltimore, fourth edition, 2013.
[53] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, Cambridge,
2016.
[54] G. R. Grimmett and D. R. Stirzaker. Probability and Random Processes. Oxford
University Press, third edition, 2001.
[55] T. J. Hastie, R. J. Tib

shirani, and J. H. Friedman. The Elements of Statistical Learn-
ing: Data mining, Inference, and Prediction. Springer, New York, 2009.
[56] T. J. Hastie, R. J. Tibshirani, and M. Wainwright. Statistical Learning with Sparsity:
The Lasso and Generalizations. CRC Press, Boca Raton, 2015.
[57] J.-B. Hiriart-Urruty and C. Lemarèchal.
Fundamentals of Convex Analysis.
Springer, New York, 2001.
[58] W. Hock and K. Schittkowski. Test Examples for Nonlinear Programming Codes.
Springer, New York, 1981.
[59] J. E. Kelley, Jr. The cutting-plane method for solving convex programs. Journal of
the Society for Industrial and Applied Mathematics, 8(4):703–712, 1960.
[60] A. K. Jain. Fundamentals of Digital Image Processing. Prentice Hall, Englewood
Cliffs, 1989.
Bibliography
501
[61] O. Kallenberg. Foundations of Modern Probability. Springer, New York, second
edition, 2002.
[62] A. Karalic. Linear regression in regression tree leaves. In Proceedings of ECAI-92,
pages 440–441, Hoboken, 1992. John Wiley & Sons.
[63] C. Kaynak.
Methods of combining multiple classifiers and their applications to
handwritten digit recognition. Master’s thesis, Institute of Graduate Studies in Sci-
ence and Engineering, Bogazici University, 1995.
[64] T. Keilath and A. H. Sayed, editors. Fast Reliable Algorithms for Matrices with
Structure. SIAM, Pennsylvania, 1999.
[65] C. Nussbaumer Knaflic. Storytelling with Data: A Data Visualization Guide for
Business Professionals. John Wiley & Sons, Hoboken, 2015.
[66] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Tech-
niques - Adaptive Computation and Machine Learning. The MIT Press, Cambridge,
2009.
[67] A. N. Kolmogorov and S. V. Fomin.
Elements of the Theory of Functions and
Functional Analysis. Dover Publications, Mineola, 1999.
[68] D. P. Kroese, T. Brereton, T. Taimre, and Z. I. Botev. Why the Monte Carlo method
is so important today. Wiley Interdisciplinary Reviews: Computational Statistics,
6(6):386–392, 2014.
[69] D. P. Kroese

 and J. C. C. Chan. Statistical Modeling and Computation. Springer,
2014.
[70] D. P. Kroese, S. Porotsky, and R. Y. Rubinstein.
The cross-entropy method for
continuous multi-extremal optimization. Methodology and Computing in Applied
Probability, 8(3):383–407, 2006.
[71] D. P. Kroese, T. Taimre, and Z. I. Botev. Handbook of Monte Carlo Methods. John
Wiley & Sons, New York, 2011.
[72] H. J. Kushner and G. G. Yin. Stochastic Approximation and Recursive Algorithms
and Applications. Springer, New York, second edition, 2003.
[73] P. Lafaye de Micheaux, R. Drouilhet, and B. Liquet. The R Software: Fundamentals
of Programming and Statistical Analysis. Springer, New York, 2014.
[74] R. J. Larsen and M. L. Marx. An Introduction to Mathematical Statistics and Its
Applications. Prentice Hall, New York, third edition, 2001.
[75] A. M. Law and W. D. Kelton. Simulation Modeling and Analysis. McGraw-Hill,
New York, third edition, 2000.
[76] P. L’Ecuyer. A unified view of IPA, SF, and LR gradient estimation techniques.
Management Science, 36:1364–1383, 1990.
502
Bibliography
[77] P. L’Ecuyer. Good parameters and implementations for combined multiple recursive
random number generators. Operations Research, 47(1):159 – 164, 1999.
[78] E. L. Lehmann and G. Casella. Theory of Point Estimation. Springer, New York,
second edition, 1998.
[79] T. G. Lewis and W. H. Payne. Generalized feedback shift register pseudorandom
number algorithm. Journal of the ACM, 20(3):456–468, 1973.
[80] R. J. A. Little and D. B. Rubin. Statistical Analysis with Missing Data. John Wiley
& Sons, Hoboken, second edition, 2002.
[81] D. C. Liu and J. Nocedal. On the limited memory BFGS method for large scale
optimization. Mathematical Programming, 45(1-3):503–528, 1989.
[82] M. Lutz. Learning Python. O’Reilly, fifth edition, 2013.
[83] M. Matsumoto and T. Nishimura.
Mersenne twister: A 623-dimensionally
equidistributed uniform pseudo-random number generator. ACM Transactions on
Modeling and Computer Simulation, 8(1

):3–30, 1998.
[84] W. McKinney. Python for Data Analysis. O’Reilly Media, Inc., second edition,
2017.
[85] G. J. McLachlan and T. Krishnan. The EM Algorithm and Extensions. John Wiley
& Sons, Hoboken, second edition, 2008.
[86] G. J. McLachlan and D. Peel. Finite Mixture Models. John Wiley & Sons, New
York, 2000.
[87] N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller.
Equations of state calculations by fast computing machines. Journal of Chemical
Physics, 21(6):1087–1092, 1953.
[88] C. A. Micchelli, Y. Xu, and H. Zhang.
Universal kernels.
Journal of Machine
Learning Research, 7:2651–2667, 2006.
[89] Z. Michalewicz.
Genetic Algorithms + Data Structures = Evolution Programs.
Springer, New York, third edition, 1996.
[90] J. F. Monahan. Numerical Methods of Statistics. Cambridge University Press, Lon-
don, 2010.
[91] T. A. Mroz. The sensitivity of an empirical model of married women’s hours of
work to economic and statistical assumptions. Econometrica, 55(4):765–799, 1987.
[92] K. P. Murphy. Machine Learning: A Probabilistic Perspective. The MIT Press,
Cambridge, 2012.
[93] J. Neyman and E. Pearson. On the problem of the most efficient tests of statistical
hypotheses. Philosophical Transactions of the Royal Society of London, Series A,
231:289–337, 1933.
Bibliography
503
[94] M. A. Nielsen. Neural Networks and Deep Learning, volume 25. Determination
Press, 2015.
[95] K. B. Petersen and M. S. Pedersen. The Matrix Cookbook. Technical University of
Denmark, 2008.
[96] J. R. Quinlan. Learning with continuous classes. In A. Adams and L. Sterling,
editors, Proceedings AI’92, pages 343–348, Singapore, 1992. World Scientific.
[97] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning.
MIT Press, Cambridge, 2006.
[98] B. D. Ripley. Stochastic Simulation. John Wiley & Sons, New York, 1987.
[99] C. P. Robert and G. Casella. Monte Carlo Statistical Methods. Springer, New York,
second edition, 2004.
[100] S. M. Ross. Simulation. 

Academic Press, New York, third edition, 2002.
[101] S. M. Ross. A First Course in Probability. Prentice Hall, Englewood Cliffs, seventh
edition, 2005.
[102] R. Y. Rubinstein. The cross-entropy method for combinatorial and continuous op-
timization. Methodology and Computing in Applied Probability, 2:127–190, 1999.
[103] R. Y. Rubinstein and D. P. Kroese. The Cross-Entropy Method: A Unified Approach
to Combinatorial Optimization, Monte-Carlo Simulation and Machine Learning.
Springer, New York, 2004.
[104] R. Y. Rubinstein and D. P. Kroese. Simulation and the Monte Carlo Method. John
Wiley & Sons, New York, third edition, 2017.
[105] S. Ruder.
An overview of gradient descent optimization algorithms.
arXiv:
1609.04747, 2016.
[106] W. Rudin. Functional Analysis. McGraw–Hill, Singapore, second edition, 1991.
[107] D. Salomon. Data Compression: The Complete Reference. Springer, New York,
2000.
[108] G. A. F. Seber and A. J. Lee. Linear Regression Analysis. John Wiley & Sons,
Hoboken, second edition, 2003.
[109] S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From The-
ory to Algorithms. Cambridge University Press, Cambridge, 2014.
[110] Z. A. Shaw. Learning Python 3 the Hard Way. Addison–Wesley, Boston, 2017.
[111] Y. Shen, S. Kiatsupaibul, Z. B. Zabinsky, and R. L. Smith. An analytically de-
rived cooling schedule for simulated annealing. Journal of Global Optimization,
38(2):333–365, 2007.
504
Bibliography
[112] N. Z. Shor. Minimization Methods for Non-differentiable Functions. Springer, Ber-
lin, 1985.
[113] B. W. Silverman. Density Estimation for Statistics and Data Analysis. Chapman &
Hall, New York, 1986.
[114] J. S. Simonoff. Smoothing Methods in Statistics. Springer, New York, 2012.
[115] I. Steinwart and A. Christmann. Support Vector Machines. Springer, New York,
2008.
[116] G. Strang. Introduction to Linear Algebra. Wellesley–Cambridge Press, Cambridge,
fifth edition, 2016.
[117] G. Strang. Linear Algebra and Learning from Data. Wellesley–Ca

mbridge Press,
Cambridge, 2019.
[118] W. N. Street, W. H. Wolberg, and O. L. Mangasarian. Nuclear feature extraction for
breast tumor diagnosis. In IS&T/SPIE 1993 International Symposium on Electronic
Imaging: Science and Technology, San Jose, CA, pages 861–870, 1993.
[119] V. M. Tikhomirov. On the representation of continuous functions of several variables
as superpositions of continuous functions of one variable and addition. In Selected
Works of A. N. Kolmogorov, pages 383–387. Springer, Berlin, 1991.
[120] S. van Buuren. Flexible Imputation of Missing Data. CRC Press, Boca Raton,
second edition, 2018.
[121] V. N. Vapnik. The Nature of Statistical Learning Theory. Springer, New York, 1995.
[122] V. N. Vapnik and A. Ya. Chervonenkis. On the uniform convergence of relative fre-
quencies of events to their probabilities. Theory of Probability and Its Applications,
16(2):264–280, 1971.
[123] G. Wahba. Spline Models for Observational Data. SIAM, Philadelphia, 1990.
[124] L. Wasserman. All of Statistics: A Concise Course in Statistical Inference. Springer,
2010.
[125] A. Webb. Statistical Pattern Recognition. Arnold, London, 1999.
[126] H. Wendland. Scattered Data Approximation. Cambridge University Press, Cam-
bridge, 2005.
[127] D. Williams. Probability with Martingales. Cambridge University Press, Cam-
bridge, 1991.
[128] C. F. J. Wu. On the convergence properties of the EM algorithm. The Annals of
Statistics, 11(1):95–103, 1983.
INDEX
A
acceptance probability, 78–80, 97
acceptance–rejection method, 73, 78
accuracy (classification–), 254
activation function, 204, 325
AdaBoost, 317–320
AdaGrad, 340
Adam method, 340, 348
adjoint operation, 361
affine transformation, 405, 435
agglomerative clustering, 147
Akaike information criterion, 126, 176,
177
algebraic multiplicity, 363
aligned arrays (Python), 482
almost sure convergence, 440
alternating direction method of
multipliers, 220, 416
alternative hypothesis, 458
anaconda (Python), 463
analysis of variance (ANOVA), 183

, 184,
196, 209
annealing schedule, 97
approximation error, 32–34, 184
approximation–estimation tradeoff, 32,
42, 323
Armijo inexact line search, 409
assignment operator (Python), 467
attributes (Python), 466
auxiliary variable methods, 128
axioms of Kolmogorov, 421
B
back-propagation, 332
backward elimination, 201
backward substitution, 370
bagged estimator, 306
bagging, 305, 307, 310
balance equations (Markov chains), 78,
79, 452
bandwidth, 131, 134, 225
barplot, 9
barrier function, 417
Barzilai–Borwein formulas, 335, 413
basis
of a vector space, 355
orthogonal –, 361
Bayes
empirical, 241
error rate, 252
factor, 58
naïve –, 258
optimal decision rule, 258
Bayes’ rule, 48, 49, 428, 454
Bayesian information criterion, 55
Bayesian statistics, 48, 50, 454
Bernoulli distribution, 425, 457
Bessel distribution, 164, 226
505
506
Index
beta distribution, 53, 425
bias of an estimator, 454
bias vector (deep learning), 326
bias–variance tradeoff, 35, 305
binomial distribution, 425
Boltzmann distribution, 96
bootstrap aggregation, see bagging
bootstrap method, 88, 306
bounded mapping, 389
boxplot, 10, 14
broadcasting (Python), 482
Broyden’s family, 411
Broyden–Fletcher–Goldfarb–Shanno
(BFGS) updating, 267, 339, 411
burn-in period, 78
C
categorical variable, 3, 177, 178, 191,
192, 251, 299
Cauchy sequence, 246, 385
Cauchy–Schwarz inequality, 223, 246,
389, 412
central difference estimator, 106
central limit theorem, 447, 458
multivariate, 448
centroid, 144
chain rule for differentiation, 401
characteristic function, 225–227, 246,
392, 441
characteristic polynomial, 363
Chebyshev’s inequality, 444
chi-squared distribution, 436, 439
Cholesky decomposition, 70, 154, 247,
264, 373
circulant matrix, 381, 393
class (Python), 474
classification, 20, 251–286
hierarchical, 256
multilabel, 256
classifier, 21, 251
coefficient of determination, 181, 195
adjusted, 181
coefficient profiles, 221
combinatorial optimization, 402
comma separated values (CSV), 2
common random numbers, 106, 120
com

plete Hilbert space, 224, 384
complete vector space, 216
complete convergence, 443
complete-data
likelihood, 128
log-likelihood, 138
composition of functions, 400
concave function, 404, 407
conditional
distribution, 431
expectation, 431
pdf, 74, 431
probability, 428
confidence interval, 85, 89, 94, 186, 457
Bayesian, 52
bootstrap, 89
confidence region, 457
confusion matrix, 253, 254
constrained optimization, 403
context management (Python), 477
continuous mapping, 389
continuous optimization, 402
control variable, 92
convergence
almost sure, 440
in Lp norm, 442
in distribution, 440
in probability, 439
sure, 439
convex
function, 63, 220, 403
program, 405–408
set, 43, 403
convolution, 380, 392
convolution neural network, 330
Cook’s distance, 212
cooling factor, 97
correlation coefficient, 71, 429
cost-complexity
measure, 303
pruning, 303
countable sample space, 422
covariance, 429
matrix, 46, 70, 430–432, 435, 436
properties, 429
coverage probability, 457
Index
507
credible
interval, 52
region, 52
critical
region, 458
value, 458
cross tabulate, 7
cross-entropy
method, 100, 111
risk, 54, 122, 125
in-sample, 176
training loss, 123
cross-validation, 38
leave-one-out, 40, 173
linear model, 174
crude Monte Carlo, 85
cubic spline, 236
cumulative distribution function (cdf),
72, 423
joint, 427
cycle, 81
D
Davidon–Fletcher–Powell updating, 353,
412
decision tree, 288
deep learning, 331
degrees of freedom, 181
dendrogram, 148
density, 385
dependent variable, 168
derivatives
multidimensional, 398
partial, 397
design matrix, 179
detailed balance equations, 453
determinant of a matrix, 357
diagonal matrix, 357
diagonalizable, 364
dictionary (Python), 473
digamma function, 127, 163
dimension, 355
direct sum, 217
directional derivative, 404
discrete
distribution, 423
Fourier transform, 392
optimization, 402
probability space, 422
sample space, 422
uniform distribution, 425
discriminant analysis, 259
distribution
Bernoulli, 425
Bessel, 226
beta, 53, 425
binomial, 425
chi-squared, 43

6, 439
discrete, 423
discrete uniform, 425
exponential, 425
extreme value, 115
F, 439
gamma, 425
Gaussian, see normal
geometric, 425
inverse-gamma, 51, 83
joint, 427
multivariate normal, 46, 435
noncentral χ2, 437
normal, 45, 425, 434
Pareto, 425
Poisson, 425
probability, 422, 427
Student’s t, 439
uniform, 425
Weibull, 425
divisive clustering, 148
dot notation (Python), 466
dual optimization problem, 407–408
E
early stopping, 50, 250
efficiency
of estimators, 454
of acceptance–rejection, 72
eigen-decomposition, 364
eigenvalue, 363
eigenvector, 363
elementary event, 422
elite sample, 100
empirical
508
Index
Bayes, 241
cdf, 11, 76
distribution, 131
entropy impurity, 292
epoch (deep learning), 350
equilikely principle, 422
ergodic Markov chain, 452
error of the first and second kind, 459
estimate, 454
estimator, 454
bias of, 454
control variable, 92
efficiency of, 454
unbiased, 454
Euclidean norm, 360
evaluation functional, 223, 245
event, 421
elementary, 422
independent, 428
exact match ratio, 256
exchangeable variables, 41
expectation, 426
conditional, 431
properties, 429, 431
vector, 46, 430, 432, 435
expectation–maximization (EM)
algorithm, 128, 137, 209
expected generalization risk, 24
expected optimism, 36
explanatory variable, 22, 168
exponential distribution, 425
extreme value distribution, 115
F
factor, 3, 177
false negative, 254
false positive, 254
fast Fourier transform, 394
Fβ score, 255
F distribution, 183, 198, 424, 439
feasible region, 403
feature, 1, 20
importance, 311
map, 189, 216, 224, 230, 243, 274
feed-forward network, 326
feedback shift register, 69
finite difference method, 107, 114
finite-dimensional distributions, 427
Fisher information matrix, 124
Fisher’s scoring method, 127
folds (cross-validation), 38
forward selection, 201
forward substitution, 370
Fourier expansion, 386
Fourier transform, 391
discrete, 392
frequentist statistics, 453
full rank matrix, 28
function (Python), 468
function space, 384
function, Ck, 403
functional, 389
function

s of random variables, 431
G
gamma
distribution, 425
function, 424
Gauss–Markov inequality, 60
Gauss–Newton search direction, 414
Gaussian distribution, see normal
distribution
Gaussian kernel, 225
Gaussian kernel density estimate, 131
Gaussian process, 71, 238
Gaussian rule of thumb, 134
generalization risk, 23, 86
generalized inverse-gamma distribution,
163
generalized linear model, 204
geometric cooling, 97
geometric distribution, 425
geometric multiplicity, 363
Gibbs pdf, 97
Gibbs sampler, 81, 83, 84
random, 82
random order, 82
reversible, 82
Gini impurity, 292
global balance equations, 452
global minimizer, 402
gradient, 397, 403
Index
509
boosting, 316
descent, 412
Gram matrix, 218, 222, 270
Gram–Schmidt procedure, 375
H
Hamming distance, 142
Hermite polynomials, 389
Hermitian matrix, 362, 365
Hessian matrix, 124, 398, 403, 404
hidden layer, 325
hierarchical classification, 256
Hilbert matrix, 33
inverse, 61
Hilbert space, 215, 385
isomorphism, 247
hinge loss, 269
histogram, 10
Hoeffding’s inequality, 63
homotopy paths, 221
hyperparameters, 51, 241
hypothesis testing, 458
I
immutable (Python), 464
importance sampling, 93–96
improper prior, 51, 83
in-sample risk, 36
incremental effects, 179
independence
of event, 428
of random variables, 429
independence sampler, 79
independent and identically distributed
(iid), 429, 446, 454
indicator, 11
indicator feature, 178
indicator loss, 251
infinitesimal perturbation analysis, 114
information matrix equality, 124
inheritance (Python), 475
initial distribution (Markov chain), 452
inner product, 360
instance (Python), 474
integration
Monte Carlo, 86
interaction, 179, 193
interior-point method, 418
interval estimate, see confidence interval
inverse
discrete Fourier transform, 393
Fourier transform, 391
matrix, 370
inverse-gamma distribution, 51, 83
inverse-transform method, 72
irreducible risk, 32
iterable (Python), 472
iterative reweighted least squares, 213,
350
iterator (Python), 472
J
Jacobi
matrix of, 409, 433
Jensen’

s inequality, 63
joint
cdf, 427
pdf, 427
jointly normal, see multivariate normal
jointly normal distribution, see
multivariate normal distribution
K
Karush–Kuhn–Tucker (KKT) conditions,
407, 408
kernel density estimation, 131, 135, 226,
330
kernel trick, 231
Kiefer–Wolfowitz algorithm, 107
K-nearest neighbors method, 268
Kolmogorov axioms, 421
Kullback–Leibler divergence, 42, 100,
128, 351
L
Lagrange
dual program, 407
function, 406
method, 406–407
multiplier, 406
Lagrangian, 406, 416
penalty, 416
Laguerre polynomials, 388
Lance–Williams update, 150
510
Index
Laplace’s approximation, 450
lasso (regression), 220
latent variable methods, see auxiliary
variable methods
law of large numbers, 67, 446, 458
law of total probability, 428
learner, 22, 168
learning rate, 335, 409
least-squares
iterative reweighted, 213
nonlinear, 190, 336, 414
ordinary, 27, 47, 171, 191, 211, 378
regularized, 172, 235, 376
leave-one-out cross-validation, 40, 173
left pseudo-inverse, 360
left-eigenvector, 365
Legendre polynomials, 387
length preserving transformation, 361
length of a vector, 360
level set, 103
Levenberg–Marquardt search direction,
415
leverage, 173
Levinson–Durbin, 71, 382
likelihood, 43, 49, 123, 456
complete-data, 128
log-, 136, 456
optimization, 137
ratio, 93
limited memory BFGS, 337
limiting pdf, 452
limiting pdf (Markov chain), 452
line search, 409
linear
discriminant function, 260
kernel, 224, 271
mapping, 389
model, 44, 211, 212
program, 406
subspace, 362
transformation, 356, 431
linearly independent, 355
link function, 204
linkage, 148
matrix, 150
list comprehension (Python), 473
local balance equations, see detailed
balance equations
local minimizer, 402
local/global minimum, 402
log-likelihood, 456
log-odds ratio, 266
logarithmic efficiency, 118
logistic distribution, 205
logistic regression, 205
long-run average reward, 89
loss function, 20
loss matrix, 253
M
M-estimator, 448
Manhattan distance, 142
marginal distribution, 427, 436
Markov chain, 74, 78, 80, 83, 451
er

godic, 452
reversible, 452
simulation of, 75
Markov chain Monte Carlo, 78
Markov property, 74, 451
Matérn kernel, 226
matplotlib (Python), 483–486
matrix, 356
blockwise inverse, 370
covariance, 70, 436
determinant, 357
diagonal —, 357
inverse, 357
of Jacobi, 398, 409, 414, 433
pseudo-inverse, 360
sparse, 379
Toeplitz, 379
trace, 357
transpose, 357
matrix multiplication (Python), 482
max-cut problem, 151
maximum a posteriori, 53
maximum distance, 142
maximum likelihood estimation, 43, 47,
100, 127, 136, 137, 456
mean integrated squared error, 133
mean squared error, 32, 88, 454
measure, 385
Index
511
Mersenne twister, 69
method (Python), 466
method of moments, 455
Metropolis–Hastings algorithm, 78, 81
minibatch, 336
minimax
equality, 408
problem, 408
minimization, 411
minimizer, 402
minimum
global, 402
local, 402
misclassification error, 253
misclassification impurity, 292
mixture density, 135
model, 40
evidence, 55
linear, 211, 212
matrix, 44, 170, 174
multiple linear regression, 169
normal linear, 174, 182, 183, 438
regression, 191
response surface, 189
simple linear regression, 187
modified Bessel function of the second
kind, 164, 226
module (Python), 470
modulo 2 generators, 69
modulus, 69
moment
generating function, 427, 436
sample-, 455
momentum method, 341
Monte Carlo
integration, 86
sampling, 68–85
simulation, 67
Moore–Penrose pseudo-inverse, 360
multi-logit, 266
multi-output linear regression, 214
nonlinear, 329
multilabel classification, 256
multiple linear regression, 169
multiple-recursive generator, 69
multiplier
Lagrange, 406
multivariate
central limit theorem, 448
normal distribution, 45–47, 435
mutable (Python), 464
N
naïve Bayes, 258
namespace (Python), 470
nested models, 59, 180
network architecture, 330
network depth, 327
network width, 327
neural networks, 323
Newton’s method, 127, 206, 213, 337,
409
— for root-finding, 409
quasi —, 337
Neyman–Pearson approach, 459
noisy optimization, 106
nominal distribution, 93
noncentral χ2 distribution, 437
no

rm, 384, 389
normal distribution, 46, 425, 434, 435
normal equations, 28
normal linear model, 47, 174, 182, 183,
438
normal matrix, 365
normal method (bootstrap), 89
normal model, 45
Bayesian, 50, 51, 83
normal updating (cross-entropy), 101
null hypothesis, 458
null space, 363
O
object (Python), 464
objective function, 402, 403, 407, 415
Occam’s razor, 173
operator, 389
operator (Python), 465
optimal decision boundary, 270
optimization
combinatorial, 402
constrained, 403
continuous, 402
unconstrained, 403
512
Index
ordinary least-squares, 27
orthogonal
basis, 361
complement, 362
matrix, 361, 382
polynomial, 388
projection, 362
vector, 360
orthonormal, 361
basis, 386
system, 385
out-of-bag, 307
overfitting, 23, 35, 141, 172, 216, 237,
289, 293, 300, 314
overloading (Python), 468
P
p-norm, 220, 408
P-value, 195, 459
pandas (Python), 2, 486–491
Pareto distribution, 425
Parseval’s formula, 392
partial derivative, 397
partition, 428
peaks function, 233
Pearson’s height data, 207
penalty function, 415, 419
exact, 416
percentile, 7
percentile method (bootstrap), 89, 91
permutation matrix, 368
Plancherel’s theorem, 392
PLU decomposition, 368
pointwise squared bias, 35
pointwise variance, 35
Poisson distribution, 425
polynomial kernel, 230
polynomial regression model, 26
positive definite
matrix, 403
positive semidefinite
function, 222
matrix, 367, 404
posterior
pdf, 49
predictive density, 50
precision, 255
predicted residual, 173
— sum of squares (PRESS), 173
prediction function, 20
prediction interval, 186
predictive mean, 240
predictor, 168
primal optimization problem, 407
principal axes, 154
principal component analysis (PCA),
153, 155
principal components, 154
prior
improper, 83
pdf, 49
predictive density, 50
uninformative, 50
probability
density function (pdf), 424
density function (pdf), joint, 427
distribution, 422, 427
mass function, 424
measure, 421
space, 422
product rule, 74, 428, 452
projected subgradient method, 107
projection matrix, 27, 173, 211, 265,
362, 43

8
projection pursuit, 350
proposal (MCMC), 78
pseudo-inverse, 28, 211, 360, 378
Pythagoras’ theorem, 180, 181, 183, 231,
361
Q
quadratic discriminant function, 260
quadratic program, 406
qualitative variable, 3
quantile, 52, 85
quantile–quantile plot, 199
quantitative variable, 3
quartile, 7
quasi-Newton method, 337, 411
quasi-random point set, 233
quotient rule for differentiation, 160
Index
513
R
radial basis function (rbf) kernel, 225,
276
random
experiment, 421
number generator, 68
numbers (Python), 483
sample
see iid sample, 454
variable, 422
vector, 427, 431
covariance of, 430
expectation of, 430
walk sampler, 80
range (Python), 472
rank, 28, 356
rarity parameter (cross-entropy), 100
ratio estimator, 89
read_csv (Python), 2
recall, 255
reference (Python), 467
regional prediction functions, 288
regression, 20, 167
function, 21
line, 169
model, 191
simple linear, 181
regularization, 216, 217
paths, 221
regularization parameter, 217
regularizer, 217
relative error (estimated), 85
relative time variance product, 454
renewal reward process, 89
representational capacity, 323
representer of evaluation, 222
reproducing kernel Hilbert space
(RKHS), 222
reproducing property, 222
resampling, 76, 88
residual squared error, 171
residual sum of squares, 171
residuals, 171, 173
response surface model, 189
response variable, 20, 168
reverse Markov chain, 452
reversibility, 452
reversible Gibbs sampler, 82
ridge regression, 216, 217
Riemann–Lebesgue lemma, 391
right pseudo-inverse, 360
risk, 20, 167
Robbins–Monro algorithm, 107
root finding, 408
R2, see coefficient of determination
S
saddle point, 403
problem, 408
sample
mean, 7, 85, 455
median, 7
quantile, 7
range, 8
space, 421
countable, 422
discrete, 422
standard deviation, 8, 455
variance, 8, 89, 455
saturation, 333
Savage–Dickey density ratio, 59
scale-mixture, 164
scatterplot, 13
scikit-learn (Python), 491–494
score function, 43, 123
method, 114
secant condition, 411
semi-simple matrix, 364
sequence object (Python), 472


set (Python), 473
shear operation, 359
Sherman–Morrison
formula, 174, 247, 248, 371
recursion, 372, 373, 414
significance level, 459
simple linear regression, 169, 187
simulated annealing, 96, 97
sinc kernel, 226
singular value, 377, 378
singular value decomposition, 154, 376
slack variable, 417
Slater’s condition, 408
514
Index
slice (Python), 3, 465
smoothing parameter, 100
softmax function, 267, 329
source vectors, 143
sparse matrix, 379
specificity, 255
spectral representation, 377
sphere the data, 264
splitting for continuous optimization,
103
splitting rule, 289
squared-error loss, 167
standard basis, 356
standard deviation, 426
sample-, 455
standard error (estimated), 85
standard normal distribution, 434
standardization, 435
stationary point, 403
statistical (estimation) error, 32, 95
statistical test
one-sided –, 458
two-sided –, 458
statistics
Bayesian, 454
frequentist, 453
steepest descent, 331, 412
step-size parameter γ, 314
stochastic approximation, 106, 336
stochastic confidence interval, 457
stochastic counterpart, 107
stochastic gradient descent, 336, 350
stochastic process, 427
strict feasibility, 408
strong duality, 408
Student’s t distribution, 183, 424, 439
multivariate, 162, 164, 226
studentized residual, 212
stumps, 319
subgradient, 404
subgradient method, 107
sum rule, 422
supervised learning, 22
support vectors, 271
Sylvester equation, 379
systematic Gibbs sampler, 82
T
tables
counts, 6
frequency, 6
margins, 7
target distribution, 78
Taylor’s theorem
multidimensional, 400
test
loss, 24
sample, 24
statistic, 458
theta KDE, 134
time-homogeneous, 452
Tobit regression, 209
Toeplitz matrix, 379
total sum of squares, 181
tower property of expectation, 431
trace of a matrix, 357
training loss, 23
training set, 21
transformation
of random variables, 431, 433
rule, 95, 432
transition
density, 74, 452
graph, 75
transpose of a matrix, 356, 357
tree branch, 301
true negative, 254
true positive, 254
trust region, 409
type (Python), 466
type I and type II e

rrors, 459
U
unbiased, 60
unbiased estimator, 454
unconstrained optimization, 403
uniform distribution, 425
union bound, 422
unitary matrix, 362
universal approximation property, 226
unsupervised learning, 22
V
validation set, 25, 303
Index
515
Vandermonde matrix, 29, 393
Vapnik–Chernovenkis bound, 63
variance, 426, 430
properties, 429
sample, 89, 455
sample-, 455
vector quantization, 143
vector space, 355
basis, 355
dimension, 355
Voronoi tessellation, 143
W
weak derivative, 114
weak duality, 407
weak learners, 313
Weibull distribution, 425
weight matrix (deep learning), 326
Wolfe dual program, 407
Woodbury identity, 248, 352, 371, 399


